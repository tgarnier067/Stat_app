{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gc-N_bxnObwn",
    "outputId": "a8cb318d-08bc-4edd-be5c-c51a68b88c5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis) (1.26.3)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis) (1.11.4)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis) (2.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis) (1.3.2)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: numexpr in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyLDAvis) (2.9.0)\n",
      "Requirement already satisfied: funcy in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyLDAvis) (2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis) (1.3.2)\n",
      "Requirement already satisfied: gensim in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyLDAvis) (4.3.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyLDAvis) (68.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gensim->pyLDAvis) (6.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->pyLDAvis) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: nltk in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: wordcloud in /usr/local/python/3.10.13/lib/python3.10/site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/codespace/.local/lib/python3.10/site-packages (from wordcloud) (1.26.3)\n",
      "Requirement already satisfied: pillow in /home/codespace/.local/lib/python3.10/site-packages (from wordcloud) (10.2.0)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.10/site-packages (from wordcloud) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Requirement already satisfied: TextBlob in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from TextBlob) (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis\n",
    "!pip install nltk\n",
    "!pip install wordcloud\n",
    "!pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mSOVtiqNKtK",
    "outputId": "06b329e1-9b0e-41db-8110-1b4af46507d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "#from IPython.display import display\n",
    "import base64\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from time import time\n",
    "# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3JH1r8K5gMe"
   },
   "source": [
    "# Récupération des communications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi6ZdjmlEK32"
   },
   "source": [
    "## Webscrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour voir notre travail de webscrapping, on pourra se référer au notebook nommé \"Essaie webscrapp.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Une première base de donnée : Une centaine d'articles du NYT et du WSJ avec le mot clef environnement sur les derniers jours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger à partir du fichier pickle\n",
    "data = pd.read_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUFpMBn5EZk8"
   },
   "source": [
    "# Traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAfkJlueLLR2"
   },
   "source": [
    "## Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Minuscule\n",
    "    text = text.lower()\n",
    "    # Supprimer la ponctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    # Suppression des stop-words\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in filtered_words])\n",
    "    \n",
    "    return lemmatized_output\n",
    "\n",
    "# Appliquer la fonction preprocess_text à la colonne 'Article'\n",
    "data['Preprocessed_Article'] = data['Article'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Nombre de mots</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Titre</th>\n",
       "      <th>ID</th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nick Tabor</td>\n",
       "      <td>529</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Copyright 2023 The New York Times Company.  Al...</td>\n",
       "      <td>Document NYTF000020240104ejcv0000d</td>\n",
       "      <td>metropolitan desk sectmb ambitious public univ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Wesley Morris</td>\n",
       "      <td>422</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>When Jim Brown and Raquel Welch, Two Sexy Star...</td>\n",
       "      <td>Document NYTF000020231231ejcv0006h</td>\n",
       "      <td>magazine desk sectmm jim brown raquel welch tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nTalking During Movi...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>179</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Talking During Movies: Totally Evil or Part of...</td>\n",
       "      <td>Document NYTF000020231231ejcv00064\\n</td>\n",
       "      <td>magazine desk sectmk talking movie totally evi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>454</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Let Kids Vote!</td>\n",
       "      <td>Document NYTF000020231231ejcv00063\\n</td>\n",
       "      <td>magazine desk sectmk let kid vote 454 word 31 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Christina Caron</td>\n",
       "      <td>428</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Are We Doomed to Disagree?</td>\n",
       "      <td>Document NYTF000020231231ejcv0005z\\n</td>\n",
       "      <td>magazine desk sectmk doomed disagree 428 word ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nHello, Fourth Grade...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>319</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Hello, Fourth Graders! A Look Back at our Clas...</td>\n",
       "      <td>Document NYTF000020231231ejcv0005t\\n</td>\n",
       "      <td>magazine desk sectmk hello fourth grader look ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\nForeign Desk; SECTA\\n72 of Our Favorite Fa...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Times Insider</td>\n",
       "      <td>914</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>72 of Our Favorite Facts From 2023</td>\n",
       "      <td>Document NYTF000020231231ejcv0005r\\n</td>\n",
       "      <td>foreign desk secta 72 favorite fact 2023 time ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\nMoney and Business/Financial Desk; SECTBU\\...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Kashmir Hill</td>\n",
       "      <td>811</td>\n",
       "      <td>Stalker Under Your Hood</td>\n",
       "      <td>The Stalker Under Your Hood</td>\n",
       "      <td>Document NYTF000020231231ejcv0005n\\n</td>\n",
       "      <td>money businessfinancial desk sectbu stalker ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\nBook Review Desk; SECTBR\\nPaperback Row\\n\\...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Shreya Chattopadhyay</td>\n",
       "      <td>431</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Paperback Row</td>\n",
       "      <td>Document NYTF000020231231ejcv0005g\\n</td>\n",
       "      <td>book review desk sectbr paperback row shreya c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n\\nNICHOLAS KRISTOF\\nEditorial Desk; SECTSR\\n...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nicholas Kristof</td>\n",
       "      <td>976</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Humans Made Progress In 2023</td>\n",
       "      <td>Document NYTF000020231231ejcv00052\\n</td>\n",
       "      <td>nicholas kristof editorial desk sectsr human m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article              Date  \\\n",
       "0  \\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...  31 December 2023   \n",
       "1  \\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...  31 December 2023   \n",
       "2  \\n\\nMagazine Desk; SECTMK\\nTalking During Movi...  31 December 2023   \n",
       "3  \\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...  31 December 2023   \n",
       "4  \\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...  31 December 2023   \n",
       "5  \\n\\nMagazine Desk; SECTMK\\nHello, Fourth Grade...  31 December 2023   \n",
       "6  \\n\\nForeign Desk; SECTA\\n72 of Our Favorite Fa...  31 December 2023   \n",
       "7  \\n\\nMoney and Business/Financial Desk; SECTBU\\...  31 December 2023   \n",
       "8  \\n\\nBook Review Desk; SECTBR\\nPaperback Row\\n\\...  31 December 2023   \n",
       "9  \\n\\nNICHOLAS KRISTOF\\nEditorial Desk; SECTSR\\n...  31 December 2023   \n",
       "\n",
       "                 Auteur  Nombre de mots                  Journal  \\\n",
       "0            Nick Tabor             529           New York Times   \n",
       "1         Wesley Morris             422           New York Times   \n",
       "2                  None             179           New York Times   \n",
       "3                  None             454           New York Times   \n",
       "4       Christina Caron             428           New York Times   \n",
       "5                  None             319           New York Times   \n",
       "6         Times Insider             914           New York Times   \n",
       "7          Kashmir Hill             811  Stalker Under Your Hood   \n",
       "8  Shreya Chattopadhyay             431           New York Times   \n",
       "9      Nicholas Kristof             976           New York Times   \n",
       "\n",
       "                                               Titre  \\\n",
       "0  Copyright 2023 The New York Times Company.  Al...   \n",
       "1  When Jim Brown and Raquel Welch, Two Sexy Star...   \n",
       "2  Talking During Movies: Totally Evil or Part of...   \n",
       "3                                     Let Kids Vote!   \n",
       "4                         Are We Doomed to Disagree?   \n",
       "5  Hello, Fourth Graders! A Look Back at our Clas...   \n",
       "6                 72 of Our Favorite Facts From 2023   \n",
       "7                        The Stalker Under Your Hood   \n",
       "8                                      Paperback Row   \n",
       "9                       Humans Made Progress In 2023   \n",
       "\n",
       "                                      ID  \\\n",
       "0     Document NYTF000020240104ejcv0000d   \n",
       "1     Document NYTF000020231231ejcv0006h   \n",
       "2  Document NYTF000020231231ejcv00064\\n    \n",
       "3  Document NYTF000020231231ejcv00063\\n    \n",
       "4  Document NYTF000020231231ejcv0005z\\n    \n",
       "5  Document NYTF000020231231ejcv0005t\\n    \n",
       "6  Document NYTF000020231231ejcv0005r\\n    \n",
       "7  Document NYTF000020231231ejcv0005n\\n    \n",
       "8  Document NYTF000020231231ejcv0005g\\n    \n",
       "9  Document NYTF000020231231ejcv00052\\n    \n",
       "\n",
       "                                Preprocessed_Article  \n",
       "0  metropolitan desk sectmb ambitious public univ...  \n",
       "1  magazine desk sectmm jim brown raquel welch tw...  \n",
       "2  magazine desk sectmk talking movie totally evi...  \n",
       "3  magazine desk sectmk let kid vote 454 word 31 ...  \n",
       "4  magazine desk sectmk doomed disagree 428 word ...  \n",
       "5  magazine desk sectmk hello fourth grader look ...  \n",
       "6  foreign desk secta 72 favorite fact 2023 time ...  \n",
       "7  money businessfinancial desk sectbu stalker ho...  \n",
       "8  book review desk sectbr paperback row shreya c...  \n",
       "9  nicholas kristof editorial desk sectsr human m...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA99lQ4VRIPj"
   },
   "source": [
    "## Analyse du sentiment des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EApSppc3ROou"
   },
   "source": [
    "### Sentiment général"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6163XiZXN2i"
   },
   "source": [
    "Le score donné varie de -1 à 1 avec -1 comme la négativité maximale et 1 comme la positivité maximale. 0 pour dire que le texte est neutre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "3cIikpC8RVPd",
    "outputId": "09d001bc-f208-4a1e-a073-8981d04cba49"
   },
   "outputs": [],
   "source": [
    "# Fonction pour calculer le sentiment\n",
    "def calculate_sentiment(text):\n",
    "    # Création d'une instance TextBlob\n",
    "    analysis = TextBlob(text)\n",
    "    # Retourner la polarité\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "# Appliquer la fonction au DataFrame\n",
    "data['Sentiment'] = data['Preprocessed_Article'].apply(calculate_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.080807\n",
       "1     0.096043\n",
       "2     0.129610\n",
       "3     0.068136\n",
       "4     0.081156\n",
       "        ...   \n",
       "95    0.017752\n",
       "96    0.062612\n",
       "97   -0.002605\n",
       "98    0.129660\n",
       "99   -0.021999\n",
       "Name: Sentiment, Length: 100, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ7rBPftRPf3"
   },
   "source": [
    "### Sentiment environnemental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idéal serait de récupérer un dictionnaire pré existant, spécialisé dans l'évaluation de termes écologique, qui attribue une score à chaque terme. La difficulté à trouver ce type de dictionnaire nous mène dans un premier temps à creuser d'autres pistes de substitution. Nous verrons plus tard si nous réussissons à trouver un dictionnaire préexistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative aux dictionnaires pré-existants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problème pour l'amélioration du dictionnaire : on ne trouve pas de dictionnaire préexistant avec comme spécialité l'environnement. Deux options : \n",
    "\n",
    " - Améliorer notre dictionnaire fait main:\n",
    "   - Avantage : On peut contrôler le poid associé à chaque mot, dans la note\n",
    "   - Inconvéniant : COnstruction peu rigoureuse, on peut avoir oublié des mots\n",
    "  \n",
    " - Utiliser un dictionnaire généraliste :\n",
    "   - Avantage : Construction plus rigoureuse, moins de chance d'oublier certains termes\n",
    "   - Inconvénient : Pas de contrôle sur le poid des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Le dictionnaire fait main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le moment, on ne trouve pas de dictionnaire pré-existant, dont chaque terme peut être associé à une note environnementale. On propose donc de construire nous même un dictionnaire, un en français et l'autre en anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un dictionnaire de termes environnementaux positifs\n",
    "Dico_env_fr = {\n",
    "    \"propre\": 1,\n",
    "    \"écologique\": 1,\n",
    "    \"durable\": 1,\n",
    "    \"vert\": 1,\n",
    "    \"économie d'énergie\": 1,\n",
    "    \"renouvelable\": 1,\n",
    "    \"responsable\": 1,\n",
    "    \"conservation\": 1,\n",
    "    \"biodiversité\": 1,\n",
    "    \"sain\": 1,\n",
    "    \"bio\": 1,\n",
    "    \"éco-friendly\": 1,\n",
    "    \"respectueux de l'environnement\": 1,\n",
    "    \"efficace\": 1,\n",
    "    \"innovant\": 1,\n",
    "    \"éthique\": 1,\n",
    "    \"équitable\": 1,\n",
    "    \"efficience\": 1,\n",
    "    \"responsabilité sociale\": 1,\n",
    "    \"sustainable\": 1,\n",
    "    \"solidaire\": 1,\n",
    "    \"propagation consciente\": 1,\n",
    "    \"soutenable\": 1,\n",
    "    \"énergie propre\": 1,\n",
    "    \"énergie renouvelable\": 1,\n",
    "    \"recyclage\": 1,\n",
    "    \"efficacité énergétique\": 1,\n",
    "    \"économie circulaire\": 1,\n",
    "    \"énergie solaire\": 1,\n",
    "    \"énergie éolienne\": 1,\n",
    "    \"régénération\": 1,\n",
    "    \"préservation\": 1,\n",
    "    \"restauration\": 1,\n",
    "    \"réhabilitation\": 1,\n",
    "    \"récupération\": 1,\n",
    "    \"restaurateur\": 1,\n",
    "    \"régénérateur\": 1,\n",
    "    \"revitalisation\": 1,\n",
    "    \"positif\": 1,\n",
    "    \"bénéfique\": 1,\n",
    "    \"valorisation\": 1,\n",
    "    \"épanouissement\": 1,\n",
    "    \"amélioration continue\": 1,\n",
    "    \"prospérité\": 1,\n",
    "    \"harmonie\": 1,\n",
    "    \"intégrité\": 1,\n",
    "    \"consommation responsable\": 1,\n",
    "    \"éco-responsable\": 1,\n",
    "    \"éco-conscient\": 1,\n",
    "    \"durabilité\": 1,\n",
    "    \"récupérable\": 1,\n",
    "    \"énergie verte\": 1,\n",
    "    \"effet de serre\": 1,\n",
    "    \"éco-efficace\": 1,\n",
    "    \"éco-innovation\": 1,\n",
    "    \"bien-être\": 1,\n",
    "    \"éco-design\": 1,\n",
    "    \"agroécologie\": 1,\n",
    "    \"permaculture\": 1,\n",
    "    \"éco-citoyen\": 1,\n",
    "    \"carbone neutre\": 1,\n",
    "    \"zéro déchet\": 1,\n",
    "    \"biologique\": 1,\n",
    "    \"éco-label\": 1,\n",
    "    \"mobilité durable\": 1,\n",
    "    \"éco-tourisme\": 1,\n",
    "    \"éco-habitat\": 1,\n",
    "    \"consommation consciente\": 1,\n",
    "\n",
    "    \"pollution\": -1,\n",
    "    \"déchet\": -1,\n",
    "    \"déforestation\": -1,\n",
    "    \"émissions de gaz à effet de serre\": -1,\n",
    "    \"contamination\": -1,\n",
    "    \"destructeur\": -1,\n",
    "    \"irresponsable\": -1,\n",
    "    \"gaspillage\": -1,\n",
    "    \"nuisible\": -1,\n",
    "    \"toxique\": -1,\n",
    "    \"détérioration\": -1,\n",
    "    \"dégradation\": -1,\n",
    "    \"dommageable\": -1,\n",
    "    \"préjudiciable\": -1,\n",
    "    \"périlleux\": -1,\n",
    "    \"inquiétant\": -1,\n",
    "    \"catastrophique\": -1,\n",
    "    \"catastrophe\": -1,\n",
    "    \"dangereux\": -1,\n",
    "    \"menace\": -1,\n",
    "    \"risque\": -1,\n",
    "    \"nocif\": -1,\n",
    "    \"néfaste\": -1,\n",
    "    \"inadéquat\": -1,\n",
    "    \"inapproprié\": -1,\n",
    "    \"inopportun\": -1,\n",
    "    \"nuire\": -1,\n",
    "    \"endommagement\": -1,\n",
    "    \"dommages\": -1,\n",
    "    \"polluant\": -1,\n",
    "    \"polluer\": -1,\n",
    "    \"détériorer\": -1,\n",
    "    \"perturbation\": -1,\n",
    "    \"irrespectueux\": -1,\n",
    "    \"malveillant\": -1,\n",
    "    \"dégât\": -1,\n",
    "    \"agressif\": -1,\n",
    "    \"ravageur\": -1,\n",
    "    \"gâcher\": -1,\n",
    "    \"perturber\": -1,\n",
    "    \"endommager\": -1,\n",
    "    \"irréparable\": -1,\n",
    "    \"toxicité\": -1,\n",
    "    \"inacceptable\": -1,\n",
    "    \"dommage écologique\": -1,\n",
    "    \"abattage illégal\": -1,\n",
    "    \"surconsommation\": -1,\n",
    "    \"pillage des ressources\": -1,\n",
    "    \"dégradation de l'environnement\": -1,\n",
    "    \"espace naturel détruit\": -1,\n",
    "    \"exploitation excessive\": -1,\n",
    "    \"surexploitation\": -1,\n",
    "    \"réchauffement climatique\": -1,\n",
    "    \"déni environnemental\": -1,\n",
    "}\n",
    "liste_negation = [\"pas\", \"non\",\"jamais\", \"aucun\", \"nul\", \"rien\", \"personne\", \"négatif\", \"sans\", \"plus\", \"moins\"]\n",
    "\n",
    "liste_annulation_negation = [\"responsable\",\"à l'origine\",\"la source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dico_env_en = {\n",
    "    \n",
    "    \"clean\": 1,\n",
    "    \"ecological\": 1,\n",
    "    \"sustainable\": 1,\n",
    "    \"green\": 1,\n",
    "    \"energy-efficient\": 1,\n",
    "    \"renewable\": 1,\n",
    "    \"responsible\": 1,\n",
    "    \"conservation\": 1,\n",
    "    \"biodiversity\": 1,\n",
    "    \"healthy\": 1,\n",
    "    \"organic\": 1,\n",
    "    \"eco-friendly\": 1,\n",
    "    \"environmentally friendly\": 1,\n",
    "    \"efficient\": 1,\n",
    "    \"innovative\": 1,\n",
    "    \"ethical\": 1,\n",
    "    \"fair\": 1,\n",
    "    \"efficiency\": 1,\n",
    "    \"social responsibility\": 1,\n",
    "    \"sustainable\": 1,\n",
    "    \"solidarity\": 1,\n",
    "    \"conscious spreading\": 1,\n",
    "    \"sustainable\": 1,\n",
    "    \"clean energy\": 1,\n",
    "    \"renewable energy\": 1,\n",
    "    \"recycling\": 1,\n",
    "    \"energy efficiency\": 1,\n",
    "    \"circular economy\": 1,\n",
    "    \"solar energy\": 1,\n",
    "    \"wind energy\": 1,\n",
    "    \"regeneration\": 1,\n",
    "    \"preservation\": 1,\n",
    "    \"restoration\": 1,\n",
    "    \"rehabilitation\": 1,\n",
    "    \"recovery\": 1,\n",
    "    \"restorer\": 1,\n",
    "    \"regenerator\": 1,\n",
    "    \"revitalization\": 1,\n",
    "    \"positive\": 1,\n",
    "    \"beneficial\": 1,\n",
    "    \"valorization\": 1,\n",
    "    \"fulfillment\": 1,\n",
    "    \"continuous improvement\": 1,\n",
    "    \"prosperity\": 1,\n",
    "    \"harmony\": 1,\n",
    "    \"integrity\": 1,\n",
    "    \"responsible consumption\": 1,\n",
    "    \"eco-responsible\": 1,\n",
    "    \"eco-conscious\": 1,\n",
    "    \"sustainability\": 1,\n",
    "    \"recoverable\": 1,\n",
    "    \"green energy\": 1,\n",
    "    \"greenhouse effect\": 1,\n",
    "    \"eco-efficient\": 1,\n",
    "    \"eco-innovation\": 1,\n",
    "    \"well-being\": 1,\n",
    "    \"eco-design\": 1,\n",
    "    \"agroecology\": 1,\n",
    "    \"permaculture\": 1,\n",
    "    \"eco-citizen\": 1,\n",
    "    \"carbon neutral\": 1,\n",
    "    \"zero waste\": 1,\n",
    "    \"organic\": 1,\n",
    "    \"eco-label\": 1,\n",
    "    \"sustainable mobility\": 1,\n",
    "    \"eco-tourism\": 1,\n",
    "    \"eco-habitat\": 1,\n",
    "    \"conscious consumption\": 1,\n",
    "    \n",
    "    \"pollution\": -1,\n",
    "    \"waste\": -1,\n",
    "    \"deforestation\": -1,\n",
    "    \"greenhouse gas emissions\": -1,\n",
    "    \"contamination\": -1,\n",
    "    \"destructive\": -1,\n",
    "    \"irresponsible\": -1,\n",
    "    \"wasteful\": -1,\n",
    "    \"harmful\": -1,\n",
    "    \"toxic\": -1,\n",
    "    \"deterioration\": -1,\n",
    "    \"degradation\": -1,\n",
    "    \"damaging\": -1,\n",
    "    \"harmful\": -1,\n",
    "    \"perilous\": -1,\n",
    "    \"worrisome\": -1,\n",
    "    \"catastrophic\": -1,\n",
    "    \"catastrophe\": -1,\n",
    "    \"dangerous\": -1,\n",
    "    \"threat\": -1,\n",
    "    \"risk\": -1,\n",
    "    \"hazardous\": -1,\n",
    "    \"harmful\": -1,\n",
    "    \"inappropriate\": -1,\n",
    "    \"inadequate\": -1,\n",
    "    \"inappropriate\": -1,\n",
    "    \"harm\": -1,\n",
    "    \"damage\": -1,\n",
    "    \"pollutant\": -1,\n",
    "    \"pollute\": -1,\n",
    "    \"deteriorate\": -1,\n",
    "    \"disruption\": -1,\n",
    "    \"disrespectful\": -1,\n",
    "    \"malevolent\": -1,\n",
    "    \"damage\": -1,\n",
    "    \"aggressive\": -1,\n",
    "    \"ravager\": -1,\n",
    "    \"spoil\": -1,\n",
    "    \"disturb\": -1,\n",
    "    \"damage\": -1,\n",
    "    \"irreparable\": -1,\n",
    "    \"toxicity\": -1,\n",
    "    \"unacceptable\": -1,\n",
    "    \"ecological damage\": -1,\n",
    "    \"illegal logging\": -1,\n",
    "    \"overconsumption\": -1,\n",
    "    \"resource plundering\": -1,\n",
    "    \"environmental degradation\": -1,\n",
    "    \"destroyed natural habitat\": -1,\n",
    "    \"excessive exploitation\": -1,\n",
    "    \"overexploitation\": -1,\n",
    "    \"climate change\": -1,\n",
    "    \"environmental denial\": -1,\n",
    "}\n",
    "\n",
    "negation_list = [\"not\", \"no\", \"never\", \"none\", \"nil\", \"nothing\", \"nobody\", \"negative\", \"without\", \"more\", \"less\"]\n",
    "\n",
    "negation_cancellation_list = [\"responsible\", \"originally\", \"source\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dictionnaire généraliste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'explorer la puissance d'un dictionnaire pré exsitant, nous faisant le choix de considérer un dictionnaire pré-existant, même s'il n'est pas spécialisé dans l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de sentiment pour le terme 'good':\n",
      "POS: 0.5, NEG: 0.0, OBJ: 0.5\n",
      "POS: 0.875, NEG: 0.0, OBJ: 0.125\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.75, NEG: 0.0, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 1.0, NEG: 0.0, OBJ: 0.0\n",
      "POS: 1.0, NEG: 0.0, OBJ: 0.0\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 1.0, NEG: 0.0, OBJ: 0.0\n",
      "POS: 0.75, NEG: 0.0, OBJ: 0.25\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 0.5, NEG: 0.0, OBJ: 0.5\n",
      "POS: 0.5, NEG: 0.0, OBJ: 0.5\n",
      "POS: 0.375, NEG: 0.0, OBJ: 0.625\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 0.75, NEG: 0.0, OBJ: 0.25\n",
      "POS: 0.75, NEG: 0.0, OBJ: 0.25\n",
      "POS: 0.875, NEG: 0.0, OBJ: 0.125\n",
      "POS: 0.5, NEG: 0.0, OBJ: 0.5\n",
      "POS: 0.375, NEG: 0.125, OBJ: 0.5\n",
      "POS: 0.75, NEG: 0.0, OBJ: 0.25\n",
      "POS: 0.375, NEG: 0.0, OBJ: 0.625\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "\n",
      "Scores de sentiment pour le terme 'bad':\n",
      "POS: 0.0, NEG: 0.875, OBJ: 0.125\n",
      "POS: 0.0, NEG: 0.625, OBJ: 0.375\n",
      "POS: 0.25, NEG: 0.25, OBJ: 0.5\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.625, OBJ: 0.375\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.625, OBJ: 0.375\n",
      "POS: 0.0, NEG: 0.5, OBJ: 0.5\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 1.0, OBJ: 0.0\n",
      "POS: 0.0, NEG: 0.375, OBJ: 0.625\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.125, NEG: 0.25, OBJ: 0.625\n",
      "POS: 0.125, NEG: 0.25, OBJ: 0.625\n",
      "\n",
      "Scores de sentiment pour le terme 'environment':\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "\n",
      "Scores de sentiment pour le terme 'technology':\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "\n",
      "Scores de sentiment pour le terme 'greenhouse':\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "\n",
      "Scores de sentiment pour le terme 'gases':\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "\n",
      "Aucun synset trouvé pour le terme 'greenhouse gas'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "# Termes à examiner\n",
    "terms = [\"good\", \"bad\", \"environment\", \"technology\",\"greenhouse\",\"gases\",\"greenhouse gas\"]\n",
    "\n",
    "for term in terms:\n",
    "    # Obtenir les synsets associés au terme\n",
    "    synsets = list(swn.senti_synsets(term))\n",
    "\n",
    "    if synsets:\n",
    "        print(f\"Scores de sentiment pour le terme '{term}':\")\n",
    "        for synset in synsets:\n",
    "            print(f\"POS: {synset.pos_score()}, NEG: {synset.neg_score()}, OBJ: {synset.obj_score()}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Aucun synset trouvé pour le terme '{term}'.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse : On a l'avantage d'avoir trois notes, qui représentent la part de positivité, de négativité et de neutralité du mot => Avancé par rapport à ce qu'on avait proposé. De plus, on considère qu'il y a plusieurs sens à chaque mot, d'où le fait qu'il y ait plusieurs évaluation pour chaque terme\n",
    "Avantage ; l'algorithme choisi la note du mot en fonction du contexte ?\n",
    "Problème : les coefficients ne sont pas forcément bons, par exemple, gases n'est jamais négatif, toujours neutre... pourquoi ?\n",
    "\n",
    "On test maintenant sur une phrase entière :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de la phrase: (0.16182795698924732, 0.020833333333333332, 0.6506720430107527)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_sentiment_scores(term):\n",
    "    synsets = list(swn.senti_synsets(term))\n",
    "    if synsets:\n",
    "        pos_score = sum(s.pos_score() for s in synsets) / len(synsets)\n",
    "        neg_score = sum(s.neg_score() for s in synsets) / len(synsets)\n",
    "        obj_score = sum(s.obj_score() for s in synsets) / len(synsets)\n",
    "\n",
    "        # Normaliser les scores\n",
    "        total_score = pos_score + neg_score + obj_score\n",
    "        if total_score != 0:\n",
    "            pos_score /= total_score\n",
    "            neg_score /= total_score\n",
    "            obj_score /= total_score\n",
    "\n",
    "        return pos_score, neg_score, obj_score\n",
    "    else:\n",
    "        return 0, 0, 0\n",
    "\n",
    "def analyze_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    scores = []\n",
    "\n",
    "    for token in tokens:\n",
    "        pos_score, neg_score, obj_score = get_sentiment_scores(token)\n",
    "        scores.append((pos_score, neg_score, obj_score))\n",
    "\n",
    "    # Calculer les scores moyens pour la phrase\n",
    "    avg_pos_score = sum(score[0] for score in scores) / len(scores)\n",
    "    avg_neg_score = sum(score[1] for score in scores) / len(scores)\n",
    "    avg_obj_score = sum(score[2] for score in scores) / len(scores)\n",
    "\n",
    "    return avg_pos_score, avg_neg_score, avg_obj_score\n",
    "\n",
    "# Exemple d'utilisation\n",
    "phrase = \"Clean technology promotes sustainable development.\"\n",
    "score_phrase = analyze_sentence(phrase)\n",
    "print(\"Score de la phrase:\", score_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque un taux de positivité de 16%, contre un taux de négativité de 2%, ainsi qu'un taux de neutralité de 65%. C'est un résultat relativement encourageant, étant donné qu'on a proposé une phrase à l'algorithme qui semblait être positive d'un point de vue environnemental.\n",
    "\n",
    "On compare ce score au score qu'a la négation de la phrase testée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de la phrase: (0.12241263440860216, 0.019791666666666666, 0.6077956989247312)\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Clean technology doesn't promotes sustainable development.\"\n",
    "score_phrase = analyze_sentence(phrase)\n",
    "print(\"Score de la phrase:\", score_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse : Résultats moins encourageant. On observe en effet une baisse du taux de positivité, qui passe de 0.16 à 0.12, en revanche, le taux de négativité n'a pas augmenté, et reste faible, alors même que la phrase semble négative\n",
    "\n",
    "On test sur une autre phrase négative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de la phrase: (0.0078125, 0.046875, 0.1953125)\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Pfizer destroyes environement.\"\n",
    "score_phrase = analyze_sentence(phrase)\n",
    "print(\"Score de la phrase:\", score_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score positif très faible, mais le négatif également. Est-ce intéressant de faire un rapport des deux ? On reste incertain quant à la significativité de la construction de notre note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_jSzcowY2Uy"
   },
   "source": [
    "On suppose que l'on a un dictionnaire `Dico_env` contenant les mots environnementaux, associés avec un score $\\in [-1,1]$. Par ex: {'pollution': -1, 'conservation': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC0efxYkYiJA"
   },
   "source": [
    "#### Colonne environmental_sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wQrq34eQRM11"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Nombre de mots</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Titre</th>\n",
       "      <th>ID</th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>environmental_sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nick Tabor</td>\n",
       "      <td>529</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Copyright 2023 The New York Times Company.  Al...</td>\n",
       "      <td>Document NYTF000020240104ejcv0000d</td>\n",
       "      <td>metropolitan desk sectmb ambitious public univ...</td>\n",
       "      <td>0.080807</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Wesley Morris</td>\n",
       "      <td>422</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>When Jim Brown and Raquel Welch, Two Sexy Star...</td>\n",
       "      <td>Document NYTF000020231231ejcv0006h</td>\n",
       "      <td>magazine desk sectmm jim brown raquel welch tw...</td>\n",
       "      <td>0.096043</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nTalking During Movi...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>179</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Talking During Movies: Totally Evil or Part of...</td>\n",
       "      <td>Document NYTF000020231231ejcv00064\\n</td>\n",
       "      <td>magazine desk sectmk talking movie totally evi...</td>\n",
       "      <td>0.129610</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>454</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Let Kids Vote!</td>\n",
       "      <td>Document NYTF000020231231ejcv00063\\n</td>\n",
       "      <td>magazine desk sectmk let kid vote 454 word 31 ...</td>\n",
       "      <td>0.068136</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Christina Caron</td>\n",
       "      <td>428</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Are We Doomed to Disagree?</td>\n",
       "      <td>Document NYTF000020231231ejcv0005z\\n</td>\n",
       "      <td>magazine desk sectmk doomed disagree 428 word ...</td>\n",
       "      <td>0.081156</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nHello, Fourth Grade...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>319</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Hello, Fourth Graders! A Look Back at our Clas...</td>\n",
       "      <td>Document NYTF000020231231ejcv0005t\\n</td>\n",
       "      <td>magazine desk sectmk hello fourth grader look ...</td>\n",
       "      <td>0.153341</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\nForeign Desk; SECTA\\n72 of Our Favorite Fa...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Times Insider</td>\n",
       "      <td>914</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>72 of Our Favorite Facts From 2023</td>\n",
       "      <td>Document NYTF000020231231ejcv0005r\\n</td>\n",
       "      <td>foreign desk secta 72 favorite fact 2023 time ...</td>\n",
       "      <td>0.054622</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\nMoney and Business/Financial Desk; SECTBU\\...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Kashmir Hill</td>\n",
       "      <td>811</td>\n",
       "      <td>Stalker Under Your Hood</td>\n",
       "      <td>The Stalker Under Your Hood</td>\n",
       "      <td>Document NYTF000020231231ejcv0005n\\n</td>\n",
       "      <td>money businessfinancial desk sectbu stalker ho...</td>\n",
       "      <td>0.033175</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\nBook Review Desk; SECTBR\\nPaperback Row\\n\\...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Shreya Chattopadhyay</td>\n",
       "      <td>431</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Paperback Row</td>\n",
       "      <td>Document NYTF000020231231ejcv0005g\\n</td>\n",
       "      <td>book review desk sectbr paperback row shreya c...</td>\n",
       "      <td>0.127856</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n\\nNICHOLAS KRISTOF\\nEditorial Desk; SECTSR\\n...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nicholas Kristof</td>\n",
       "      <td>976</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Humans Made Progress In 2023</td>\n",
       "      <td>Document NYTF000020231231ejcv00052\\n</td>\n",
       "      <td>nicholas kristof editorial desk sectsr human m...</td>\n",
       "      <td>0.011590</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article              Date  \\\n",
       "0  \\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...  31 December 2023   \n",
       "1  \\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...  31 December 2023   \n",
       "2  \\n\\nMagazine Desk; SECTMK\\nTalking During Movi...  31 December 2023   \n",
       "3  \\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...  31 December 2023   \n",
       "4  \\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...  31 December 2023   \n",
       "5  \\n\\nMagazine Desk; SECTMK\\nHello, Fourth Grade...  31 December 2023   \n",
       "6  \\n\\nForeign Desk; SECTA\\n72 of Our Favorite Fa...  31 December 2023   \n",
       "7  \\n\\nMoney and Business/Financial Desk; SECTBU\\...  31 December 2023   \n",
       "8  \\n\\nBook Review Desk; SECTBR\\nPaperback Row\\n\\...  31 December 2023   \n",
       "9  \\n\\nNICHOLAS KRISTOF\\nEditorial Desk; SECTSR\\n...  31 December 2023   \n",
       "\n",
       "                 Auteur  Nombre de mots                  Journal  \\\n",
       "0            Nick Tabor             529           New York Times   \n",
       "1         Wesley Morris             422           New York Times   \n",
       "2                  None             179           New York Times   \n",
       "3                  None             454           New York Times   \n",
       "4       Christina Caron             428           New York Times   \n",
       "5                  None             319           New York Times   \n",
       "6         Times Insider             914           New York Times   \n",
       "7          Kashmir Hill             811  Stalker Under Your Hood   \n",
       "8  Shreya Chattopadhyay             431           New York Times   \n",
       "9      Nicholas Kristof             976           New York Times   \n",
       "\n",
       "                                               Titre  \\\n",
       "0  Copyright 2023 The New York Times Company.  Al...   \n",
       "1  When Jim Brown and Raquel Welch, Two Sexy Star...   \n",
       "2  Talking During Movies: Totally Evil or Part of...   \n",
       "3                                     Let Kids Vote!   \n",
       "4                         Are We Doomed to Disagree?   \n",
       "5  Hello, Fourth Graders! A Look Back at our Clas...   \n",
       "6                 72 of Our Favorite Facts From 2023   \n",
       "7                        The Stalker Under Your Hood   \n",
       "8                                      Paperback Row   \n",
       "9                       Humans Made Progress In 2023   \n",
       "\n",
       "                                      ID  \\\n",
       "0     Document NYTF000020240104ejcv0000d   \n",
       "1     Document NYTF000020231231ejcv0006h   \n",
       "2  Document NYTF000020231231ejcv00064\\n    \n",
       "3  Document NYTF000020231231ejcv00063\\n    \n",
       "4  Document NYTF000020231231ejcv0005z\\n    \n",
       "5  Document NYTF000020231231ejcv0005t\\n    \n",
       "6  Document NYTF000020231231ejcv0005r\\n    \n",
       "7  Document NYTF000020231231ejcv0005n\\n    \n",
       "8  Document NYTF000020231231ejcv0005g\\n    \n",
       "9  Document NYTF000020231231ejcv00052\\n    \n",
       "\n",
       "                                Preprocessed_Article  Sentiment  \\\n",
       "0  metropolitan desk sectmb ambitious public univ...   0.080807   \n",
       "1  magazine desk sectmm jim brown raquel welch tw...   0.096043   \n",
       "2  magazine desk sectmk talking movie totally evi...   0.129610   \n",
       "3  magazine desk sectmk let kid vote 454 word 31 ...   0.068136   \n",
       "4  magazine desk sectmk doomed disagree 428 word ...   0.081156   \n",
       "5  magazine desk sectmk hello fourth grader look ...   0.153341   \n",
       "6  foreign desk secta 72 favorite fact 2023 time ...   0.054622   \n",
       "7  money businessfinancial desk sectbu stalker ho...   0.033175   \n",
       "8  book review desk sectbr paperback row shreya c...   0.127856   \n",
       "9  nicholas kristof editorial desk sectsr human m...   0.011590   \n",
       "\n",
       "   environmental_sentiment_score  \n",
       "0                            0.0  \n",
       "1                            0.0  \n",
       "2                            0.0  \n",
       "3                            0.0  \n",
       "4                            0.0  \n",
       "5                            0.0  \n",
       "6                            0.0  \n",
       "7                            0.0  \n",
       "8                            0.0  \n",
       "9                            0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_environmental_score(token_list, Dico_env):\n",
    "    score = 0\n",
    "    token_count = len(token_list)\n",
    "\n",
    "    for token in token_list:\n",
    "        if token in Dico_env:\n",
    "            score += Dico_env[token]\n",
    "\n",
    "    # Normalize the score to be between -1 and 1\n",
    "    if token_count > 0:\n",
    "        normalized_score = score / token_count\n",
    "        return max(min(normalized_score, 1), -1)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data['environmental_sentiment_score'] = data['Preprocessed_Article'].apply(lambda x: get_environmental_score(x, Dico_env_en))\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     False\n",
      "1     False\n",
      "2     False\n",
      "3     False\n",
      "4     False\n",
      "      ...  \n",
      "95    False\n",
      "96    False\n",
      "97    False\n",
      "98    False\n",
      "99    False\n",
      "Name: environmental_sentiment_score, Length: 100, dtype: bool\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PX7KM0duezTz"
   },
   "source": [
    "## Idée avec une base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkb1ZHzje3fw"
   },
   "source": [
    "Si on a une base de données qui contient un certain nomre d'articles deja labellisés avec une note environnementale, on pourrait entrainer un modèle de machine learning plus traditionnel que de l'analyse de sentiment.\n",
    "\n",
    "en effet, on peut passer par de la vectorization des mots par TF-IDF (ou autre - à voir), puis entrainer un modèle de régression linéaire (ou autre - à voir), et prédire pour les nouveaux articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYxpksZle1d_"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dataset['text'])  # 'texts' is the column with your text data\n",
    "y = dataset['scores']  # 'scores' is the column with your positivity scores\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "new_text = vectorizer.transform([\"New text\"])\n",
    "new_score = model.predict(new_text)\n",
    "print(f'Predicted Sentiment Score: {new_score}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
