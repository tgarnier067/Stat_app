{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gc-N_bxnObwn",
    "outputId": "a8cb318d-08bc-4edd-be5c-c51a68b88c5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.24.2 in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis) (1.26.3)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis) (1.11.4)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis) (2.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis) (1.3.2)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis) (3.1.2)\n",
      "Collecting numexpr (from pyLDAvis)\n",
      "  Downloading numexpr-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
      "Collecting funcy (from pyLDAvis)\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from pyLDAvis) (1.3.2)\n",
      "Collecting gensim (from pyLDAvis)\n",
      "  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyLDAvis) (68.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.2.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim->pyLDAvis)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->pyLDAvis) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numexpr-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (375 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.2/375.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: funcy, smart-open, numexpr, gensim, pyLDAvis\n",
      "Successfully installed funcy-2.0 gensim-4.3.2 numexpr-2.9.0 pyLDAvis-3.4.1 smart-open-6.4.0\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m757.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 regex-2023.12.25 tqdm-4.66.1\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/codespace/.local/lib/python3.10/site-packages (from wordcloud) (1.26.3)\n",
      "Requirement already satisfied: pillow in /home/codespace/.local/lib/python3.10/site-packages (from wordcloud) (10.2.0)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.10/site-packages (from wordcloud) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.1/511.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.3\n",
      "Collecting TextBlob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from TextBlob) (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (4.66.1)\n",
      "Installing collected packages: TextBlob\n",
      "Successfully installed TextBlob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis\n",
    "!pip install nltk\n",
    "!pip install wordcloud\n",
    "!pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mSOVtiqNKtK",
    "outputId": "06b329e1-9b0e-41db-8110-1b4af46507d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/genesis.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "#from IPython.display import display\n",
    "import base64\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from time import time\n",
    "# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3JH1r8K5gMe"
   },
   "source": [
    "# Récupération des communications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi6ZdjmlEK32"
   },
   "source": [
    "## Webscrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour voir notre travail de webscrapping, on pourra se référer au notebook nommé \"Essaie webscrapp.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Une première base de donnée : Une centaine d'articles du NYT et du WSJ avec le mot clef environnement sur les derniers jours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger à partir du fichier pickle\n",
    "data = pd.read_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUFpMBn5EZk8"
   },
   "source": [
    "# Traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAfkJlueLLR2"
   },
   "source": [
    "## Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Minuscule\n",
    "    text = text.lower()\n",
    "    # Supprimer la ponctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    # Suppression des stop-words\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_output = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "    \n",
    "    return lemmatized_output\n",
    "\n",
    "# Appliquer la fonction preprocess_text à la colonne 'Article'\n",
    "data['Preprocessed_Article'] = data['Article'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Nombre de mots</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Titre</th>\n",
       "      <th>ID</th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nick Tabor</td>\n",
       "      <td>529</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Copyright 2023 The New York Times Company.  Al...</td>\n",
       "      <td>Document NYTF000020240104ejcv0000d</td>\n",
       "      <td>[metropolitan, desk, sectmb, ambitious, public...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Wesley Morris</td>\n",
       "      <td>422</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>When Jim Brown and Raquel Welch, Two Sexy Star...</td>\n",
       "      <td>Document NYTF000020231231ejcv0006h</td>\n",
       "      <td>[magazine, desk, sectmm, jim, brown, raquel, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nTalking During Movi...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>179</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Talking During Movies: Totally Evil or Part of...</td>\n",
       "      <td>Document NYTF000020231231ejcv00064\\n</td>\n",
       "      <td>[magazine, desk, sectmk, talking, movie, total...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>454</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Let Kids Vote!</td>\n",
       "      <td>Document NYTF000020231231ejcv00063\\n</td>\n",
       "      <td>[magazine, desk, sectmk, let, kid, vote, 454, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Christina Caron</td>\n",
       "      <td>428</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Are We Doomed to Disagree?</td>\n",
       "      <td>Document NYTF000020231231ejcv0005z\\n</td>\n",
       "      <td>[magazine, desk, sectmk, doomed, disagree, 428...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nHello, Fourth Grade...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>319</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Hello, Fourth Graders! A Look Back at our Clas...</td>\n",
       "      <td>Document NYTF000020231231ejcv0005t\\n</td>\n",
       "      <td>[magazine, desk, sectmk, hello, fourth, grader...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\nForeign Desk; SECTA\\n72 of Our Favorite Fa...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Times Insider</td>\n",
       "      <td>914</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>72 of Our Favorite Facts From 2023</td>\n",
       "      <td>Document NYTF000020231231ejcv0005r\\n</td>\n",
       "      <td>[foreign, desk, secta, 72, favorite, fact, 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\nMoney and Business/Financial Desk; SECTBU\\...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Kashmir Hill</td>\n",
       "      <td>811</td>\n",
       "      <td>Stalker Under Your Hood</td>\n",
       "      <td>The Stalker Under Your Hood</td>\n",
       "      <td>Document NYTF000020231231ejcv0005n\\n</td>\n",
       "      <td>[money, businessfinancial, desk, sectbu, stalk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\nBook Review Desk; SECTBR\\nPaperback Row\\n\\...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Shreya Chattopadhyay</td>\n",
       "      <td>431</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Paperback Row</td>\n",
       "      <td>Document NYTF000020231231ejcv0005g\\n</td>\n",
       "      <td>[book, review, desk, sectbr, paperback, row, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n\\nNICHOLAS KRISTOF\\nEditorial Desk; SECTSR\\n...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nicholas Kristof</td>\n",
       "      <td>976</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Humans Made Progress In 2023</td>\n",
       "      <td>Document NYTF000020231231ejcv00052\\n</td>\n",
       "      <td>[nicholas, kristof, editorial, desk, sectsr, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article              Date  \\\n",
       "0  \\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...  31 December 2023   \n",
       "1  \\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...  31 December 2023   \n",
       "2  \\n\\nMagazine Desk; SECTMK\\nTalking During Movi...  31 December 2023   \n",
       "3  \\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...  31 December 2023   \n",
       "4  \\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...  31 December 2023   \n",
       "5  \\n\\nMagazine Desk; SECTMK\\nHello, Fourth Grade...  31 December 2023   \n",
       "6  \\n\\nForeign Desk; SECTA\\n72 of Our Favorite Fa...  31 December 2023   \n",
       "7  \\n\\nMoney and Business/Financial Desk; SECTBU\\...  31 December 2023   \n",
       "8  \\n\\nBook Review Desk; SECTBR\\nPaperback Row\\n\\...  31 December 2023   \n",
       "9  \\n\\nNICHOLAS KRISTOF\\nEditorial Desk; SECTSR\\n...  31 December 2023   \n",
       "\n",
       "                 Auteur  Nombre de mots                  Journal  \\\n",
       "0            Nick Tabor             529           New York Times   \n",
       "1         Wesley Morris             422           New York Times   \n",
       "2                  None             179           New York Times   \n",
       "3                  None             454           New York Times   \n",
       "4       Christina Caron             428           New York Times   \n",
       "5                  None             319           New York Times   \n",
       "6         Times Insider             914           New York Times   \n",
       "7          Kashmir Hill             811  Stalker Under Your Hood   \n",
       "8  Shreya Chattopadhyay             431           New York Times   \n",
       "9      Nicholas Kristof             976           New York Times   \n",
       "\n",
       "                                               Titre  \\\n",
       "0  Copyright 2023 The New York Times Company.  Al...   \n",
       "1  When Jim Brown and Raquel Welch, Two Sexy Star...   \n",
       "2  Talking During Movies: Totally Evil or Part of...   \n",
       "3                                     Let Kids Vote!   \n",
       "4                         Are We Doomed to Disagree?   \n",
       "5  Hello, Fourth Graders! A Look Back at our Clas...   \n",
       "6                 72 of Our Favorite Facts From 2023   \n",
       "7                        The Stalker Under Your Hood   \n",
       "8                                      Paperback Row   \n",
       "9                       Humans Made Progress In 2023   \n",
       "\n",
       "                                      ID  \\\n",
       "0     Document NYTF000020240104ejcv0000d   \n",
       "1     Document NYTF000020231231ejcv0006h   \n",
       "2  Document NYTF000020231231ejcv00064\\n    \n",
       "3  Document NYTF000020231231ejcv00063\\n    \n",
       "4  Document NYTF000020231231ejcv0005z\\n    \n",
       "5  Document NYTF000020231231ejcv0005t\\n    \n",
       "6  Document NYTF000020231231ejcv0005r\\n    \n",
       "7  Document NYTF000020231231ejcv0005n\\n    \n",
       "8  Document NYTF000020231231ejcv0005g\\n    \n",
       "9  Document NYTF000020231231ejcv00052\\n    \n",
       "\n",
       "                                Preprocessed_Article  \n",
       "0  [metropolitan, desk, sectmb, ambitious, public...  \n",
       "1  [magazine, desk, sectmm, jim, brown, raquel, w...  \n",
       "2  [magazine, desk, sectmk, talking, movie, total...  \n",
       "3  [magazine, desk, sectmk, let, kid, vote, 454, ...  \n",
       "4  [magazine, desk, sectmk, doomed, disagree, 428...  \n",
       "5  [magazine, desk, sectmk, hello, fourth, grader...  \n",
       "6  [foreign, desk, secta, 72, favorite, fact, 202...  \n",
       "7  [money, businessfinancial, desk, sectbu, stalk...  \n",
       "8  [book, review, desk, sectbr, paperback, row, s...  \n",
       "9  [nicholas, kristof, editorial, desk, sectsr, h...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA99lQ4VRIPj"
   },
   "source": [
    "## Analyse du sentiment des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EApSppc3ROou"
   },
   "source": [
    "### Sentiment général"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6163XiZXN2i"
   },
   "source": [
    "Le score donné varie de -1 à 1 avec -1 comme la négativité maximale et 1 comme la positivité maximale. 0 pour dire que le texte est neutre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "3cIikpC8RVPd",
    "outputId": "09d001bc-f208-4a1e-a073-8981d04cba49"
   },
   "outputs": [],
   "source": [
    "def calculate_sentiment(word_list):\n",
    "    # Convertir la liste de mots en une chaîne de caractères\n",
    "    text = ' '.join(word_list)\n",
    "    # Création d'une instance TextBlob\n",
    "    analysis = TextBlob(text)\n",
    "    # Retourner la polarité\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "# Appliquer la fonction au DataFrame\n",
    "data['Sentiment'] = data['Preprocessed_Article'].apply(calculate_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.080807\n",
       "1     0.096043\n",
       "2     0.129610\n",
       "3     0.068136\n",
       "4     0.081156\n",
       "        ...   \n",
       "95    0.017752\n",
       "96    0.062612\n",
       "97   -0.002605\n",
       "98    0.129660\n",
       "99   -0.021999\n",
       "Name: Sentiment, Length: 100, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ7rBPftRPf3"
   },
   "source": [
    "### Sentiment environnemental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idéal serait de récupérer un dictionnaire pré existant, spécialisé dans l'évaluation de termes écologique, qui attribue une score à chaque terme. La difficulté à trouver ce type de dictionnaire nous mène dans un premier temps à creuser d'autres pistes de substitution. Nous verrons plus tard si nous réussissons à trouver un dictionnaire préexistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative aux dictionnaires pré-existants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problème pour l'amélioration du dictionnaire : on ne trouve pas de dictionnaire préexistant avec comme spécialité l'environnement. Deux options : \n",
    "\n",
    " - Améliorer notre dictionnaire fait main:\n",
    "   - Avantage : On peut contrôler le poid associé à chaque mot, dans la note\n",
    "   - Inconvéniant : COnstruction peu rigoureuse, on peut avoir oublié des mots\n",
    "  \n",
    " - Utiliser un dictionnaire généraliste :\n",
    "   - Avantage : Construction plus rigoureuse, moins de chance d'oublier certains termes\n",
    "   - Inconvénient : Pas de contrôle sur le poid des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Le dictionnaire fait main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le moment, on ne trouve pas de dictionnaire pré-existant, dont chaque terme peut être associé à une note environnementale. On propose donc de construire nous même un dictionnaire, un en français et l'autre en anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un dictionnaire de termes environnementaux positifs\n",
    "Dico_env_fr = {\n",
    "    \"propre\": 1,\n",
    "    \"écologique\": 1,\n",
    "    \"durable\": 1,\n",
    "    \"vert\": 1,\n",
    "    \"économie d'énergie\": 1,\n",
    "    \"renouvelable\": 1,\n",
    "    \"responsable\": 1,\n",
    "    \"conservation\": 1,\n",
    "    \"biodiversité\": 1,\n",
    "    \"sain\": 1,\n",
    "    \"bio\": 1,\n",
    "    \"éco-friendly\": 1,\n",
    "    \"respectueux de l'environnement\": 1,\n",
    "    \"efficace\": 1,\n",
    "    \"innovant\": 1,\n",
    "    \"éthique\": 1,\n",
    "    \"équitable\": 1,\n",
    "    \"efficience\": 1,\n",
    "    \"responsabilité sociale\": 1,\n",
    "    \"sustainable\": 1,\n",
    "    \"solidaire\": 1,\n",
    "    \"propagation consciente\": 1,\n",
    "    \"soutenable\": 1,\n",
    "    \"énergie propre\": 1,\n",
    "    \"énergie renouvelable\": 1,\n",
    "    \"recyclage\": 1,\n",
    "    \"efficacité énergétique\": 1,\n",
    "    \"économie circulaire\": 1,\n",
    "    \"énergie solaire\": 1,\n",
    "    \"énergie éolienne\": 1,\n",
    "    \"régénération\": 1,\n",
    "    \"préservation\": 1,\n",
    "    \"restauration\": 1,\n",
    "    \"réhabilitation\": 1,\n",
    "    \"récupération\": 1,\n",
    "    \"restaurateur\": 1,\n",
    "    \"régénérateur\": 1,\n",
    "    \"revitalisation\": 1,\n",
    "    \"positif\": 1,\n",
    "    \"bénéfique\": 1,\n",
    "    \"valorisation\": 1,\n",
    "    \"épanouissement\": 1,\n",
    "    \"amélioration continue\": 1,\n",
    "    \"prospérité\": 1,\n",
    "    \"harmonie\": 1,\n",
    "    \"intégrité\": 1,\n",
    "    \"consommation responsable\": 1,\n",
    "    \"éco-responsable\": 1,\n",
    "    \"éco-conscient\": 1,\n",
    "    \"durabilité\": 1,\n",
    "    \"récupérable\": 1,\n",
    "    \"énergie verte\": 1,\n",
    "    \"effet de serre\": 1,\n",
    "    \"éco-efficace\": 1,\n",
    "    \"éco-innovation\": 1,\n",
    "    \"bien-être\": 1,\n",
    "    \"éco-design\": 1,\n",
    "    \"agroécologie\": 1,\n",
    "    \"permaculture\": 1,\n",
    "    \"éco-citoyen\": 1,\n",
    "    \"carbone neutre\": 1,\n",
    "    \"zéro déchet\": 1,\n",
    "    \"biologique\": 1,\n",
    "    \"éco-label\": 1,\n",
    "    \"mobilité durable\": 1,\n",
    "    \"éco-tourisme\": 1,\n",
    "    \"éco-habitat\": 1,\n",
    "    \"consommation consciente\": 1,\n",
    "\n",
    "    \"pollution\": -1,\n",
    "    \"déchet\": -1,\n",
    "    \"déforestation\": -1,\n",
    "    \"émissions de gaz à effet de serre\": -1,\n",
    "    \"contamination\": -1,\n",
    "    \"destructeur\": -1,\n",
    "    \"irresponsable\": -1,\n",
    "    \"gaspillage\": -1,\n",
    "    \"nuisible\": -1,\n",
    "    \"toxique\": -1,\n",
    "    \"détérioration\": -1,\n",
    "    \"dégradation\": -1,\n",
    "    \"dommageable\": -1,\n",
    "    \"préjudiciable\": -1,\n",
    "    \"périlleux\": -1,\n",
    "    \"inquiétant\": -1,\n",
    "    \"catastrophique\": -1,\n",
    "    \"catastrophe\": -1,\n",
    "    \"dangereux\": -1,\n",
    "    \"menace\": -1,\n",
    "    \"risque\": -1,\n",
    "    \"nocif\": -1,\n",
    "    \"néfaste\": -1,\n",
    "    \"inadéquat\": -1,\n",
    "    \"inapproprié\": -1,\n",
    "    \"inopportun\": -1,\n",
    "    \"nuire\": -1,\n",
    "    \"endommagement\": -1,\n",
    "    \"dommages\": -1,\n",
    "    \"polluant\": -1,\n",
    "    \"polluer\": -1,\n",
    "    \"détériorer\": -1,\n",
    "    \"perturbation\": -1,\n",
    "    \"irrespectueux\": -1,\n",
    "    \"malveillant\": -1,\n",
    "    \"dégât\": -1,\n",
    "    \"agressif\": -1,\n",
    "    \"ravageur\": -1,\n",
    "    \"gâcher\": -1,\n",
    "    \"perturber\": -1,\n",
    "    \"endommager\": -1,\n",
    "    \"irréparable\": -1,\n",
    "    \"toxicité\": -1,\n",
    "    \"inacceptable\": -1,\n",
    "    \"dommage écologique\": -1,\n",
    "    \"abattage illégal\": -1,\n",
    "    \"surconsommation\": -1,\n",
    "    \"pillage des ressources\": -1,\n",
    "    \"dégradation de l'environnement\": -1,\n",
    "    \"espace naturel détruit\": -1,\n",
    "    \"exploitation excessive\": -1,\n",
    "    \"surexploitation\": -1,\n",
    "    \"réchauffement climatique\": -1,\n",
    "    \"déni environnemental\": -1,\n",
    "}\n",
    "liste_negation = [\"pas\", \"non\",\"jamais\", \"aucun\", \"nul\", \"rien\", \"personne\", \"négatif\", \"sans\", \"plus\", \"moins\"]\n",
    "\n",
    "liste_annulation_negation = [\"responsable\",\"à l'origine\",\"la source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dico_env_en = {\n",
    "    \n",
    "    \"clean\": 1,\n",
    "    \"ecological\": 1,\n",
    "    \"sustainable\": 1,\n",
    "    \"green\": 1,\n",
    "    \"energy-efficient\": 1,\n",
    "    \"renewable\": 1,\n",
    "    \"responsible\": 1,\n",
    "    \"conservation\": 1,\n",
    "    \"biodiversity\": 1,\n",
    "    \"healthy\": 1,\n",
    "    \"organic\": 1,\n",
    "    \"eco-friendly\": 1,\n",
    "    \"environmentally friendly\": 1,\n",
    "    \"efficient\": 1,\n",
    "    \"innovative\": 1,\n",
    "    \"ethical\": 1,\n",
    "    \"fair\": 1,\n",
    "    \"efficiency\": 1,\n",
    "    \"social responsibility\": 1,\n",
    "    \"sustainable\": 1,\n",
    "    \"solidarity\": 1,\n",
    "    \"conscious spreading\": 1,\n",
    "    \"sustainable\": 1,\n",
    "    \"clean energy\": 1,\n",
    "    \"renewable energy\": 1,\n",
    "    \"recycling\": 1,\n",
    "    \"energy efficiency\": 1,\n",
    "    \"circular economy\": 1,\n",
    "    \"solar energy\": 1,\n",
    "    \"wind energy\": 1,\n",
    "    \"regeneration\": 1,\n",
    "    \"preservation\": 1,\n",
    "    \"restoration\": 1,\n",
    "    \"rehabilitation\": 1,\n",
    "    \"recovery\": 1,\n",
    "    \"restorer\": 1,\n",
    "    \"regenerator\": 1,\n",
    "    \"revitalization\": 1,\n",
    "    \"positive\": 1,\n",
    "    \"beneficial\": 1,\n",
    "    \"valorization\": 1,\n",
    "    \"fulfillment\": 1,\n",
    "    \"continuous improvement\": 1,\n",
    "    \"prosperity\": 1,\n",
    "    \"harmony\": 1,\n",
    "    \"integrity\": 1,\n",
    "    \"responsible consumption\": 1,\n",
    "    \"eco-responsible\": 1,\n",
    "    \"eco-conscious\": 1,\n",
    "    \"sustainability\": 1,\n",
    "    \"recoverable\": 1,\n",
    "    \"green energy\": 1,\n",
    "    \"greenhouse effect\": 1,\n",
    "    \"eco-efficient\": 1,\n",
    "    \"eco-innovation\": 1,\n",
    "    \"well-being\": 1,\n",
    "    \"eco-design\": 1,\n",
    "    \"agroecology\": 1,\n",
    "    \"permaculture\": 1,\n",
    "    \"eco-citizen\": 1,\n",
    "    \"carbon neutral\": 1,\n",
    "    \"zero waste\": 1,\n",
    "    \"organic\": 1,\n",
    "    \"eco-label\": 1,\n",
    "    \"sustainable mobility\": 1,\n",
    "    \"eco-tourism\": 1,\n",
    "    \"eco-habitat\": 1,\n",
    "    \"conscious consumption\": 1,\n",
    "    \n",
    "    \"pollution\": -1,\n",
    "    \"waste\": -1,\n",
    "    \"deforestation\": -1,\n",
    "    \"greenhouse gas emissions\": -1,\n",
    "    \"contamination\": -1,\n",
    "    \"destructive\": -1,\n",
    "    \"irresponsible\": -1,\n",
    "    \"wasteful\": -1,\n",
    "    \"harmful\": -1,\n",
    "    \"toxic\": -1,\n",
    "    \"deterioration\": -1,\n",
    "    \"degradation\": -1,\n",
    "    \"damaging\": -1,\n",
    "    \"harmful\": -1,\n",
    "    \"perilous\": -1,\n",
    "    \"worrisome\": -1,\n",
    "    \"catastrophic\": -1,\n",
    "    \"catastrophe\": -1,\n",
    "    \"dangerous\": -1,\n",
    "    \"threat\": -1,\n",
    "    \"risk\": -1,\n",
    "    \"hazardous\": -1,\n",
    "    \"harmful\": -1,\n",
    "    \"inappropriate\": -1,\n",
    "    \"inadequate\": -1,\n",
    "    \"inappropriate\": -1,\n",
    "    \"harm\": -1,\n",
    "    \"damage\": -1,\n",
    "    \"pollutant\": -1,\n",
    "    \"pollute\": -1,\n",
    "    \"deteriorate\": -1,\n",
    "    \"disruption\": -1,\n",
    "    \"disrespectful\": -1,\n",
    "    \"malevolent\": -1,\n",
    "    \"damage\": -1,\n",
    "    \"aggressive\": -1,\n",
    "    \"ravager\": -1,\n",
    "    \"spoil\": -1,\n",
    "    \"disturb\": -1,\n",
    "    \"damage\": -1,\n",
    "    \"irreparable\": -1,\n",
    "    \"toxicity\": -1,\n",
    "    \"unacceptable\": -1,\n",
    "    \"ecological damage\": -1,\n",
    "    \"illegal logging\": -1,\n",
    "    \"overconsumption\": -1,\n",
    "    \"resource plundering\": -1,\n",
    "    \"environmental degradation\": -1,\n",
    "    \"destroyed natural habitat\": -1,\n",
    "    \"excessive exploitation\": -1,\n",
    "    \"overexploitation\": -1,\n",
    "    \"climate change\": -1,\n",
    "    \"environmental denial\": -1,\n",
    "}\n",
    "\n",
    "negation_list = [\"not\", \"no\", \"never\", \"none\", \"nil\", \"nothing\", \"nobody\", \"negative\", \"without\", \"more\", \"less\"]\n",
    "\n",
    "negation_cancellation_list = [\"responsible\", \"originally\", \"source\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dictionnaire généraliste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'explorer la puissance d'un dictionnaire pré exsitant, nous faisant le choix de considérer un dictionnaire pré-existant, même s'il n'est pas spécialisé dans l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "# Termes à examiner\n",
    "terms = [\"good\", \"bad\", \"environment\", \"technology\",\"greenhouse\",\"gases\",\"greenhouse gas\"]\n",
    "\n",
    "for term in terms:\n",
    "    # Obtenir les synsets associés au terme\n",
    "    synsets = list(swn.senti_synsets(term))\n",
    "\n",
    "    if synsets:\n",
    "        print(f\"Scores de sentiment pour le terme '{term}':\")\n",
    "        for synset in synsets:\n",
    "            print(f\"POS: {synset.pos_score()}, NEG: {synset.neg_score()}, OBJ: {synset.obj_score()}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Aucun synset trouvé pour le terme '{term}'.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse : On a l'avantage d'avoir trois notes, qui représentent la part de positivité, de négativité et de neutralité du mot => Avancé par rapport à ce qu'on avait proposé. De plus, on considère qu'il y a plusieurs sens à chaque mot, d'où le fait qu'il y ait plusieurs évaluation pour chaque terme\n",
    "Avantage ; l'algorithme choisi la note du mot en fonction du contexte ?\n",
    "Problème : les coefficients ne sont pas forcément bons, par exemple, gases n'est jamais négatif, toujours neutre... pourquoi ?\n",
    "\n",
    "On test maintenant sur une phrase entière :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_sentiment_scores(term):\n",
    "    synsets = list(swn.senti_synsets(term))\n",
    "    if synsets:\n",
    "        pos_score = sum(s.pos_score() for s in synsets) / len(synsets)\n",
    "        neg_score = sum(s.neg_score() for s in synsets) / len(synsets)\n",
    "        obj_score = sum(s.obj_score() for s in synsets) / len(synsets)\n",
    "\n",
    "        # Normaliser les scores\n",
    "        total_score = pos_score + neg_score + obj_score\n",
    "        if total_score != 0:\n",
    "            pos_score /= total_score\n",
    "            neg_score /= total_score\n",
    "            obj_score /= total_score\n",
    "\n",
    "        return pos_score, neg_score, obj_score\n",
    "    else:\n",
    "        return 0, 0, 0\n",
    "\n",
    "def analyze_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    scores = []\n",
    "\n",
    "    for token in tokens:\n",
    "        pos_score, neg_score, obj_score = get_sentiment_scores(token)\n",
    "        scores.append((pos_score, neg_score, obj_score))\n",
    "\n",
    "    # Calculer les scores moyens pour la phrase\n",
    "    avg_pos_score = sum(score[0] for score in scores) / len(scores)\n",
    "    avg_neg_score = sum(score[1] for score in scores) / len(scores)\n",
    "    avg_obj_score = sum(score[2] for score in scores) / len(scores)\n",
    "\n",
    "    return avg_pos_score, avg_neg_score, avg_obj_score\n",
    "\n",
    "# Exemple d'utilisation\n",
    "phrase = \"Clean technology promotes sustainable development.\"\n",
    "score_phrase = analyze_sentence(phrase)\n",
    "print(\"Score de la phrase:\", score_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque un taux de positivité de 16%, contre un taux de négativité de 2%, ainsi qu'un taux de neutralité de 65%. C'est un résultat relativement encourageant, étant donné qu'on a proposé une phrase à l'algorithme qui semblait être positive d'un point de vue environnemental.\n",
    "\n",
    "On compare ce score au score qu'a la négation de la phrase testée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"Clean technology doesn't promotes sustainable development.\"\n",
    "score_phrase = analyze_sentence(phrase)\n",
    "print(\"Score de la phrase:\", score_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse : Résultats moins encourageant. On observe en effet une baisse du taux de positivité, qui passe de 0.16 à 0.12, en revanche, le taux de négativité n'a pas augmenté, et reste faible, alors même que la phrase semble négative\n",
    "\n",
    "On test sur une autre phrase négative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"Pfizer destroyes environement.\"\n",
    "score_phrase = analyze_sentence(phrase)\n",
    "print(\"Score de la phrase:\", score_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score positif très faible, mais le négatif également. Est-ce intéressant de faire un rapport des deux ? On reste incertain quant à la significativité de la construction de notre note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_jSzcowY2Uy"
   },
   "source": [
    "On suppose que l'on a un dictionnaire `Dico_env` contenant les mots environnementaux, associés avec un score $\\in [-1,1]$. Par ex: {'pollution': -1, 'conservation': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC0efxYkYiJA"
   },
   "source": [
    "#### Colonne environmental_sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQrq34eQRM11"
   },
   "outputs": [],
   "source": [
    "def get_environmental_score(token_list, Dico_env):\n",
    "    score = 0\n",
    "    token_count = len(token_list)\n",
    "\n",
    "    for token in token_list:\n",
    "        if token in Dico_env:\n",
    "            score += Dico_env[token]\n",
    "\n",
    "    # Normalize the score to be between -1 and 1\n",
    "    if token_count > 0:\n",
    "        normalized_score = score / token_count\n",
    "        return max(min(normalized_score, 1), -1)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data['environmental_sentiment_score'] = data['Preprocessed_Article'].apply(lambda x: get_environmental_score(x, Dico_env_en))\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score environnemental semble être nul pour une partie des articles, on regarde si le score est parfois différent de 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somme_valeurs_absolues = data['environmental_sentiment_score'].abs().sum()\n",
    "print(\"La somme des valeurs absolue de la variable du score environnement est :\", somme_valeurs_absolues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui pourrait expliquer le fait qu'aucune / très peu de texte, ai une note différente de 0, est le fait que les éléments du dictionnaire n'ont pas été prétraité (tokenisation etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la fonction preprocess_text aux clés du dictionnaire\n",
    "preprocessed_dict_en = {preprocess_text(key): value for key, value in Dico_env_en.items()}\n",
    "preprocessed_dict_fr = {preprocess_text(key): value for key, value in Dico_env_fr.items()}\n",
    "\n",
    "\n",
    "# Affichage du dictionnaire après prétraitement des clés\n",
    "print(preprocessed_dict_en)\n",
    "print(preprocessed_dict_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention, il ne semble pas que la tokenisation ou la lemnisation ait fonctionné..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_environmental_score(token_list, Dico_env):\n",
    "    score = 0\n",
    "    token_count = len(token_list)\n",
    "\n",
    "    for token in token_list:\n",
    "        if token in Dico_env:\n",
    "            score += Dico_env[token]\n",
    "\n",
    "    # Normalize the score to be between -1 and 1\n",
    "    if token_count > 0:\n",
    "        normalized_score = score / token_count\n",
    "        return max(min(normalized_score, 1), -1)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data['environmental_sentiment_score'] = data['Preprocessed_Article'].apply(lambda x: get_environmental_score(x, preprocessed_dict_en))\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somme_valeurs_absolues = data['environmental_sentiment_score'].abs().sum()\n",
    "print(\"La somme des valeurs absolue de la variable du score environnement est :\", somme_valeurs_absolues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Est ce que les éléments de la colonne Processed_Article contiennent une liste de mots qui composent l'article, ou bien c'est un seul élément (une grande chaine de charactère) ? Si c'est une grande chaîne de caractère, ça peut poser problème car pour construire la variable de score envorionnementale, on compare un mot à un texte => Donc le score environnemental reste nul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Preprocessed_Article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Preprocessed_Article'][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les élements de la variable Processed_Article est donc une longue et unique chaîne de caractère. On créer un code qui prends une chaîne de caractère en entrée, et qui renvoie une liste de mots qui composent cet article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mots_dans_article(article):\n",
    "    # Diviser l'article en mots en utilisant l'espace comme délimiteur\n",
    "    mots = article.split()\n",
    "\n",
    "    # Retourner la liste des mots\n",
    "    return mots\n",
    "\n",
    "data['Preprocessed_Article_split']=data['Preprocessed_Article'].apply(lambda x: mots_dans_article(x))\n",
    "\n",
    "data['Preprocessed_Article_split']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['environmental_sentiment_score_test'] = data['Preprocessed_Article_split'].apply(lambda x: get_environmental_score(x, preprocessed_dict_en))\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amélioration du score environnemental => Certains score ne sont pas nul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On essaie de voir maintenant quelles entreprises ont été citées dans le texte :\n",
    "On peut supposer qu'on a une colonne avec toutes les entreprises qui ont été citées dans le texte, ou dans le titre : entr_citées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Charger le DataFrame des entreprises\n",
    "df_entreprises = pd.read_csv(\"Firms.csv\")\n",
    "\n",
    "# Mettre en minuscules les noms des entreprises\n",
    "df_entreprises['Company'] = df_entreprises['Company'].str.lower()\n",
    "\n",
    "# Copier la colonne 'Preprocessed_Article' du DataFrame 'data' dans un nouveau DataFrame\n",
    "df_articles = data[['Preprocessed_Article']].copy()\n",
    "\n",
    "# Mettre en minuscules le contenu de la colonne 'Preprocessed_Article'\n",
    "df_articles['Preprocessed_Article'] = df_articles['Preprocessed_Article'].apply(lambda x: x.lower())\n",
    "\n",
    "# Initialisation de la nouvelle colonne pour stocker les noms des entreprises citées dans chaque article\n",
    "df_articles['Entreprises_citées'] = ''\n",
    "\n",
    "# Boucle à travers les articles\n",
    "for index, article in df_articles.iterrows():\n",
    "    entreprises_citées = []\n",
    "    contenu_article = article['Preprocessed_Article']\n",
    "    \n",
    "    # Vérifier si le contenu de l'article est une chaîne de caractères\n",
    "    if isinstance(contenu_article, str):\n",
    "        # Vérifier la présence de chaque entreprise dans le contenu de l'article\n",
    "        for index_ent, entreprise in df_entreprises.iterrows():\n",
    "            nom_entreprise = entreprise['Company']\n",
    "            if isinstance(nom_entreprise, str) and nom_entreprise in contenu_article:\n",
    "                entreprises_citées.append(nom_entreprise)\n",
    "    \n",
    "    # Stocker les entreprises citées dans la nouvelle colonne\n",
    "    df_articles.at[index, 'Entreprises_citées'] = ', '.join(entreprises_citées)\n",
    "\n",
    "# Afficher les articles avec les entreprises citées\n",
    "print(df_articles[['Preprocessed_Article', 'Entreprises_citées']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche maintenant à associer à chaque article, une entreprise spécifique (même s'il est cité plusieurs entreprises)\n",
    "\n",
    "On part du principe qu'un article qui ne cite aucune entreprise n'est pas pertinent à étudier => il n'y aurait aucune communication verte\n",
    "Si jamais un article ne cite qu'une seule entreprise, alors il ne peut y avoir (ou pas) communication verte que sur cette entreprise (mais pas de communication verte de plusieurs entreprises dans un seul article)\n",
    "Si jamais il y a plusieurs entreprises citées dans un seul articles, alors on traitera ces données à part, en pensant qu'il peut y avoir plusieurs communications vertes, venant de différentes entreprises, et tout cela dans un seul article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter une colonne pour le nombre d'entreprises citées dans chaque article\n",
    "df_articles['Nombre_entreprises_citées'] = df_articles['Entreprises_citées'].apply(lambda x: x.count(',') + 1)\n",
    "\n",
    "# Compter le nombre d'articles par nombre d'entreprises citées\n",
    "comptage_entreprises = df_articles['Nombre_entreprises_citées'].value_counts()\n",
    "\n",
    "# Afficher le comptage des articles par nombre d'entreprises citées\n",
    "print(\"Nombre d'articles par nombre d'entreprises citées :\")\n",
    "print(comptage_entreprises)\n",
    "\n",
    "# Afficher le nombre d'articles qui ne citent aucune entreprise, qui en citent une seule, etc.\n",
    "print(\"\\nRésumé du nombre d'articles par nombre d'entreprises citées :\")\n",
    "print(\"Aucune entreprise citée :\", comptage_entreprises.get(0, 0))\n",
    "for i in range(1, max(comptage_entreprises.index) + 1):\n",
    "    print(f\"{i} entreprise(s) citée(s) :\", comptage_entreprises.get(i, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des données sans entreprises citées\n",
    "df_articles = df_articles[df_articles['Entreprises_citées'] != '']\n",
    "\n",
    "# Identification des articles avec une seule entreprise citée\n",
    "df_articles['Entreprise_unique'] = df_articles['Entreprises_citées'].apply(lambda x: x.split(',')[0] if ',' not in x else '')\n",
    "\n",
    "# Affichage des articles avec une seule entreprise citée\n",
    "articles_avec_entreprise_unique = df_articles[df_articles['Entreprise_unique'] != '']\n",
    "print(articles_avec_entreprise_unique[['Titre_article', 'Entreprise_unique']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkb1ZHzje3fw"
   },
   "source": [
    "Si on a une base de données qui contient un certain nomre d'articles deja labellisés avec une note environnementale, on pourrait entrainer un modèle de machine learning plus traditionnel que de l'analyse de sentiment.\n",
    "\n",
    "en effet, on peut passer par de la vectorization des mots par TF-IDF (ou autre - à voir), puis entrainer un modèle de régression linéaire (ou autre - à voir), et prédire pour les nouveaux articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYxpksZle1d_"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dataset['text'])  # 'texts' is the column with your text data\n",
    "y = dataset['scores']  # 'scores' is the column with your positivity scores\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "new_text = vectorizer.transform([\"New text\"])\n",
    "new_score = model.predict(new_text)\n",
    "print(f'Predicted Sentiment Score: {new_score}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
