{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gc-N_bxnObwn",
    "outputId": "a8cb318d-08bc-4edd-be5c-c51a68b88c5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.24.2 in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (1.26.3)\n",
      "Requirement already satisfied: scipy in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (1.12.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (1.3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (3.1.3)\n",
      "Collecting numexpr (from pyLDAvis)\n",
      "  Downloading numexpr-2.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
      "Collecting funcy (from pyLDAvis)\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (1.4.0)\n",
      "Requirement already satisfied: gensim in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (4.3.2)\n",
      "Requirement already satisfied: setuptools in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (68.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/mamba/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/mamba/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/mamba/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/mamba/lib/python3.11/site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/mamba/lib/python3.11/site-packages (from gensim->pyLDAvis) (6.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.11/site-packages (from jinja2->pyLDAvis) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Downloading numexpr-2.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (377 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.5/377.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: funcy, numexpr, pyLDAvis\n",
      "Successfully installed funcy-2.0 numexpr-2.9.0 pyLDAvis-3.4.1\n",
      "Requirement already satisfied: nltk in /opt/mamba/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/mamba/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/mamba/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/mamba/lib/python3.11/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/mamba/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/mamba/lib/python3.11/site-packages (from wordcloud) (1.26.3)\n",
      "Requirement already satisfied: pillow in /opt/mamba/lib/python3.11/site-packages (from wordcloud) (10.2.0)\n",
      "Requirement already satisfied: matplotlib in /opt/mamba/lib/python3.11/site-packages (from wordcloud) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/mamba/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/mamba/lib/python3.11/site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/mamba/lib/python3.11/site-packages (from matplotlib->wordcloud) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/mamba/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.11/site-packages (from matplotlib->wordcloud) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/mamba/lib/python3.11/site-packages (from matplotlib->wordcloud) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/mamba/lib/python3.11/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (548 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.3/548.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.3\n",
      "Collecting TextBlob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /opt/mamba/lib/python3.11/site-packages (from TextBlob) (3.8.1)\n",
      "Requirement already satisfied: click in /opt/mamba/lib/python3.11/site-packages (from nltk>=3.1->TextBlob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/mamba/lib/python3.11/site-packages (from nltk>=3.1->TextBlob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/mamba/lib/python3.11/site-packages (from nltk>=3.1->TextBlob) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/mamba/lib/python3.11/site-packages (from nltk>=3.1->TextBlob) (4.66.1)\n",
      "Installing collected packages: TextBlob\n",
      "Successfully installed TextBlob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis\n",
    "!pip install nltk\n",
    "!pip install wordcloud\n",
    "!pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mSOVtiqNKtK",
    "outputId": "06b329e1-9b0e-41db-8110-1b4af46507d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package genesis to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/genesis.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/onyxia/nltk_data...\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/onyxia/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "#from IPython.display import display\n",
    "import base64\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from time import time\n",
    "# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3JH1r8K5gMe"
   },
   "source": [
    "# Récupération des communications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi6ZdjmlEK32"
   },
   "source": [
    "## Webscrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour voir notre travail de webscrapping, on pourra se référer au notebook nommé \"Essaie webscrapp.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Une première base de donnée : Une centaine d'articles du NYT et du WSJ avec le mot clef environnement sur les derniers jours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger à partir du fichier pickle\n",
    "data = pd.read_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUFpMBn5EZk8"
   },
   "source": [
    "# Traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAfkJlueLLR2"
   },
   "source": [
    "## Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Minuscule\n",
    "    text = text.lower()\n",
    "    # Supprimer la ponctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    # Suppression des stop-words\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_output = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "    \n",
    "    return lemmatized_output\n",
    "\n",
    "# Appliquer la fonction preprocess_text à la colonne 'Article'\n",
    "data['Preprocessed_Article'] = data['Article'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Nombre de mots</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Titre</th>\n",
       "      <th>ID</th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nick Tabor</td>\n",
       "      <td>529</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Copyright 2023 The New York Times Company.  Al...</td>\n",
       "      <td>NYTF000020240104ejcv0000d</td>\n",
       "      <td>[metropolitan, desk, sectmb, ambitious, public...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Wesley Morris</td>\n",
       "      <td>422</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>When Jim Brown and Raquel Welch, Two Sexy Star...</td>\n",
       "      <td>NYTF000020231231ejcv0006h</td>\n",
       "      <td>[magazine, desk, sectmm, jim, brown, raquel, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nTalking During Movi...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>179</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Talking During Movies: Totally Evil or Part of...</td>\n",
       "      <td>NYTF000020231231ejcv00064</td>\n",
       "      <td>[magazine, desk, sectmk, talking, movie, total...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>454</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Let Kids Vote!</td>\n",
       "      <td>NYTF000020231231ejcv00063</td>\n",
       "      <td>[magazine, desk, sectmk, let, kid, vote, 454, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Christina Caron</td>\n",
       "      <td>428</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Are We Doomed to Disagree?</td>\n",
       "      <td>NYTF000020231231ejcv0005z</td>\n",
       "      <td>[magazine, desk, sectmk, doomed, disagree, 428...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nHello, Fourth Grade...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>319</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Hello, Fourth Graders! A Look Back at our Clas...</td>\n",
       "      <td>NYTF000020231231ejcv0005t</td>\n",
       "      <td>[magazine, desk, sectmk, hello, fourth, grader...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\nForeign Desk; SECTA\\n72 of Our Favorite Fa...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Times Insider</td>\n",
       "      <td>914</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>72 of Our Favorite Facts From 2023</td>\n",
       "      <td>NYTF000020231231ejcv0005r</td>\n",
       "      <td>[foreign, desk, secta, 72, favorite, fact, 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\nMoney and Business/Financial Desk; SECTBU\\...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Kashmir Hill</td>\n",
       "      <td>811</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>The Stalker Under Your Hood</td>\n",
       "      <td>NYTF000020231231ejcv0005n</td>\n",
       "      <td>[money, businessfinancial, desk, sectbu, stalk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\nBook Review Desk; SECTBR\\nPaperback Row\\n\\...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Shreya Chattopadhyay</td>\n",
       "      <td>431</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Paperback Row</td>\n",
       "      <td>NYTF000020231231ejcv0005g</td>\n",
       "      <td>[book, review, desk, sectbr, paperback, row, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n\\nNICHOLAS KRISTOF\\nEditorial Desk; SECTSR\\n...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nicholas Kristof</td>\n",
       "      <td>976</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Humans Made Progress In 2023</td>\n",
       "      <td>NYTF000020231231ejcv00052</td>\n",
       "      <td>[nicholas, kristof, editorial, desk, sectsr, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article              Date  \\\n",
       "0  \\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...  31 December 2023   \n",
       "1  \\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...  31 December 2023   \n",
       "2  \\n\\nMagazine Desk; SECTMK\\nTalking During Movi...  31 December 2023   \n",
       "3  \\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...  31 December 2023   \n",
       "4  \\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...  31 December 2023   \n",
       "5  \\n\\nMagazine Desk; SECTMK\\nHello, Fourth Grade...  31 December 2023   \n",
       "6  \\n\\nForeign Desk; SECTA\\n72 of Our Favorite Fa...  31 December 2023   \n",
       "7  \\n\\nMoney and Business/Financial Desk; SECTBU\\...  31 December 2023   \n",
       "8  \\n\\nBook Review Desk; SECTBR\\nPaperback Row\\n\\...  31 December 2023   \n",
       "9  \\n\\nNICHOLAS KRISTOF\\nEditorial Desk; SECTSR\\n...  31 December 2023   \n",
       "\n",
       "                 Auteur  Nombre de mots         Journal  \\\n",
       "0            Nick Tabor             529  New York Times   \n",
       "1         Wesley Morris             422  New York Times   \n",
       "2                  None             179  New York Times   \n",
       "3                  None             454  New York Times   \n",
       "4       Christina Caron             428  New York Times   \n",
       "5                  None             319  New York Times   \n",
       "6         Times Insider             914  New York Times   \n",
       "7          Kashmir Hill             811  New York Times   \n",
       "8  Shreya Chattopadhyay             431  New York Times   \n",
       "9      Nicholas Kristof             976  New York Times   \n",
       "\n",
       "                                               Titre  \\\n",
       "0  Copyright 2023 The New York Times Company.  Al...   \n",
       "1  When Jim Brown and Raquel Welch, Two Sexy Star...   \n",
       "2  Talking During Movies: Totally Evil or Part of...   \n",
       "3                                     Let Kids Vote!   \n",
       "4                         Are We Doomed to Disagree?   \n",
       "5  Hello, Fourth Graders! A Look Back at our Clas...   \n",
       "6                 72 of Our Favorite Facts From 2023   \n",
       "7                        The Stalker Under Your Hood   \n",
       "8                                      Paperback Row   \n",
       "9                       Humans Made Progress In 2023   \n",
       "\n",
       "                            ID  \\\n",
       "0    NYTF000020240104ejcv0000d   \n",
       "1    NYTF000020231231ejcv0006h   \n",
       "2   NYTF000020231231ejcv00064    \n",
       "3   NYTF000020231231ejcv00063    \n",
       "4   NYTF000020231231ejcv0005z    \n",
       "5   NYTF000020231231ejcv0005t    \n",
       "6   NYTF000020231231ejcv0005r    \n",
       "7   NYTF000020231231ejcv0005n    \n",
       "8   NYTF000020231231ejcv0005g    \n",
       "9   NYTF000020231231ejcv00052    \n",
       "\n",
       "                                Preprocessed_Article  \n",
       "0  [metropolitan, desk, sectmb, ambitious, public...  \n",
       "1  [magazine, desk, sectmm, jim, brown, raquel, w...  \n",
       "2  [magazine, desk, sectmk, talking, movie, total...  \n",
       "3  [magazine, desk, sectmk, let, kid, vote, 454, ...  \n",
       "4  [magazine, desk, sectmk, doomed, disagree, 428...  \n",
       "5  [magazine, desk, sectmk, hello, fourth, grader...  \n",
       "6  [foreign, desk, secta, 72, favorite, fact, 202...  \n",
       "7  [money, businessfinancial, desk, sectbu, stalk...  \n",
       "8  [book, review, desk, sectbr, paperback, row, s...  \n",
       "9  [nicholas, kristof, editorial, desk, sectsr, h...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA99lQ4VRIPj"
   },
   "source": [
    "## Analyse du sentiment des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EApSppc3ROou"
   },
   "source": [
    "### Sentiment général"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6163XiZXN2i"
   },
   "source": [
    "Le score donné varie de -1 à 1 avec -1 comme la négativité maximale et 1 comme la positivité maximale. 0 pour dire que le texte est neutre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "3cIikpC8RVPd",
    "outputId": "09d001bc-f208-4a1e-a073-8981d04cba49"
   },
   "outputs": [],
   "source": [
    "def calculate_sentiment(word_list):\n",
    "    # Convertir la liste de mots en une chaîne de caractères\n",
    "    text = ' '.join(word_list)\n",
    "    # Création d'une instance TextBlob\n",
    "    analysis = TextBlob(text)\n",
    "    # Retourner la polarité\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "# Appliquer la fonction au DataFrame\n",
    "data['Sentiment'] = data['Preprocessed_Article'].apply(calculate_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.080807\n",
       "1     0.096043\n",
       "2     0.129610\n",
       "3     0.068136\n",
       "4     0.081156\n",
       "        ...   \n",
       "95    0.017752\n",
       "96    0.062612\n",
       "97   -0.002605\n",
       "98    0.129660\n",
       "99   -0.021999\n",
       "Name: Sentiment, Length: 100, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ7rBPftRPf3"
   },
   "source": [
    "### Sentiment environnemental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idéal serait de récupérer un dictionnaire pré existant, spécialisé dans l'évaluation de termes écologique, qui attribue une score à chaque terme. La difficulté à trouver ce type de dictionnaire nous mène dans un premier temps à creuser d'autres pistes de substitution. Nous verrons plus tard si nous réussissons à trouver un dictionnaire préexistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative aux dictionnaires pré-existants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problème pour l'amélioration du dictionnaire : on ne trouve pas de dictionnaire préexistant avec comme spécialité l'environnement. Deux options : \n",
    "\n",
    " - Améliorer notre dictionnaire fait main:\n",
    "   - Avantage : On peut contrôler le poid associé à chaque mot, dans la note\n",
    "   - Inconvéniant : COnstruction peu rigoureuse, on peut avoir oublié des mots\n",
    "  \n",
    " - Utiliser un dictionnaire généraliste :\n",
    "   - Avantage : Construction plus rigoureuse, moins de chance d'oublier certains termes\n",
    "   - Inconvénient : Pas de contrôle sur le poid des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Le dictionnaire fait main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le moment, on ne trouve pas de dictionnaire pré-existant, dont chaque terme peut être associé à une note environnementale. On propose donc de construire nous même un dictionnaire, un en français et l'autre en anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un dictionnaire de termes environnementaux positifs\n",
    "Dico_env_fr = {\n",
    "    \"propre\": 1,\n",
    "    \"écologique\": 1,\n",
    "    \"durable\": 1,\n",
    "    \"vert\": 1,\n",
    "    \"économie d'énergie\": 1,\n",
    "    \"renouvelable\": 1,\n",
    "    \"responsable\": 1,\n",
    "    \"conservation\": 1,\n",
    "    \"biodiversité\": 1,\n",
    "    \"sain\": 1,\n",
    "    \"bio\": 1,\n",
    "    \"éco-friendly\": 1,\n",
    "    \"respectueux de l'environnement\": 1,\n",
    "    \"efficace\": 1,\n",
    "    \"innovant\": 1,\n",
    "    \"éthique\": 1,\n",
    "    \"équitable\": 1,\n",
    "    \"efficience\": 1,\n",
    "    \"responsabilité sociale\": 1,\n",
    "    \"sustainable\": 1,\n",
    "    \"solidaire\": 1,\n",
    "    \"propagation consciente\": 1,\n",
    "    \"soutenable\": 1,\n",
    "    \"énergie propre\": 1,\n",
    "    \"énergie renouvelable\": 1,\n",
    "    \"recyclage\": 1,\n",
    "    \"efficacité énergétique\": 1,\n",
    "    \"économie circulaire\": 1,\n",
    "    \"énergie solaire\": 1,\n",
    "    \"énergie éolienne\": 1,\n",
    "    \"régénération\": 1,\n",
    "    \"préservation\": 1,\n",
    "    \"restauration\": 1,\n",
    "    \"réhabilitation\": 1,\n",
    "    \"récupération\": 1,\n",
    "    \"restaurateur\": 1,\n",
    "    \"régénérateur\": 1,\n",
    "    \"revitalisation\": 1,\n",
    "    \"positif\": 1,\n",
    "    \"bénéfique\": 1,\n",
    "    \"valorisation\": 1,\n",
    "    \"épanouissement\": 1,\n",
    "    \"amélioration continue\": 1,\n",
    "    \"prospérité\": 1,\n",
    "    \"harmonie\": 1,\n",
    "    \"intégrité\": 1,\n",
    "    \"consommation responsable\": 1,\n",
    "    \"éco-responsable\": 1,\n",
    "    \"éco-conscient\": 1,\n",
    "    \"durabilité\": 1,\n",
    "    \"récupérable\": 1,\n",
    "    \"énergie verte\": 1,\n",
    "    \"effet de serre\": 1,\n",
    "    \"éco-efficace\": 1,\n",
    "    \"éco-innovation\": 1,\n",
    "    \"bien-être\": 1,\n",
    "    \"éco-design\": 1,\n",
    "    \"agroécologie\": 1,\n",
    "    \"permaculture\": 1,\n",
    "    \"éco-citoyen\": 1,\n",
    "    \"carbone neutre\": 1,\n",
    "    \"zéro déchet\": 1,\n",
    "    \"biologique\": 1,\n",
    "    \"éco-label\": 1,\n",
    "    \"mobilité durable\": 1,\n",
    "    \"éco-tourisme\": 1,\n",
    "    \"éco-habitat\": 1,\n",
    "    \"consommation consciente\": 1,\n",
    "\n",
    "    \"pollution\": -1,\n",
    "    \"déchet\": -1,\n",
    "    \"déforestation\": -1,\n",
    "    \"émissions de gaz à effet de serre\": -1,\n",
    "    \"contamination\": -1,\n",
    "    \"destructeur\": -1,\n",
    "    \"irresponsable\": -1,\n",
    "    \"gaspillage\": -1,\n",
    "    \"nuisible\": -1,\n",
    "    \"toxique\": -1,\n",
    "    \"détérioration\": -1,\n",
    "    \"dégradation\": -1,\n",
    "    \"dommageable\": -1,\n",
    "    \"préjudiciable\": -1,\n",
    "    \"périlleux\": -1,\n",
    "    \"inquiétant\": -1,\n",
    "    \"catastrophique\": -1,\n",
    "    \"catastrophe\": -1,\n",
    "    \"dangereux\": -1,\n",
    "    \"menace\": -1,\n",
    "    \"risque\": -1,\n",
    "    \"nocif\": -1,\n",
    "    \"néfaste\": -1,\n",
    "    \"inadéquat\": -1,\n",
    "    \"inapproprié\": -1,\n",
    "    \"inopportun\": -1,\n",
    "    \"nuire\": -1,\n",
    "    \"endommagement\": -1,\n",
    "    \"dommages\": -1,\n",
    "    \"polluant\": -1,\n",
    "    \"polluer\": -1,\n",
    "    \"détériorer\": -1,\n",
    "    \"perturbation\": -1,\n",
    "    \"irrespectueux\": -1,\n",
    "    \"malveillant\": -1,\n",
    "    \"dégât\": -1,\n",
    "    \"agressif\": -1,\n",
    "    \"ravageur\": -1,\n",
    "    \"gâcher\": -1,\n",
    "    \"perturber\": -1,\n",
    "    \"endommager\": -1,\n",
    "    \"irréparable\": -1,\n",
    "    \"toxicité\": -1,\n",
    "    \"inacceptable\": -1,\n",
    "    \"dommage écologique\": -1,\n",
    "    \"abattage illégal\": -1,\n",
    "    \"surconsommation\": -1,\n",
    "    \"pillage des ressources\": -1,\n",
    "    \"dégradation de l'environnement\": -1,\n",
    "    \"espace naturel détruit\": -1,\n",
    "    \"exploitation excessive\": -1,\n",
    "    \"surexploitation\": -1,\n",
    "    \"réchauffement climatique\": -1,\n",
    "    \"déni environnemental\": -1,\n",
    "}\n",
    "liste_negation = [\"pas\", \"non\",\"jamais\", \"aucun\", \"nul\", \"rien\", \"personne\", \"négatif\", \"sans\", \"plus\", \"moins\"]\n",
    "\n",
    "liste_annulation_negation = [\"responsable\",\"à l'origine\",\"la source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dico_env_en = {\n",
    "    \n",
    "    \"clean\": 1,\n",
    "    \"ecological\": 1,\n",
    "    \"sustainable\": 1,\n",
    "    \"green\": 1,\n",
    "    \"energy-efficient\": 1,\n",
    "    \"renewable\": 1,\n",
    "    \"responsible\": 1,\n",
    "    \"conservation\": 1,\n",
    "    \"biodiversity\": 1,\n",
    "    \"healthy\": 1,\n",
    "    \"organic\": 1,\n",
    "    \"eco-friendly\": 1,\n",
    "    \"environmentally friendly\": 1,\n",
    "    \"efficient\": 1,\n",
    "    \"innovative\": 1,\n",
    "    \"ethical\": 1,\n",
    "    \"fair\": 1,\n",
    "    \"efficiency\": 1,\n",
    "    \"social responsibility\": 1,\n",
    "    \"sustainable\": 1,\n",
    "    \"solidarity\": 1,\n",
    "    \"conscious spreading\": 1,\n",
    "    \"sustainable\": 1,\n",
    "    \"clean energy\": 1,\n",
    "    \"renewable energy\": 1,\n",
    "    \"recycling\": 1,\n",
    "    \"energy efficiency\": 1,\n",
    "    \"circular economy\": 1,\n",
    "    \"solar energy\": 1,\n",
    "    \"wind energy\": 1,\n",
    "    \"regeneration\": 1,\n",
    "    \"preservation\": 1,\n",
    "    \"restoration\": 1,\n",
    "    \"rehabilitation\": 1,\n",
    "    \"recovery\": 1,\n",
    "    \"restorer\": 1,\n",
    "    \"regenerator\": 1,\n",
    "    \"revitalization\": 1,\n",
    "    \"positive\": 1,\n",
    "    \"beneficial\": 1,\n",
    "    \"valorization\": 1,\n",
    "    \"fulfillment\": 1,\n",
    "    \"continuous improvement\": 1,\n",
    "    \"prosperity\": 1,\n",
    "    \"harmony\": 1,\n",
    "    \"integrity\": 1,\n",
    "    \"responsible consumption\": 1,\n",
    "    \"eco-responsible\": 1,\n",
    "    \"eco-conscious\": 1,\n",
    "    \"sustainability\": 1,\n",
    "    \"recoverable\": 1,\n",
    "    \"green energy\": 1,\n",
    "    \"greenhouse effect\": 1,\n",
    "    \"eco-efficient\": 1,\n",
    "    \"eco-innovation\": 1,\n",
    "    \"well-being\": 1,\n",
    "    \"eco-design\": 1,\n",
    "    \"agroecology\": 1,\n",
    "    \"permaculture\": 1,\n",
    "    \"eco-citizen\": 1,\n",
    "    \"carbon neutral\": 1,\n",
    "    \"zero waste\": 1,\n",
    "    \"organic\": 1,\n",
    "    \"eco-label\": 1,\n",
    "    \"sustainable mobility\": 1,\n",
    "    \"eco-tourism\": 1,\n",
    "    \"eco-habitat\": 1,\n",
    "    \"conscious consumption\": 1,\n",
    "    \n",
    "    \"pollution\": -1,\n",
    "    \"waste\": -1,\n",
    "    \"deforestation\": -1,\n",
    "    \"greenhouse gas emissions\": -1,\n",
    "    \"contamination\": -1,\n",
    "    \"destructive\": -1,\n",
    "    \"irresponsible\": -1,\n",
    "    \"wasteful\": -1,\n",
    "    \"harmful\": -1,\n",
    "    \"toxic\": -1,\n",
    "    \"deterioration\": -1,\n",
    "    \"degradation\": -1,\n",
    "    \"damaging\": -1,\n",
    "    \"harmful\": -1,\n",
    "    \"perilous\": -1,\n",
    "    \"worrisome\": -1,\n",
    "    \"catastrophic\": -1,\n",
    "    \"catastrophe\": -1,\n",
    "    \"dangerous\": -1,\n",
    "    \"threat\": -1,\n",
    "    \"risk\": -1,\n",
    "    \"hazardous\": -1,\n",
    "    \"harmful\": -1,\n",
    "    \"inappropriate\": -1,\n",
    "    \"inadequate\": -1,\n",
    "    \"inappropriate\": -1,\n",
    "    \"harm\": -1,\n",
    "    \"damage\": -1,\n",
    "    \"pollutant\": -1,\n",
    "    \"pollute\": -1,\n",
    "    \"deteriorate\": -1,\n",
    "    \"disruption\": -1,\n",
    "    \"disrespectful\": -1,\n",
    "    \"malevolent\": -1,\n",
    "    \"damage\": -1,\n",
    "    \"aggressive\": -1,\n",
    "    \"ravager\": -1,\n",
    "    \"spoil\": -1,\n",
    "    \"disturb\": -1,\n",
    "    \"damage\": -1,\n",
    "    \"irreparable\": -1,\n",
    "    \"toxicity\": -1,\n",
    "    \"unacceptable\": -1,\n",
    "    \"ecological damage\": -1,\n",
    "    \"illegal logging\": -1,\n",
    "    \"overconsumption\": -1,\n",
    "    \"resource plundering\": -1,\n",
    "    \"environmental degradation\": -1,\n",
    "    \"destroyed natural habitat\": -1,\n",
    "    \"excessive exploitation\": -1,\n",
    "    \"overexploitation\": -1,\n",
    "    \"climate change\": -1,\n",
    "    \"environmental denial\": -1,\n",
    "}\n",
    "\n",
    "negation_list = [\"not\", \"no\", \"never\", \"none\", \"nil\", \"nothing\", \"nobody\", \"negative\", \"without\", \"more\", \"less\"]\n",
    "\n",
    "negation_cancellation_list = [\"responsible\", \"originally\", \"source\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dictionnaire généraliste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'explorer la puissance d'un dictionnaire pré exsitant, nous faisant le choix de considérer un dictionnaire pré-existant, même s'il n'est pas spécialisé dans l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/onyxia/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de sentiment pour le terme 'good':\n",
      "POS: 0.5, NEG: 0.0, OBJ: 0.5\n",
      "POS: 0.875, NEG: 0.0, OBJ: 0.125\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.75, NEG: 0.0, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 1.0, NEG: 0.0, OBJ: 0.0\n",
      "POS: 1.0, NEG: 0.0, OBJ: 0.0\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 1.0, NEG: 0.0, OBJ: 0.0\n",
      "POS: 0.75, NEG: 0.0, OBJ: 0.25\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 0.5, NEG: 0.0, OBJ: 0.5\n",
      "POS: 0.5, NEG: 0.0, OBJ: 0.5\n",
      "POS: 0.375, NEG: 0.0, OBJ: 0.625\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 0.75, NEG: 0.0, OBJ: 0.25\n",
      "POS: 0.75, NEG: 0.0, OBJ: 0.25\n",
      "POS: 0.875, NEG: 0.0, OBJ: 0.125\n",
      "POS: 0.5, NEG: 0.0, OBJ: 0.5\n",
      "POS: 0.375, NEG: 0.125, OBJ: 0.5\n",
      "POS: 0.75, NEG: 0.0, OBJ: 0.25\n",
      "POS: 0.375, NEG: 0.0, OBJ: 0.625\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "\n",
      "Scores de sentiment pour le terme 'bad':\n",
      "POS: 0.0, NEG: 0.875, OBJ: 0.125\n",
      "POS: 0.0, NEG: 0.625, OBJ: 0.375\n",
      "POS: 0.25, NEG: 0.25, OBJ: 0.5\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.625, OBJ: 0.375\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.625, OBJ: 0.375\n",
      "POS: 0.0, NEG: 0.5, OBJ: 0.5\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 1.0, OBJ: 0.0\n",
      "POS: 0.0, NEG: 0.375, OBJ: 0.625\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.125, NEG: 0.25, OBJ: 0.625\n",
      "POS: 0.125, NEG: 0.25, OBJ: 0.625\n",
      "\n",
      "Scores de sentiment pour le terme 'environment':\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "\n",
      "Scores de sentiment pour le terme 'technology':\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "\n",
      "Scores de sentiment pour le terme 'greenhouse':\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "\n",
      "Scores de sentiment pour le terme 'gases':\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "\n",
      "Aucun synset trouvé pour le terme 'greenhouse gas'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "# Termes à examiner\n",
    "terms = [\"good\", \"bad\", \"environment\", \"technology\",\"greenhouse\",\"gases\",\"greenhouse gas\"]\n",
    "\n",
    "for term in terms:\n",
    "    # Obtenir les synsets associés au terme\n",
    "    synsets = list(swn.senti_synsets(term))\n",
    "\n",
    "    if synsets:\n",
    "        print(f\"Scores de sentiment pour le terme '{term}':\")\n",
    "        for synset in synsets:\n",
    "            print(f\"POS: {synset.pos_score()}, NEG: {synset.neg_score()}, OBJ: {synset.obj_score()}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Aucun synset trouvé pour le terme '{term}'.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse : On a l'avantage d'avoir trois notes, qui représentent la part de positivité, de négativité et de neutralité du mot => Avancé par rapport à ce qu'on avait proposé. De plus, on considère qu'il y a plusieurs sens à chaque mot, d'où le fait qu'il y ait plusieurs évaluation pour chaque terme\n",
    "Avantage ; l'algorithme choisi la note du mot en fonction du contexte ?\n",
    "Problème : les coefficients ne sont pas forcément bons, par exemple, gases n'est jamais négatif, toujours neutre... pourquoi ?\n",
    "\n",
    "On test maintenant sur une phrase entière :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de la phrase: (0.16182795698924732, 0.020833333333333332, 0.6506720430107527)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_sentiment_scores(term):\n",
    "    synsets = list(swn.senti_synsets(term))\n",
    "    if synsets:\n",
    "        pos_score = sum(s.pos_score() for s in synsets) / len(synsets)\n",
    "        neg_score = sum(s.neg_score() for s in synsets) / len(synsets)\n",
    "        obj_score = sum(s.obj_score() for s in synsets) / len(synsets)\n",
    "\n",
    "        # Normaliser les scores\n",
    "        total_score = pos_score + neg_score + obj_score\n",
    "        if total_score != 0:\n",
    "            pos_score /= total_score\n",
    "            neg_score /= total_score\n",
    "            obj_score /= total_score\n",
    "\n",
    "        return pos_score, neg_score, obj_score\n",
    "    else:\n",
    "        return 0, 0, 0\n",
    "\n",
    "def analyze_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    scores = []\n",
    "\n",
    "    for token in tokens:\n",
    "        pos_score, neg_score, obj_score = get_sentiment_scores(token)\n",
    "        scores.append((pos_score, neg_score, obj_score))\n",
    "\n",
    "    # Calculer les scores moyens pour la phrase\n",
    "    avg_pos_score = sum(score[0] for score in scores) / len(scores)\n",
    "    avg_neg_score = sum(score[1] for score in scores) / len(scores)\n",
    "    avg_obj_score = sum(score[2] for score in scores) / len(scores)\n",
    "\n",
    "    return avg_pos_score, avg_neg_score, avg_obj_score\n",
    "\n",
    "# Exemple d'utilisation\n",
    "phrase = \"Clean technology promotes sustainable development.\"\n",
    "score_phrase = analyze_sentence(phrase)\n",
    "print(\"Score de la phrase:\", score_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque un taux de positivité de 16%, contre un taux de négativité de 2%, ainsi qu'un taux de neutralité de 65%. C'est un résultat relativement encourageant, étant donné qu'on a proposé une phrase à l'algorithme qui semblait être positive d'un point de vue environnemental.\n",
    "\n",
    "On compare ce score au score qu'a la négation de la phrase testée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de la phrase: (0.12241263440860216, 0.019791666666666666, 0.6077956989247312)\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Clean technology doesn't promotes sustainable development.\"\n",
    "score_phrase = analyze_sentence(phrase)\n",
    "print(\"Score de la phrase:\", score_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse : Résultats moins encourageant. On observe en effet une baisse du taux de positivité, qui passe de 0.16 à 0.12, en revanche, le taux de négativité n'a pas augmenté, et reste faible, alors même que la phrase semble négative\n",
    "\n",
    "On test sur une autre phrase négative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de la phrase: (0.0078125, 0.046875, 0.1953125)\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Pfizer destroyes environement.\"\n",
    "score_phrase = analyze_sentence(phrase)\n",
    "print(\"Score de la phrase:\", score_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score positif très faible, mais le négatif également. Est-ce intéressant de faire un rapport des deux ? On reste incertain quant à la significativité de la construction de notre note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_jSzcowY2Uy"
   },
   "source": [
    "On suppose que l'on a un dictionnaire `Dico_env` contenant les mots environnementaux, associés avec un score $\\in [-1,1]$. Par ex: {'pollution': -1, 'conservation': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC0efxYkYiJA"
   },
   "source": [
    "#### Colonne environmental_sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wQrq34eQRM11"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Nombre de mots</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Titre</th>\n",
       "      <th>ID</th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>environmental_sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nick Tabor</td>\n",
       "      <td>529</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Copyright 2023 The New York Times Company.  Al...</td>\n",
       "      <td>NYTF000020240104ejcv0000d</td>\n",
       "      <td>[metropolitan, desk, sectmb, ambitious, public...</td>\n",
       "      <td>0.080807</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Wesley Morris</td>\n",
       "      <td>422</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>When Jim Brown and Raquel Welch, Two Sexy Star...</td>\n",
       "      <td>NYTF000020231231ejcv0006h</td>\n",
       "      <td>[magazine, desk, sectmm, jim, brown, raquel, w...</td>\n",
       "      <td>0.096043</td>\n",
       "      <td>0.001267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nTalking During Movi...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>179</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Talking During Movies: Totally Evil or Part of...</td>\n",
       "      <td>NYTF000020231231ejcv00064</td>\n",
       "      <td>[magazine, desk, sectmk, talking, movie, total...</td>\n",
       "      <td>0.129610</td>\n",
       "      <td>-0.007692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>454</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Let Kids Vote!</td>\n",
       "      <td>NYTF000020231231ejcv00063</td>\n",
       "      <td>[magazine, desk, sectmk, let, kid, vote, 454, ...</td>\n",
       "      <td>0.068136</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Christina Caron</td>\n",
       "      <td>428</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Are We Doomed to Disagree?</td>\n",
       "      <td>NYTF000020231231ejcv0005z</td>\n",
       "      <td>[magazine, desk, sectmk, doomed, disagree, 428...</td>\n",
       "      <td>0.081156</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nHello, Fourth Grade...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>319</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Hello, Fourth Graders! A Look Back at our Clas...</td>\n",
       "      <td>NYTF000020231231ejcv0005t</td>\n",
       "      <td>[magazine, desk, sectmk, hello, fourth, grader...</td>\n",
       "      <td>0.153341</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\nForeign Desk; SECTA\\n72 of Our Favorite Fa...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Times Insider</td>\n",
       "      <td>914</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>72 of Our Favorite Facts From 2023</td>\n",
       "      <td>NYTF000020231231ejcv0005r</td>\n",
       "      <td>[foreign, desk, secta, 72, favorite, fact, 202...</td>\n",
       "      <td>0.054622</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\nMoney and Business/Financial Desk; SECTBU\\...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Kashmir Hill</td>\n",
       "      <td>811</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>The Stalker Under Your Hood</td>\n",
       "      <td>NYTF000020231231ejcv0005n</td>\n",
       "      <td>[money, businessfinancial, desk, sectbu, stalk...</td>\n",
       "      <td>0.033175</td>\n",
       "      <td>-0.000993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\nBook Review Desk; SECTBR\\nPaperback Row\\n\\...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Shreya Chattopadhyay</td>\n",
       "      <td>431</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Paperback Row</td>\n",
       "      <td>NYTF000020231231ejcv0005g</td>\n",
       "      <td>[book, review, desk, sectbr, paperback, row, s...</td>\n",
       "      <td>0.127856</td>\n",
       "      <td>0.003534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n\\nNICHOLAS KRISTOF\\nEditorial Desk; SECTSR\\n...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nicholas Kristof</td>\n",
       "      <td>976</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Humans Made Progress In 2023</td>\n",
       "      <td>NYTF000020231231ejcv00052</td>\n",
       "      <td>[nicholas, kristof, editorial, desk, sectsr, h...</td>\n",
       "      <td>0.011590</td>\n",
       "      <td>-0.001842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article              Date  \\\n",
       "0  \\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...  31 December 2023   \n",
       "1  \\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...  31 December 2023   \n",
       "2  \\n\\nMagazine Desk; SECTMK\\nTalking During Movi...  31 December 2023   \n",
       "3  \\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...  31 December 2023   \n",
       "4  \\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...  31 December 2023   \n",
       "5  \\n\\nMagazine Desk; SECTMK\\nHello, Fourth Grade...  31 December 2023   \n",
       "6  \\n\\nForeign Desk; SECTA\\n72 of Our Favorite Fa...  31 December 2023   \n",
       "7  \\n\\nMoney and Business/Financial Desk; SECTBU\\...  31 December 2023   \n",
       "8  \\n\\nBook Review Desk; SECTBR\\nPaperback Row\\n\\...  31 December 2023   \n",
       "9  \\n\\nNICHOLAS KRISTOF\\nEditorial Desk; SECTSR\\n...  31 December 2023   \n",
       "\n",
       "                 Auteur  Nombre de mots         Journal  \\\n",
       "0            Nick Tabor             529  New York Times   \n",
       "1         Wesley Morris             422  New York Times   \n",
       "2                  None             179  New York Times   \n",
       "3                  None             454  New York Times   \n",
       "4       Christina Caron             428  New York Times   \n",
       "5                  None             319  New York Times   \n",
       "6         Times Insider             914  New York Times   \n",
       "7          Kashmir Hill             811  New York Times   \n",
       "8  Shreya Chattopadhyay             431  New York Times   \n",
       "9      Nicholas Kristof             976  New York Times   \n",
       "\n",
       "                                               Titre  \\\n",
       "0  Copyright 2023 The New York Times Company.  Al...   \n",
       "1  When Jim Brown and Raquel Welch, Two Sexy Star...   \n",
       "2  Talking During Movies: Totally Evil or Part of...   \n",
       "3                                     Let Kids Vote!   \n",
       "4                         Are We Doomed to Disagree?   \n",
       "5  Hello, Fourth Graders! A Look Back at our Clas...   \n",
       "6                 72 of Our Favorite Facts From 2023   \n",
       "7                        The Stalker Under Your Hood   \n",
       "8                                      Paperback Row   \n",
       "9                       Humans Made Progress In 2023   \n",
       "\n",
       "                            ID  \\\n",
       "0    NYTF000020240104ejcv0000d   \n",
       "1    NYTF000020231231ejcv0006h   \n",
       "2   NYTF000020231231ejcv00064    \n",
       "3   NYTF000020231231ejcv00063    \n",
       "4   NYTF000020231231ejcv0005z    \n",
       "5   NYTF000020231231ejcv0005t    \n",
       "6   NYTF000020231231ejcv0005r    \n",
       "7   NYTF000020231231ejcv0005n    \n",
       "8   NYTF000020231231ejcv0005g    \n",
       "9   NYTF000020231231ejcv00052    \n",
       "\n",
       "                                Preprocessed_Article  Sentiment  \\\n",
       "0  [metropolitan, desk, sectmb, ambitious, public...   0.080807   \n",
       "1  [magazine, desk, sectmm, jim, brown, raquel, w...   0.096043   \n",
       "2  [magazine, desk, sectmk, talking, movie, total...   0.129610   \n",
       "3  [magazine, desk, sectmk, let, kid, vote, 454, ...   0.068136   \n",
       "4  [magazine, desk, sectmk, doomed, disagree, 428...   0.081156   \n",
       "5  [magazine, desk, sectmk, hello, fourth, grader...   0.153341   \n",
       "6  [foreign, desk, secta, 72, favorite, fact, 202...   0.054622   \n",
       "7  [money, businessfinancial, desk, sectbu, stalk...   0.033175   \n",
       "8  [book, review, desk, sectbr, paperback, row, s...   0.127856   \n",
       "9  [nicholas, kristof, editorial, desk, sectsr, h...   0.011590   \n",
       "\n",
       "   environmental_sentiment_score  \n",
       "0                       0.000000  \n",
       "1                       0.001267  \n",
       "2                      -0.007692  \n",
       "3                       0.000000  \n",
       "4                       0.000000  \n",
       "5                       0.000000  \n",
       "6                       0.000000  \n",
       "7                      -0.000993  \n",
       "8                       0.003534  \n",
       "9                      -0.001842  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_environmental_score(token_list, Dico_env):\n",
    "    score = 0\n",
    "    token_count = len(token_list)\n",
    "\n",
    "    for token in token_list:\n",
    "        if token in Dico_env:\n",
    "            score += Dico_env[token]\n",
    "\n",
    "    # Normalize the score to be between -1 and 1\n",
    "    if token_count > 0:\n",
    "        normalized_score = score / token_count\n",
    "        return max(min(normalized_score, 1), -1)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data['environmental_sentiment_score'] = data['Preprocessed_Article'].apply(lambda x: get_environmental_score(x, Dico_env_en))\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code possiblement inutile  (Avant de finaliser le cleaning de la base de donnée, le premier code pour attribuer un score environnemental ne marchait pas, donc on a proposé un deuxième code qui semblait fonctionner. Maintenant que la base de donné est clean, le premier code fonction... donc le code qui suit est possiblement inutile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score environnemental semble être nul pour une partie des articles, on regarde si le score est parfois différent de 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somme_valeurs_absolues = data['environmental_sentiment_score'].abs().sum()\n",
    "#print(\"La somme des valeurs absolue de la variable du score environnement est :\", somme_valeurs_absolues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui pourrait expliquer le fait qu'aucune / très peu de texte, ai une note différente de 0, est le fait que les éléments du dictionnaire n'ont pas été prétraité (tokenisation etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la fonction preprocess_text aux clés du dictionnaire\n",
    "#preprocessed_dict_en = {preprocess_text(key): value for key, value in Dico_env_en.items()}\n",
    "#preprocessed_dict_fr = {preprocess_text(key): value for key, value in Dico_env_fr.items()}\n",
    "\n",
    "\n",
    "# Affichage du dictionnaire après prétraitement des clés\n",
    "#print(preprocessed_dict_en)\n",
    "#print(preprocessed_dict_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention, il ne semble pas que la tokenisation ou la lemnisation ait fonctionné..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_environmental_score(token_list, Dico_env):\n",
    "#    score = 0\n",
    "#    token_count = len(token_list)\n",
    "#\n",
    " #   for token in token_list:\n",
    "  #      if token in Dico_env:\n",
    "   #         score += Dico_env[token]\n",
    "#\n",
    "    # Normalize the score to be between -1 and 1\n",
    "#    if token_count > 0:\n",
    " #       normalized_score = score / token_count\n",
    "  #      return max(min(normalized_score, 1), -1)\n",
    "   # else:\n",
    "    #    return 0\n",
    "\n",
    "#data['environmental_sentiment_score'] = data['Preprocessed_Article'].apply(lambda x: get_environmental_score(x, preprocessed_dict_en))\n",
    "\n",
    "#data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#somme_valeurs_absolues = data['environmental_sentiment_score'].abs().sum()\n",
    "#print(\"La somme des valeurs absolue de la variable du score environnement est :\", somme_valeurs_absolues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Est ce que les éléments de la colonne Processed_Article contiennent une liste de mots qui composent l'article, ou bien c'est un seul élément (une grande chaine de charactère) ? Si c'est une grande chaîne de caractère, ça peut poser problème car pour construire la variable de score envorionnementale, on compare un mot à un texte => Donc le score environnemental reste nul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Preprocessed_Article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Preprocessed_Article'][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les élements de la variable Processed_Article est donc une longue et unique chaîne de caractère. On créer un code qui prends une chaîne de caractère en entrée, et qui renvoie une liste de mots qui composent cet article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def mots_dans_article(article):\n",
    "    # Diviser l'article en mots en utilisant l'espace comme délimiteur\n",
    "#    mots = article.split()\n",
    "\n",
    "    # Retourner la liste des mots\n",
    "#    return mots\n",
    "\n",
    "#data['Preprocessed_Article_split']=data['Preprocessed_Article'].apply(lambda x: mots_dans_article(x))\n",
    "\n",
    "#data['Preprocessed_Article_split']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['environmental_sentiment_score_test'] = data['Preprocessed_Article_split'].apply(lambda x: get_environmental_score(x, preprocessed_dict_en))\n",
    "\n",
    "#data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amélioration du score environnemental => Certains score ne sont pas nul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyse des entreprises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On essaie de voir maintenant quelles entreprises ont été citées dans le texte :\n",
    "On peut supposer qu'on a une colonne avec toutes les entreprises qui ont été citées dans le texte, ou dans le titre : entr_citées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      \n",
       "1      \n",
       "2      \n",
       "3      \n",
       "4      \n",
       "     ..\n",
       "95     \n",
       "96     \n",
       "97     \n",
       "98     \n",
       "99     \n",
       "Name: Entreprises_citées, Length: 100, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importer pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Charger le DataFrame des entreprises\n",
    "df_entreprises = pd.read_csv(\"Firms.csv\")\n",
    "\n",
    "# Mettre en minuscules les noms des entreprises\n",
    "df_entreprises['Company'] = df_entreprises['Company'].str.lower()\n",
    "\n",
    "# Copier la colonne 'Preprocessed_Article' du DataFrame 'data' dans un nouveau DataFrame\n",
    "df_articles = data[['Preprocessed_Article']].copy()\n",
    "\n",
    "# Appliquer la mise en minuscule aux chaînes de caractères dans la liste 'Preprocessed_Article'\n",
    "df_articles['Preprocessed_Article'] = df_articles['Preprocessed_Article'].apply(\n",
    "    lambda x: [word.lower() if isinstance(word, str) else word for word in x]\n",
    ")\n",
    "\n",
    "# Initialisation de la nouvelle colonne pour stocker les noms des entreprises citées dans chaque article\n",
    "df_articles['Entreprises_citées'] = ''\n",
    "\n",
    "# Boucle à travers les articles\n",
    "for index, article in df_articles.iterrows():\n",
    "    entreprises_citées = []\n",
    "    contenu_article = article['Preprocessed_Article']\n",
    "    \n",
    "    # Vérifier si le contenu de l'article est une chaîne de caractères\n",
    "    if isinstance(contenu_article, str):\n",
    "        # Vérifier la présence de chaque entreprise dans le contenu de l'article\n",
    "        for index_ent, entreprise in df_entreprises.iterrows():\n",
    "            nom_entreprise = entreprise['Company']\n",
    "            if isinstance(nom_entreprise, str) and nom_entreprise in contenu_article:\n",
    "                entreprises_citées.append(nom_entreprise)\n",
    "    \n",
    "    # Stocker les entreprises citées dans la nouvelle colonne\n",
    "    df_articles.at[index, 'Entreprises_citées'] = ', '.join(entreprises_citées)\n",
    "\n",
    "# Afficher les articles avec les entreprises citées\n",
    "df_articles['Entreprises_citées']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche maintenant à associer à chaque article, une entreprise spécifique (même s'il est cité plusieurs entreprises)\n",
    "\n",
    "On part du principe qu'un article qui ne cite aucune entreprise n'est pas pertinent à étudier => il n'y aurait aucune communication verte\n",
    "Si jamais un article ne cite qu'une seule entreprise, alors il ne peut y avoir (ou pas) communication verte que sur cette entreprise (mais pas de communication verte de plusieurs entreprises dans un seul article)\n",
    "Si jamais il y a plusieurs entreprises citées dans un seul articles, alors on traitera ces données à part, en pensant qu'il peut y avoir plusieurs communications vertes, venant de différentes entreprises, et tout cela dans un seul article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'articles par nombre d'entreprises citées :\n",
      "Nombre_entreprises_citées\n",
      "0    100\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Résumé du nombre d'articles par nombre d'entreprises citées :\n",
      "Aucune entreprise citée : 100\n"
     ]
    }
   ],
   "source": [
    "# Ajouter une colonne pour le nombre d'entreprises citées dans chaque article\n",
    "df_articles['Nombre_entreprises_citées'] = df_articles['Entreprises_citées'].apply(lambda x: 0 if x == '' else x.count(',') + 1)\n",
    "\n",
    "# Compter le nombre d'articles par nombre d'entreprises citées\n",
    "comptage_entreprises = df_articles['Nombre_entreprises_citées'].value_counts()\n",
    "\n",
    "# Afficher le comptage des articles par nombre d'entreprises citées\n",
    "print(\"Nombre d'articles par nombre d'entreprises citées :\")\n",
    "print(comptage_entreprises)\n",
    "\n",
    "# Afficher le nombre d'articles qui ne citent aucune entreprise, qui en citent une seule, etc.\n",
    "print(\"\\nRésumé du nombre d'articles par nombre d'entreprises citées :\")\n",
    "print(\"Aucune entreprise citée :\", comptage_entreprises.get(0, 0))\n",
    "for i in range(1, max(comptage_entreprises.index) + 1):\n",
    "    print(f\"{i} entreprise(s) citée(s) :\", comptage_entreprises.get(i, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "      <th>Entreprises_citées</th>\n",
       "      <th>Nombre_entreprises_citées</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[metropolitan, desk, sectmb, ambitious, public...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[magazine, desk, sectmm, jim, brown, raquel, w...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[magazine, desk, sectmk, talking, movie, total...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[magazine, desk, sectmk, let, kid, vote, 454, ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[magazine, desk, sectmk, doomed, disagree, 428...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[global, health, science, desk, sectd, new, ho...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[foreign, desk, secta, hotel, intended, house,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[businessfinancial, desk, sectb, eu, open, inv...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[trilobite, science, desk, sectd, proof, wine,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[impossible, energy, transition, mario, loyola...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Preprocessed_Article Entreprises_citées  \\\n",
       "0   [metropolitan, desk, sectmb, ambitious, public...                      \n",
       "1   [magazine, desk, sectmm, jim, brown, raquel, w...                      \n",
       "2   [magazine, desk, sectmk, talking, movie, total...                      \n",
       "3   [magazine, desk, sectmk, let, kid, vote, 454, ...                      \n",
       "4   [magazine, desk, sectmk, doomed, disagree, 428...                      \n",
       "..                                                ...                ...   \n",
       "95  [global, health, science, desk, sectd, new, ho...                      \n",
       "96  [foreign, desk, secta, hotel, intended, house,...                      \n",
       "97  [businessfinancial, desk, sectb, eu, open, inv...                      \n",
       "98  [trilobite, science, desk, sectd, proof, wine,...                      \n",
       "99  [impossible, energy, transition, mario, loyola...                      \n",
       "\n",
       "    Nombre_entreprises_citées  \n",
       "0                           0  \n",
       "1                           0  \n",
       "2                           0  \n",
       "3                           0  \n",
       "4                           0  \n",
       "..                        ...  \n",
       "95                          0  \n",
       "96                          0  \n",
       "97                          0  \n",
       "98                          0  \n",
       "99                          0  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Company</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US7170811035</td>\n",
       "      <td>pfizer inc</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US58933Y1055</td>\n",
       "      <td>merck &amp; co inc</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GB0009252882</td>\n",
       "      <td>gsk plc</td>\n",
       "      <td>Unable to resolve all requested identifiers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US5324571083</td>\n",
       "      <td>eli lilly and co</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE000BAY0017</td>\n",
       "      <td>bayer ag</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13294</th>\n",
       "      <td>US87410C1045</td>\n",
       "      <td>talaris therapeutics inc</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13295</th>\n",
       "      <td>CNE100002DP3</td>\n",
       "      <td>shenzhen senior technology material co ltd</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13296</th>\n",
       "      <td>AU0000181984</td>\n",
       "      <td>vulcan steel ltd</td>\n",
       "      <td>New Zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13297</th>\n",
       "      <td>TH0042010007</td>\n",
       "      <td>bangkok insurance pcl</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13298</th>\n",
       "      <td>JP3766400000</td>\n",
       "      <td>hakuto co ltd</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13299 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ISIN                                     Company  \\\n",
       "0      US7170811035                                  pfizer inc   \n",
       "1      US58933Y1055                              merck & co inc   \n",
       "2      GB0009252882                                     gsk plc   \n",
       "3      US5324571083                            eli lilly and co   \n",
       "4      DE000BAY0017                                    bayer ag   \n",
       "...             ...                                         ...   \n",
       "13294  US87410C1045                    talaris therapeutics inc   \n",
       "13295  CNE100002DP3  shenzhen senior technology material co ltd   \n",
       "13296  AU0000181984                            vulcan steel ltd   \n",
       "13297  TH0042010007                       bangkok insurance pcl   \n",
       "13298  JP3766400000                               hakuto co ltd   \n",
       "\n",
       "                                            Country  \n",
       "0                          United States of America  \n",
       "1                          United States of America  \n",
       "2      Unable to resolve all requested identifiers.  \n",
       "3                          United States of America  \n",
       "4                                           Germany  \n",
       "...                                             ...  \n",
       "13294                      United States of America  \n",
       "13295                                         China  \n",
       "13296                                   New Zealand  \n",
       "13297                                      Thailand  \n",
       "13298                                         Japan  \n",
       "\n",
       "[13299 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entreprises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il semblerait qu'aucune entreprise n'ait été trouvé dans aucun des articles. Peut être qu'elles ne sont pas cité de la même manière dans la base de données des articles, et dans celles des entreprises. On cherche à améliorer l'identification des entreprises dans les articles. Voici des suggestions d'amélioration :\n",
    "\n",
    "- Tokenization et Stemming : Utilisez des techniques de traitement de langage naturel (NLP) telles que la tokenization et le stemming pour normaliser les noms des entreprises dans les articles et dans la base de données avant de comparer les noms. Cela peut vous aider à trouver des correspondances malgré les variations de formulation.\n",
    "\n",
    "- Algorithmes de correspondance de chaînes : Des algorithmes comme Levenshtein Distance ou Jaccard Similarity peuvent être utilisés pour mesurer la similarité entre deux chaînes de caractères. Vous pouvez utiliser ces mesures pour comparer les noms d'entreprises dans les articles avec ceux dans la base de données et établir des seuils de similarité acceptables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, on essaie d'appliquer les méthodes de NLP aux noms fournis des entreprises :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Vérifier si le texte est une chaîne de caractères\n",
    "    if isinstance(text, str):\n",
    "        # Minuscule\n",
    "        text = text.lower()\n",
    "        # Supprimer la ponctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Tokenization\n",
    "        words = word_tokenize(text)\n",
    "        # Suppression des stop-words\n",
    "        filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "        # Lemmatisation\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_output = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "        return lemmatized_output\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "df_entreprises['Preprocessed_Company'] = df_entreprises['Company'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Company</th>\n",
       "      <th>Country</th>\n",
       "      <th>Preprocessed_Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US7170811035</td>\n",
       "      <td>pfizer inc</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>[pfizer, inc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US58933Y1055</td>\n",
       "      <td>merck &amp; co inc</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>[merck, co, inc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GB0009252882</td>\n",
       "      <td>gsk plc</td>\n",
       "      <td>Unable to resolve all requested identifiers.</td>\n",
       "      <td>[gsk, plc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US5324571083</td>\n",
       "      <td>eli lilly and co</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>[eli, lilly, co]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE000BAY0017</td>\n",
       "      <td>bayer ag</td>\n",
       "      <td>Germany</td>\n",
       "      <td>[bayer, ag]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13294</th>\n",
       "      <td>US87410C1045</td>\n",
       "      <td>talaris therapeutics inc</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>[talaris, therapeutic, inc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13295</th>\n",
       "      <td>CNE100002DP3</td>\n",
       "      <td>shenzhen senior technology material co ltd</td>\n",
       "      <td>China</td>\n",
       "      <td>[shenzhen, senior, technology, material, co, ltd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13296</th>\n",
       "      <td>AU0000181984</td>\n",
       "      <td>vulcan steel ltd</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>[vulcan, steel, ltd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13297</th>\n",
       "      <td>TH0042010007</td>\n",
       "      <td>bangkok insurance pcl</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>[bangkok, insurance, pcl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13298</th>\n",
       "      <td>JP3766400000</td>\n",
       "      <td>hakuto co ltd</td>\n",
       "      <td>Japan</td>\n",
       "      <td>[hakuto, co, ltd]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13299 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ISIN                                     Company  \\\n",
       "0      US7170811035                                  pfizer inc   \n",
       "1      US58933Y1055                              merck & co inc   \n",
       "2      GB0009252882                                     gsk plc   \n",
       "3      US5324571083                            eli lilly and co   \n",
       "4      DE000BAY0017                                    bayer ag   \n",
       "...             ...                                         ...   \n",
       "13294  US87410C1045                    talaris therapeutics inc   \n",
       "13295  CNE100002DP3  shenzhen senior technology material co ltd   \n",
       "13296  AU0000181984                            vulcan steel ltd   \n",
       "13297  TH0042010007                       bangkok insurance pcl   \n",
       "13298  JP3766400000                               hakuto co ltd   \n",
       "\n",
       "                                            Country  \\\n",
       "0                          United States of America   \n",
       "1                          United States of America   \n",
       "2      Unable to resolve all requested identifiers.   \n",
       "3                          United States of America   \n",
       "4                                           Germany   \n",
       "...                                             ...   \n",
       "13294                      United States of America   \n",
       "13295                                         China   \n",
       "13296                                   New Zealand   \n",
       "13297                                      Thailand   \n",
       "13298                                         Japan   \n",
       "\n",
       "                                    Preprocessed_Company  \n",
       "0                                          [pfizer, inc]  \n",
       "1                                       [merck, co, inc]  \n",
       "2                                             [gsk, plc]  \n",
       "3                                       [eli, lilly, co]  \n",
       "4                                            [bayer, ag]  \n",
       "...                                                  ...  \n",
       "13294                        [talaris, therapeutic, inc]  \n",
       "13295  [shenzhen, senior, technology, material, co, ltd]  \n",
       "13296                               [vulcan, steel, ltd]  \n",
       "13297                          [bangkok, insurance, pcl]  \n",
       "13298                                  [hakuto, co, ltd]  \n",
       "\n",
       "[13299 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entreprises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser une nouvelle colonne dans df_articles pour stocker les entreprises identifiées\n",
    "df_articles['Entreprises_Identifiees'] = None\n",
    "\n",
    "# Parcourir chaque article dans la base de données\n",
    "for index, row_article in df_articles.iterrows():\n",
    "    article = row_article['Preprocessed_Article']  # Contenu de l'article prétraité\n",
    "    entreprises_identifiees = []  # Liste pour stocker les entreprises identifiées pour cet article\n",
    "    \n",
    "    # Parcourir chaque entreprise dans votre DataFrame df\n",
    "    for index_ent, row_ent in df_entreprises.iterrows():\n",
    "        companies = row_ent['Company']  # Nom de l'entreprise correspondante à cet article\n",
    "        keywords = row_ent['Preprocessed_Company']  # Liste de mots-clés prétraités\n",
    "        \n",
    "        # Parcourir chaque mot-clé dans la liste des mots-clés prétraités\n",
    "        for keyword in keywords:\n",
    "            # Vérifiez si le mot-clé est présent dans l'article\n",
    "            if keyword in article:\n",
    "                # Ajouter l'entreprise à la liste des entreprises identifiées pour cet article\n",
    "                entreprises_identifiees.append(companies)\n",
    "    \n",
    "    # Mettre à jour la colonne 'Entreprises_Identifiees' avec les entreprises identifiées pour cet article\n",
    "    df_articles.at[index, 'Entreprises_Identifiees'] = entreprises_identifiees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles['Nombre_Entreprises_Identifiees'] = df_articles['Entreprises_Identifiees'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "      <th>Entreprises_citées</th>\n",
       "      <th>Nombre_entreprises_citées</th>\n",
       "      <th>Entreprises_Identifiees</th>\n",
       "      <th>Nombre_Entreprises_Identifiees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[metropolitan, desk, sectmb, ambitious, public...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[anglo american plc, british american tobacco ...</td>\n",
       "      <td>2784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[magazine, desk, sectmm, jim, brown, raquel, w...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[general motors co, anglo american plc, philip...</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[magazine, desk, sectmk, talking, movie, total...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, tokyo e...</td>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[magazine, desk, sectmk, let, kid, vote, 454, ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[anglo american plc, british american tobacco ...</td>\n",
       "      <td>796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[magazine, desk, sectmk, doomed, disagree, 428...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[roche holding ag, hsbc holdings plc, mercedes...</td>\n",
       "      <td>2872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[global, health, science, desk, sectd, new, ho...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[takeda pharmaceutical co ltd, rio tinto plc, ...</td>\n",
       "      <td>5011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[foreign, desk, secta, hotel, intended, house,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[general motors co, international business mac...</td>\n",
       "      <td>2653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[businessfinancial, desk, sectb, eu, open, inv...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, mercede...</td>\n",
       "      <td>2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[trilobite, science, desk, sectd, proof, wine,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, sumitom...</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[impossible, energy, transition, mario, loyola...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[pfizer inc, merck &amp; co inc, dow inc, dow inc,...</td>\n",
       "      <td>5443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Preprocessed_Article Entreprises_citées  \\\n",
       "0   [metropolitan, desk, sectmb, ambitious, public...                      \n",
       "1   [magazine, desk, sectmm, jim, brown, raquel, w...                      \n",
       "2   [magazine, desk, sectmk, talking, movie, total...                      \n",
       "3   [magazine, desk, sectmk, let, kid, vote, 454, ...                      \n",
       "4   [magazine, desk, sectmk, doomed, disagree, 428...                      \n",
       "..                                                ...                ...   \n",
       "95  [global, health, science, desk, sectd, new, ho...                      \n",
       "96  [foreign, desk, secta, hotel, intended, house,...                      \n",
       "97  [businessfinancial, desk, sectb, eu, open, inv...                      \n",
       "98  [trilobite, science, desk, sectd, proof, wine,...                      \n",
       "99  [impossible, energy, transition, mario, loyola...                      \n",
       "\n",
       "    Nombre_entreprises_citées  \\\n",
       "0                           0   \n",
       "1                           0   \n",
       "2                           0   \n",
       "3                           0   \n",
       "4                           0   \n",
       "..                        ...   \n",
       "95                          0   \n",
       "96                          0   \n",
       "97                          0   \n",
       "98                          0   \n",
       "99                          0   \n",
       "\n",
       "                              Entreprises_Identifiees  \\\n",
       "0   [anglo american plc, british american tobacco ...   \n",
       "1   [general motors co, anglo american plc, philip...   \n",
       "2   [international business machines corp, tokyo e...   \n",
       "3   [anglo american plc, british american tobacco ...   \n",
       "4   [roche holding ag, hsbc holdings plc, mercedes...   \n",
       "..                                                ...   \n",
       "95  [takeda pharmaceutical co ltd, rio tinto plc, ...   \n",
       "96  [general motors co, international business mac...   \n",
       "97  [international business machines corp, mercede...   \n",
       "98  [international business machines corp, sumitom...   \n",
       "99  [pfizer inc, merck & co inc, dow inc, dow inc,...   \n",
       "\n",
       "    Nombre_Entreprises_Identifiees  \n",
       "0                             2784  \n",
       "1                             1035  \n",
       "2                              554  \n",
       "3                              796  \n",
       "4                             2872  \n",
       "..                             ...  \n",
       "95                            5011  \n",
       "96                            2653  \n",
       "97                            2544  \n",
       "98                            1265  \n",
       "99                            5443  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsqu'on observe les deux méthodes, on voit que leur résultat est trop extrême : soit on identifie aucune entreprise, soit on en identifie beaucoup trop... Peut être que la seconde méthode énonce beaucoup trop d'entreprises car elle cherche des termes qui ne sont pas vraiment dans le nom de l'entreprise (exemple : pour pfizer, dont le nom de compagnie est Pfizer Inc, le code va également chercher les termes Inc. Ou encore, pour coca cola, dont le réel nom est coca cola co, on va chercher des termes tels que \"co\"). Pour résoudre ce problème, on propose de ne cherche à identifier que le premier terme de la liste des entreprises :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser une nouvelle colonne dans df_articles pour stocker les entreprises identifiées\n",
    "df_articles['Entreprises_Identifiees_2'] = None\n",
    "\n",
    "# Parcourir chaque article dans la base de données\n",
    "for index, row_article in df_articles.iterrows():\n",
    "    article = row_article['Preprocessed_Article']  # Contenu de l'article prétraité\n",
    "    entreprises_identifiees = []  # Liste pour stocker les entreprises identifiées pour cet article\n",
    "    \n",
    "    # Parcourir chaque entreprise dans votre DataFrame df\n",
    "    for index_ent, row_ent in df_entreprises.iterrows():\n",
    "        companies = row_ent['Company']  # Nom de l'entreprise correspondante à cet article\n",
    "        keywords = row_ent['Preprocessed_Company']  # Liste de mots-clés prétraités\n",
    "        \n",
    "        # Utiliser seulement le premier mot-clé\n",
    "        if keywords:  # Vérifiez si la liste des mots-clés n'est pas vide\n",
    "            first_keyword = keywords[0]  # Prenez le premier mot-clé\n",
    "        \n",
    "            # Vérifiez si le mot-clé est présent dans l'article\n",
    "            if first_keyword in article:\n",
    "                # Ajouter l'entreprise à la liste des entreprises identifiées pour cet article\n",
    "                entreprises_identifiees.append(companies)\n",
    "    \n",
    "    # Mettre à jour la colonne 'Entreprises_Identifiees_2' avec les entreprises identifiées pour cet article\n",
    "    df_articles.at[index, 'Entreprises_Identifiees_2'] = entreprises_identifiees\n",
    "    df_articles['Nombre_Entreprises_Identifiees_2'] = df_articles['Entreprises_Identifiees_2'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "      <th>Entreprises_citées</th>\n",
       "      <th>Nombre_entreprises_citées</th>\n",
       "      <th>Entreprises_Identifiees</th>\n",
       "      <th>Nombre_Entreprises_Identifiees</th>\n",
       "      <th>Entreprises_Identifiees_2</th>\n",
       "      <th>Nombre_Entreprises_Identifiees_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[metropolitan, desk, sectmb, ambitious, public...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[anglo american plc, british american tobacco ...</td>\n",
       "      <td>2784</td>\n",
       "      <td>[american international group inc, american ex...</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[magazine, desk, sectmm, jim, brown, raquel, w...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[general motors co, anglo american plc, philip...</td>\n",
       "      <td>1035</td>\n",
       "      <td>[general motors co, general electric co, ameri...</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[magazine, desk, sectmk, talking, movie, total...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, tokyo e...</td>\n",
       "      <td>554</td>\n",
       "      <td>[new gold inc, w. r. berkley corp, city develo...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[magazine, desk, sectmk, let, kid, vote, 454, ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[anglo american plc, british american tobacco ...</td>\n",
       "      <td>796</td>\n",
       "      <td>[american international group inc, american ex...</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[magazine, desk, sectmk, doomed, disagree, 428...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[roche holding ag, hsbc holdings plc, mercedes...</td>\n",
       "      <td>2872</td>\n",
       "      <td>[u.s. bancorp, best buy co inc, new gold inc, ...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[global, health, science, desk, sectd, new, ho...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[takeda pharmaceutical co ltd, rio tinto plc, ...</td>\n",
       "      <td>5011</td>\n",
       "      <td>[rio tinto plc, international business machine...</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[foreign, desk, secta, hotel, intended, house,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[general motors co, international business mac...</td>\n",
       "      <td>2653</td>\n",
       "      <td>[general motors co, international business mac...</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[businessfinancial, desk, sectb, eu, open, inv...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, mercede...</td>\n",
       "      <td>2544</td>\n",
       "      <td>[marks and spencer group plc, united states st...</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[trilobite, science, desk, sectd, proof, wine,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, sumitom...</td>\n",
       "      <td>1265</td>\n",
       "      <td>[best buy co inc, southwest airlines co, regio...</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[impossible, energy, transition, mario, loyola...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[pfizer inc, merck &amp; co inc, dow inc, dow inc,...</td>\n",
       "      <td>5443</td>\n",
       "      <td>[dow inc, international business machines corp...</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Preprocessed_Article Entreprises_citées  \\\n",
       "0   [metropolitan, desk, sectmb, ambitious, public...                      \n",
       "1   [magazine, desk, sectmm, jim, brown, raquel, w...                      \n",
       "2   [magazine, desk, sectmk, talking, movie, total...                      \n",
       "3   [magazine, desk, sectmk, let, kid, vote, 454, ...                      \n",
       "4   [magazine, desk, sectmk, doomed, disagree, 428...                      \n",
       "..                                                ...                ...   \n",
       "95  [global, health, science, desk, sectd, new, ho...                      \n",
       "96  [foreign, desk, secta, hotel, intended, house,...                      \n",
       "97  [businessfinancial, desk, sectb, eu, open, inv...                      \n",
       "98  [trilobite, science, desk, sectd, proof, wine,...                      \n",
       "99  [impossible, energy, transition, mario, loyola...                      \n",
       "\n",
       "    Nombre_entreprises_citées  \\\n",
       "0                           0   \n",
       "1                           0   \n",
       "2                           0   \n",
       "3                           0   \n",
       "4                           0   \n",
       "..                        ...   \n",
       "95                          0   \n",
       "96                          0   \n",
       "97                          0   \n",
       "98                          0   \n",
       "99                          0   \n",
       "\n",
       "                              Entreprises_Identifiees  \\\n",
       "0   [anglo american plc, british american tobacco ...   \n",
       "1   [general motors co, anglo american plc, philip...   \n",
       "2   [international business machines corp, tokyo e...   \n",
       "3   [anglo american plc, british american tobacco ...   \n",
       "4   [roche holding ag, hsbc holdings plc, mercedes...   \n",
       "..                                                ...   \n",
       "95  [takeda pharmaceutical co ltd, rio tinto plc, ...   \n",
       "96  [general motors co, international business mac...   \n",
       "97  [international business machines corp, mercede...   \n",
       "98  [international business machines corp, sumitom...   \n",
       "99  [pfizer inc, merck & co inc, dow inc, dow inc,...   \n",
       "\n",
       "    Nombre_Entreprises_Identifiees  \\\n",
       "0                             2784   \n",
       "1                             1035   \n",
       "2                              554   \n",
       "3                              796   \n",
       "4                             2872   \n",
       "..                             ...   \n",
       "95                            5011   \n",
       "96                            2653   \n",
       "97                            2544   \n",
       "98                            1265   \n",
       "99                            5443   \n",
       "\n",
       "                            Entreprises_Identifiees_2  \\\n",
       "0   [american international group inc, american ex...   \n",
       "1   [general motors co, general electric co, ameri...   \n",
       "2   [new gold inc, w. r. berkley corp, city develo...   \n",
       "3   [american international group inc, american ex...   \n",
       "4   [u.s. bancorp, best buy co inc, new gold inc, ...   \n",
       "..                                                ...   \n",
       "95  [rio tinto plc, international business machine...   \n",
       "96  [general motors co, international business mac...   \n",
       "97  [marks and spencer group plc, united states st...   \n",
       "98  [best buy co inc, southwest airlines co, regio...   \n",
       "99  [dow inc, international business machines corp...   \n",
       "\n",
       "    Nombre_Entreprises_Identifiees_2  \n",
       "0                                394  \n",
       "1                                320  \n",
       "2                                 60  \n",
       "3                                130  \n",
       "4                                 92  \n",
       "..                               ...  \n",
       "95                               640  \n",
       "96                               345  \n",
       "97                               252  \n",
       "98                               165  \n",
       "99                               440  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On arrive à réduire le nombre de compagnies identifiées... sans doute parce qu'on ne prends pas en compte les 'Inc', 'Co' ... On peut sans doute encore améliorer ce code de manière à affiner la manière dont on identifie nos entreprises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaites maintenant récupérer le contexte autour de l'entreprise dans l'article. Pour cela, on propose de récupérer les k (constante fixée arbitrairement) mots avant et après le mot qui a permis d'identifier une entreprise. L'intérêt de choisir un k faible, est de s'assurer que le contexte s'applique bien à l'entreprise que l'on souhaites évaluer. En revanche, on perds de l'information si on choisis un k trop faible. A l'inverse, choisir un k élevé permet de récupérer beaucoup de termes, mais on prends le risque de liéer ces termes à l'entreprises, alors qu'il est possible que dans l'article, il n'y ait pas de lien. On améliore donc le code précédent de la manière suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constante k\n",
    "k = 5  # Vous pouvez ajuster cette valeur selon vos besoins\n",
    "\n",
    "# Parcourir chaque article dans la base de données\n",
    "for index, row_article in df_articles.iterrows():\n",
    "    article = row_article['Preprocessed_Article']  # Contenu de l'article prétraité\n",
    "    entreprises_identifiees = []  # Liste pour stocker les entreprises identifiées pour cet article\n",
    "    \n",
    "    # Parcourir chaque entreprise dans votre DataFrame df\n",
    "    for index_ent, row_ent in df_entreprises.iterrows():\n",
    "        companies = row_ent['Company']  # Nom de l'entreprise correspondante à cet article\n",
    "        keywords = row_ent['Preprocessed_Company']  # Liste de mots-clés prétraités\n",
    "        \n",
    "        # Utiliser seulement le premier mot-clé\n",
    "        if keywords:  # Vérifiez si la liste des mots-clés n'est pas vide\n",
    "            first_keyword = keywords[0]  # Prenez le premier mot-clé\n",
    "        \n",
    "            # Vérifiez si le mot-clé est présent dans l'article\n",
    "            if first_keyword in article:\n",
    "                # Trouver l'indice du premier mot-clé dans l'article\n",
    "                idx_keyword = article.index(first_keyword)\n",
    "                \n",
    "                # Extraire les k mots avant et après le mot identifié\n",
    "                start_idx = max(0, idx_keyword - k)\n",
    "                end_idx = min(len(article), idx_keyword + k + 1)  # Ajouter 1 pour inclure le dernier indice\n",
    "                context_words = article[start_idx:end_idx]\n",
    "                \n",
    "                # Ajouter l'entreprise et le contexte à la liste des entreprises identifiées pour cet article\n",
    "                entreprises_identifiees.append((companies, context_words))\n",
    "    \n",
    "    # Mettre à jour la colonne 'Entreprises_Identifiees_2' avec les entreprises identifiées pour cet article\n",
    "    df_articles.at[index, 'Entreprises_Identifiees_2'] = entreprises_identifiees\n",
    "    df_articles['Nombre_Entreprises_Identifiees_2'] = df_articles['Entreprises_Identifiees_2'].apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('american international group inc',\n",
       " ['including',\n",
       "  'one',\n",
       "  'largest',\n",
       "  'donation',\n",
       "  'university',\n",
       "  'american',\n",
       "  'history',\n",
       "  'right',\n",
       "  'develop',\n",
       "  'climate',\n",
       "  'solution'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles['Entreprises_Identifiees_2'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['including',\n",
       " 'one',\n",
       " 'largest',\n",
       " 'donation',\n",
       " 'university',\n",
       " 'american',\n",
       " 'history',\n",
       " 'right',\n",
       " 'develop',\n",
       " 'climate',\n",
       " 'solution']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles['Entreprises_Identifiees_2'][0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourrait chercher à appliquer l'analyse de sentiment à ces mots, puis l'associer à l'entreprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_environmental_score(df_articles['Entreprises_Identifiees_2'][0][0][1],Dico_env_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constante k\n",
    "k = 5  # Vous pouvez ajuster cette valeur selon vos besoins\n",
    "\n",
    "# Parcourir chaque article dans la base de données\n",
    "for index, row_article in df_articles.iterrows():\n",
    "    article = row_article['Preprocessed_Article']  # Contenu de l'article prétraité\n",
    "    entreprises_identifiees = []  # Liste pour stocker les entreprises identifiées pour cet article\n",
    "    \n",
    "    # Parcourir chaque entreprise dans votre DataFrame df\n",
    "    for index_ent, row_ent in df_entreprises.iterrows():\n",
    "        companies = row_ent['Company']  # Nom de l'entreprise correspondante à cet article\n",
    "        keywords = row_ent['Preprocessed_Company']  # Liste de mots-clés prétraités\n",
    "        \n",
    "        # Utiliser seulement le premier mot-clé\n",
    "        if keywords:  # Vérifiez si la liste des mots-clés n'est pas vide\n",
    "            first_keyword = keywords[0]  # Prenez le premier mot-clé\n",
    "        \n",
    "            # Vérifiez si le mot-clé est présent dans l'article\n",
    "            if first_keyword in article:\n",
    "                # Trouver l'indice du premier mot-clé dans l'article\n",
    "                idx_keyword = article.index(first_keyword)\n",
    "                \n",
    "                # Extraire les k mots avant et après le mot identifié\n",
    "                start_idx = max(0, idx_keyword - k)\n",
    "                end_idx = min(len(article), idx_keyword + k + 1)  # Ajouter 1 pour inclure le dernier indice\n",
    "                context_words = article[start_idx:end_idx]\n",
    "                environmental_score = get_environmental_score(context_words,Dico_env_en)\n",
    "                \n",
    "                # Ajouter l'entreprise et le contexte à la liste des entreprises identifiées pour cet article\n",
    "                entreprises_identifiees.append((companies, context_words, environmental_score))\n",
    "    \n",
    "    # Mettre à jour la colonne 'Entreprises_Identifiees_2' avec les entreprises identifiées pour cet article\n",
    "    df_articles.at[index, 'Entreprises_Identifiees_2'] = entreprises_identifiees\n",
    "    df_articles['Nombre_Entreprises_Identifiees_2'] = df_articles['Entreprises_Identifiees_2'].apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "      <th>Entreprises_citées</th>\n",
       "      <th>Nombre_entreprises_citées</th>\n",
       "      <th>Entreprises_Identifiees</th>\n",
       "      <th>Nombre_Entreprises_Identifiees</th>\n",
       "      <th>Entreprises_Identifiees_2</th>\n",
       "      <th>Nombre_Entreprises_Identifiees_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[metropolitan, desk, sectmb, ambitious, public...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[anglo american plc, british american tobacco ...</td>\n",
       "      <td>2784</td>\n",
       "      <td>[(american international group inc, [including...</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[magazine, desk, sectmm, jim, brown, raquel, w...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[general motors co, anglo american plc, philip...</td>\n",
       "      <td>1035</td>\n",
       "      <td>[(general motors co, [play, sheriff, triumphan...</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[magazine, desk, sectmk, talking, movie, total...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, tokyo e...</td>\n",
       "      <td>554</td>\n",
       "      <td>[(new gold inc, [179, word, 31, december, 2023...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[magazine, desk, sectmk, let, kid, vote, 454, ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[anglo american plc, british american tobacco ...</td>\n",
       "      <td>796</td>\n",
       "      <td>[(american international group inc, [past, 10,...</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[magazine, desk, sectmk, doomed, disagree, 428...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[roche holding ag, hsbc holdings plc, mercedes...</td>\n",
       "      <td>2872</td>\n",
       "      <td>[(u.s. bancorp, [live, connected, world, inter...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[global, health, science, desk, sectd, new, ho...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[takeda pharmaceutical co ltd, rio tinto plc, ...</td>\n",
       "      <td>5011</td>\n",
       "      <td>[(rio tinto plc, [therapy, leishmaniasis, mede...</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[foreign, desk, secta, hotel, intended, house,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[general motors co, international business mac...</td>\n",
       "      <td>2653</td>\n",
       "      <td>[(general motors co, [circumstance, recognize,...</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[businessfinancial, desk, sectb, eu, open, inv...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, mercede...</td>\n",
       "      <td>2544</td>\n",
       "      <td>[(marks and spencer group plc, [pay, authentic...</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[trilobite, science, desk, sectd, proof, wine,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, sumitom...</td>\n",
       "      <td>1265</td>\n",
       "      <td>[(best buy co inc, [ranking, instituted, napol...</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[impossible, energy, transition, mario, loyola...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[pfizer inc, merck &amp; co inc, dow inc, dow inc,...</td>\n",
       "      <td>5443</td>\n",
       "      <td>[(dow inc, [j, a13, english, copyright, 2023, ...</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Preprocessed_Article Entreprises_citées  \\\n",
       "0   [metropolitan, desk, sectmb, ambitious, public...                      \n",
       "1   [magazine, desk, sectmm, jim, brown, raquel, w...                      \n",
       "2   [magazine, desk, sectmk, talking, movie, total...                      \n",
       "3   [magazine, desk, sectmk, let, kid, vote, 454, ...                      \n",
       "4   [magazine, desk, sectmk, doomed, disagree, 428...                      \n",
       "..                                                ...                ...   \n",
       "95  [global, health, science, desk, sectd, new, ho...                      \n",
       "96  [foreign, desk, secta, hotel, intended, house,...                      \n",
       "97  [businessfinancial, desk, sectb, eu, open, inv...                      \n",
       "98  [trilobite, science, desk, sectd, proof, wine,...                      \n",
       "99  [impossible, energy, transition, mario, loyola...                      \n",
       "\n",
       "    Nombre_entreprises_citées  \\\n",
       "0                           0   \n",
       "1                           0   \n",
       "2                           0   \n",
       "3                           0   \n",
       "4                           0   \n",
       "..                        ...   \n",
       "95                          0   \n",
       "96                          0   \n",
       "97                          0   \n",
       "98                          0   \n",
       "99                          0   \n",
       "\n",
       "                              Entreprises_Identifiees  \\\n",
       "0   [anglo american plc, british american tobacco ...   \n",
       "1   [general motors co, anglo american plc, philip...   \n",
       "2   [international business machines corp, tokyo e...   \n",
       "3   [anglo american plc, british american tobacco ...   \n",
       "4   [roche holding ag, hsbc holdings plc, mercedes...   \n",
       "..                                                ...   \n",
       "95  [takeda pharmaceutical co ltd, rio tinto plc, ...   \n",
       "96  [general motors co, international business mac...   \n",
       "97  [international business machines corp, mercede...   \n",
       "98  [international business machines corp, sumitom...   \n",
       "99  [pfizer inc, merck & co inc, dow inc, dow inc,...   \n",
       "\n",
       "    Nombre_Entreprises_Identifiees  \\\n",
       "0                             2784   \n",
       "1                             1035   \n",
       "2                              554   \n",
       "3                              796   \n",
       "4                             2872   \n",
       "..                             ...   \n",
       "95                            5011   \n",
       "96                            2653   \n",
       "97                            2544   \n",
       "98                            1265   \n",
       "99                            5443   \n",
       "\n",
       "                            Entreprises_Identifiees_2  \\\n",
       "0   [(american international group inc, [including...   \n",
       "1   [(general motors co, [play, sheriff, triumphan...   \n",
       "2   [(new gold inc, [179, word, 31, december, 2023...   \n",
       "3   [(american international group inc, [past, 10,...   \n",
       "4   [(u.s. bancorp, [live, connected, world, inter...   \n",
       "..                                                ...   \n",
       "95  [(rio tinto plc, [therapy, leishmaniasis, mede...   \n",
       "96  [(general motors co, [circumstance, recognize,...   \n",
       "97  [(marks and spencer group plc, [pay, authentic...   \n",
       "98  [(best buy co inc, [ranking, instituted, napol...   \n",
       "99  [(dow inc, [j, a13, english, copyright, 2023, ...   \n",
       "\n",
       "    Nombre_Entreprises_Identifiees_2  \n",
       "0                                394  \n",
       "1                                320  \n",
       "2                                 60  \n",
       "3                                130  \n",
       "4                                 92  \n",
       "..                               ...  \n",
       "95                               640  \n",
       "96                               345  \n",
       "97                               252  \n",
       "98                               165  \n",
       "99                               440  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles['Entreprises_Identifiees_2'][0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles['Entreprises_Identifiees_2'][0][1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles['Entreprises_Identifiees_2'][0][2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles['Entreprises_Identifiees_2'][0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Titre_article'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Affichage des articles avec une seule entreprise citée\u001b[39;00m\n\u001b[1;32m      8\u001b[0m articles_avec_entreprise_unique \u001b[38;5;241m=\u001b[39m df_articles[df_articles[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntreprise_unique\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43marticles_avec_entreprise_unique\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTitre_article\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEntreprise_unique\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/pandas/core/frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/pandas/core/indexes/base.py:6199\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6197\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6201\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6203\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/pandas/core/indexes/base.py:6251\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6250\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6251\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Titre_article'] not in index\""
     ]
    }
   ],
   "source": [
    "# Suppression des données sans entreprises citées\n",
    "df_articles = df_articles[df_articles['Entreprises_citées'] != '']\n",
    "\n",
    "# Identification des articles avec une seule entreprise citée\n",
    "df_articles['Entreprise_unique'] = df_articles['Entreprises_citées'].apply(lambda x: x.split(',')[0] if ',' not in x else '')\n",
    "\n",
    "# Affichage des articles avec une seule entreprise citée\n",
    "articles_avec_entreprise_unique = df_articles[df_articles['Entreprise_unique'] != '']\n",
    "print(articles_avec_entreprise_unique[['Titre_article', 'Entreprise_unique']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkb1ZHzje3fw"
   },
   "source": [
    "Si on a une base de données qui contient un certain nomre d'articles deja labellisés avec une note environnementale, on pourrait entrainer un modèle de machine learning plus traditionnel que de l'analyse de sentiment.\n",
    "\n",
    "en effet, on peut passer par de la vectorization des mots par TF-IDF (ou autre - à voir), puis entrainer un modèle de régression linéaire (ou autre - à voir), et prédire pour les nouveaux articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYxpksZle1d_"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dataset['text'])  # 'texts' is the column with your text data\n",
    "y = dataset['scores']  # 'scores' is the column with your positivity scores\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "new_text = vectorizer.transform([\"New text\"])\n",
    "new_score = model.predict(new_text)\n",
    "print(f'Predicted Sentiment Score: {new_score}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
