{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gc-N_bxnObwn",
    "outputId": "a8cb318d-08bc-4edd-be5c-c51a68b88c5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (1.13.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (2.2.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (1.3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (3.1.3)\n",
      "Collecting numexpr (from pyLDAvis)\n",
      "  Downloading numexpr-2.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
      "Collecting funcy (from pyLDAvis)\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (1.4.1.post1)\n",
      "Requirement already satisfied: gensim in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (4.3.2)\n",
      "Requirement already satisfied: setuptools in /opt/mamba/lib/python3.11/site-packages (from pyLDAvis) (69.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/mamba/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/mamba/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/mamba/lib/python3.11/site-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/mamba/lib/python3.11/site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.4.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/mamba/lib/python3.11/site-packages (from gensim->pyLDAvis) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/mamba/lib/python3.11/site-packages (from jinja2->pyLDAvis) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: wrapt in /opt/mamba/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.16.0)\n",
      "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Downloading numexpr-2.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.3/378.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: funcy, numexpr, pyLDAvis\n",
      "Successfully installed funcy-2.0 numexpr-2.10.0 pyLDAvis-3.4.1\n",
      "Requirement already satisfied: nltk in /opt/mamba/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/mamba/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/mamba/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/mamba/lib/python3.11/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/mamba/lib/python3.11/site-packages (from nltk) (4.66.2)\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/mamba/lib/python3.11/site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in /opt/mamba/lib/python3.11/site-packages (from wordcloud) (10.3.0)\n",
      "Requirement already satisfied: matplotlib in /opt/mamba/lib/python3.11/site-packages (from wordcloud) (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/mamba/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/mamba/lib/python3.11/site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/mamba/lib/python3.11/site-packages (from matplotlib->wordcloud) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/mamba/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.11/site-packages (from matplotlib->wordcloud) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/mamba/lib/python3.11/site-packages (from matplotlib->wordcloud) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/mamba/lib/python3.11/site-packages (from matplotlib->wordcloud) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (548 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.3/548.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.3\n",
      "Collecting TextBlob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in /opt/mamba/lib/python3.11/site-packages (from TextBlob) (3.8.1)\n",
      "Requirement already satisfied: click in /opt/mamba/lib/python3.11/site-packages (from nltk>=3.8->TextBlob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/mamba/lib/python3.11/site-packages (from nltk>=3.8->TextBlob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/mamba/lib/python3.11/site-packages (from nltk>=3.8->TextBlob) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/mamba/lib/python3.11/site-packages (from nltk>=3.8->TextBlob) (4.66.2)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: TextBlob\n",
      "Successfully installed TextBlob-0.18.0.post0\n",
      "Requirement already satisfied: matplotlib in /opt/mamba/lib/python3.11/site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/mamba/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/mamba/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/mamba/lib/python3.11/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/mamba/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /opt/mamba/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/mamba/lib/python3.11/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/mamba/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/mamba/lib/python3.11/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyLDAvis\n",
    "!pip3 install nltk\n",
    "!pip3 install wordcloud\n",
    "!pip3 install TextBlob\n",
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mSOVtiqNKtK",
    "outputId": "06b329e1-9b0e-41db-8110-1b4af46507d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package genesis to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/genesis.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/onyxia/nltk_data...\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/onyxia/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "#from IPython.display import display\n",
    "import base64\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from time import time\n",
    "# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3JH1r8K5gMe"
   },
   "source": [
    "# Récupération des communications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi6ZdjmlEK32"
   },
   "source": [
    "## Webscrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour voir notre travail de webscrapping, on pourra se référer au notebook nommé \"Essaie webscrapp.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Une première base de donnée : Une centaine d'articles du NYT et du WSJ avec le mot clef environnement sur les derniers jours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger à partir du fichier pickle\n",
    "data = pd.read_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUFpMBn5EZk8"
   },
   "source": [
    "# Traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAfkJlueLLR2"
   },
   "source": [
    "## Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Minuscule\n",
    "    text = text.lower()\n",
    "    # Supprimer la ponctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    # Suppression des stop-words\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_output = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "    \n",
    "    return lemmatized_output\n",
    "\n",
    "# Appliquer la fonction preprocess_text à la colonne 'Article'\n",
    "data['Preprocessed_Article'] = data['Article'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Nombre de mots</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Titre</th>\n",
       "      <th>ID</th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nick Tabor</td>\n",
       "      <td>529</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Copyright 2023 The New York Times Company.  Al...</td>\n",
       "      <td>NYTF000020240104ejcv0000d</td>\n",
       "      <td>[metropolitan, desk, sectmb, ambitious, public...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Wesley Morris</td>\n",
       "      <td>422</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>When Jim Brown and Raquel Welch, Two Sexy Star...</td>\n",
       "      <td>NYTF000020231231ejcv0006h</td>\n",
       "      <td>[magazine, desk, sectmm, jim, brown, raquel, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nTalking During Movi...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>179</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Talking During Movies: Totally Evil or Part of...</td>\n",
       "      <td>NYTF000020231231ejcv00064</td>\n",
       "      <td>[magazine, desk, sectmk, talking, movie, total...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>454</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Let Kids Vote!</td>\n",
       "      <td>NYTF000020231231ejcv00063</td>\n",
       "      <td>[magazine, desk, sectmk, let, kid, vote, 454, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Christina Caron</td>\n",
       "      <td>428</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Are We Doomed to Disagree?</td>\n",
       "      <td>NYTF000020231231ejcv0005z</td>\n",
       "      <td>[magazine, desk, sectmk, doomed, disagree, 428...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nHello, Fourth Grade...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>319</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Hello, Fourth Graders! A Look Back at our Clas...</td>\n",
       "      <td>NYTF000020231231ejcv0005t</td>\n",
       "      <td>[magazine, desk, sectmk, hello, fourth, grader...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\nForeign Desk; SECTA\\n72 of Our Favorite Fa...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Times Insider</td>\n",
       "      <td>914</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>72 of Our Favorite Facts From 2023</td>\n",
       "      <td>NYTF000020231231ejcv0005r</td>\n",
       "      <td>[foreign, desk, secta, 72, favorite, fact, 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\nMoney and Business/Financial Desk; SECTBU\\...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Kashmir Hill</td>\n",
       "      <td>811</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>The Stalker Under Your Hood</td>\n",
       "      <td>NYTF000020231231ejcv0005n</td>\n",
       "      <td>[money, businessfinancial, desk, sectbu, stalk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\nBook Review Desk; SECTBR\\nPaperback Row\\n\\...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Shreya Chattopadhyay</td>\n",
       "      <td>431</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Paperback Row</td>\n",
       "      <td>NYTF000020231231ejcv0005g</td>\n",
       "      <td>[book, review, desk, sectbr, paperback, row, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n\\nNICHOLAS KRISTOF\\nEditorial Desk; SECTSR\\n...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nicholas Kristof</td>\n",
       "      <td>976</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Humans Made Progress In 2023</td>\n",
       "      <td>NYTF000020231231ejcv00052</td>\n",
       "      <td>[nicholas, kristof, editorial, desk, sectsr, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article              Date  \\\n",
       "0  \\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...  31 December 2023   \n",
       "1  \\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...  31 December 2023   \n",
       "2  \\n\\nMagazine Desk; SECTMK\\nTalking During Movi...  31 December 2023   \n",
       "3  \\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...  31 December 2023   \n",
       "4  \\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...  31 December 2023   \n",
       "5  \\n\\nMagazine Desk; SECTMK\\nHello, Fourth Grade...  31 December 2023   \n",
       "6  \\n\\nForeign Desk; SECTA\\n72 of Our Favorite Fa...  31 December 2023   \n",
       "7  \\n\\nMoney and Business/Financial Desk; SECTBU\\...  31 December 2023   \n",
       "8  \\n\\nBook Review Desk; SECTBR\\nPaperback Row\\n\\...  31 December 2023   \n",
       "9  \\n\\nNICHOLAS KRISTOF\\nEditorial Desk; SECTSR\\n...  31 December 2023   \n",
       "\n",
       "                 Auteur  Nombre de mots         Journal  \\\n",
       "0            Nick Tabor             529  New York Times   \n",
       "1         Wesley Morris             422  New York Times   \n",
       "2                  None             179  New York Times   \n",
       "3                  None             454  New York Times   \n",
       "4       Christina Caron             428  New York Times   \n",
       "5                  None             319  New York Times   \n",
       "6         Times Insider             914  New York Times   \n",
       "7          Kashmir Hill             811  New York Times   \n",
       "8  Shreya Chattopadhyay             431  New York Times   \n",
       "9      Nicholas Kristof             976  New York Times   \n",
       "\n",
       "                                               Titre  \\\n",
       "0  Copyright 2023 The New York Times Company.  Al...   \n",
       "1  When Jim Brown and Raquel Welch, Two Sexy Star...   \n",
       "2  Talking During Movies: Totally Evil or Part of...   \n",
       "3                                     Let Kids Vote!   \n",
       "4                         Are We Doomed to Disagree?   \n",
       "5  Hello, Fourth Graders! A Look Back at our Clas...   \n",
       "6                 72 of Our Favorite Facts From 2023   \n",
       "7                        The Stalker Under Your Hood   \n",
       "8                                      Paperback Row   \n",
       "9                       Humans Made Progress In 2023   \n",
       "\n",
       "                            ID  \\\n",
       "0    NYTF000020240104ejcv0000d   \n",
       "1    NYTF000020231231ejcv0006h   \n",
       "2   NYTF000020231231ejcv00064    \n",
       "3   NYTF000020231231ejcv00063    \n",
       "4   NYTF000020231231ejcv0005z    \n",
       "5   NYTF000020231231ejcv0005t    \n",
       "6   NYTF000020231231ejcv0005r    \n",
       "7   NYTF000020231231ejcv0005n    \n",
       "8   NYTF000020231231ejcv0005g    \n",
       "9   NYTF000020231231ejcv00052    \n",
       "\n",
       "                                Preprocessed_Article  \n",
       "0  [metropolitan, desk, sectmb, ambitious, public...  \n",
       "1  [magazine, desk, sectmm, jim, brown, raquel, w...  \n",
       "2  [magazine, desk, sectmk, talking, movie, total...  \n",
       "3  [magazine, desk, sectmk, let, kid, vote, 454, ...  \n",
       "4  [magazine, desk, sectmk, doomed, disagree, 428...  \n",
       "5  [magazine, desk, sectmk, hello, fourth, grader...  \n",
       "6  [foreign, desk, secta, 72, favorite, fact, 202...  \n",
       "7  [money, businessfinancial, desk, sectbu, stalk...  \n",
       "8  [book, review, desk, sectbr, paperback, row, s...  \n",
       "9  [nicholas, kristof, editorial, desk, sectsr, h...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA99lQ4VRIPj"
   },
   "source": [
    "## Analyse du sentiment des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EApSppc3ROou"
   },
   "source": [
    "### Sentiment général"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6163XiZXN2i"
   },
   "source": [
    "Le score donné varie de -1 à 1 avec -1 comme la négativité maximale et 1 comme la positivité maximale. 0 pour dire que le texte est neutre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "3cIikpC8RVPd",
    "outputId": "09d001bc-f208-4a1e-a073-8981d04cba49"
   },
   "outputs": [],
   "source": [
    "def calculate_sentiment(word_list):\n",
    "    # Convertir la liste de mots en une chaîne de caractères\n",
    "    text = ' '.join(word_list)\n",
    "    # Création d'une instance TextBlob\n",
    "    analysis = TextBlob(text)\n",
    "    # Retourner la polarité\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "# Appliquer la fonction au DataFrame\n",
    "data['Sentiment'] = data['Preprocessed_Article'].apply(calculate_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.080807\n",
       "1     0.096043\n",
       "2     0.129610\n",
       "3     0.068136\n",
       "4     0.081156\n",
       "        ...   \n",
       "95    0.017752\n",
       "96    0.062612\n",
       "97   -0.002605\n",
       "98    0.129660\n",
       "99   -0.021999\n",
       "Name: Sentiment, Length: 100, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ7rBPftRPf3"
   },
   "source": [
    "### Sentiment environnemental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idéal serait de récupérer un dictionnaire pré existant, spécialisé dans l'évaluation de termes écologique, qui attribue une score à chaque terme. La difficulté à trouver ce type de dictionnaire nous mène dans un premier temps à creuser d'autres pistes de substitution. Nous verrons plus tard si nous réussissons à trouver un dictionnaire préexistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative aux dictionnaires pré-existants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problème pour l'amélioration du dictionnaire : on ne trouve pas de dictionnaire préexistant avec comme spécialité l'environnement. Deux options : \n",
    "\n",
    " - Améliorer notre dictionnaire fait main:\n",
    "   - Avantage : On peut contrôler le poid associé à chaque mot, dans la note\n",
    "   - Inconvéniant : COnstruction peu rigoureuse, on peut avoir oublié des mots\n",
    "  \n",
    " - Utiliser un dictionnaire généraliste :\n",
    "   - Avantage : Construction plus rigoureuse, moins de chance d'oublier certains termes\n",
    "   - Inconvénient : Pas de contrôle sur le poid des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Le dictionnaire fait main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le moment, on ne trouve pas de dictionnaire pré-existant, dont chaque terme peut être associé à une note environnementale. On propose donc de construire nous même un dictionnaire, un en français et l'autre en anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dico_env_en = {\n",
    "    \n",
    "\"clean\": 2.40,\n",
    "\"ecological\": 2.35,\n",
    "\"sustainable\": 2.51,\n",
    "\"green\": 1.97,\n",
    "\"energy-efficient\": 1.87,\n",
    "\"renewable\": 2.35,\n",
    "\"responsible\": 2.29,\n",
    "\"conservation\": 2.03,\n",
    "\"biodiversity\": 3.04,\n",
    "\"healthy\": 1,\n",
    "\"organic\": 1.65,\n",
    "\"eco-friendly\": 2.13,\n",
    "\"environmentally friendly\": 1,\n",
    "\"efficient\": 1,\n",
    "\"innovative\": 1,\n",
    "\"ethical\": 1,\n",
    "\"fair\": 1,\n",
    "\"efficiency\": 1.76,\n",
    "\"social responsibility\": 1,\n",
    "\"solidarity\": 1,\n",
    "\"conscious spreading\": 2.03,\n",
    "\"clean energy\": 2.40,\n",
    "\"renewable energy\": 1,\n",
    "\"recycling\": 2.40,\n",
    "\"energy efficiency\": 1.76,\n",
    "\"circular economy\": 1.71,\n",
    "\"solar energy\": 1.81,\n",
    "\"wind energy\": 1.71,\n",
    "\"regeneration\": 2.13,\n",
    "\"preservation\": 2.83,\n",
    "\"restoration\": 2.61,\n",
    "\"rehabilitation\": 1.76,\n",
    "\"recovery\": 2.03,\n",
    "\"restorer\": 1.55,\n",
    "\"regenerator\": 2.13,\n",
    "\"revitalization\": 2.19,\n",
    "\"positive\": 1,\n",
    "\"beneficial\": 1,\n",
    "\"valorization\": 1.87,\n",
    "\"fulfillment\": 1.33,\n",
    "\"continuous improvement\": 2.13,\n",
    "\"prosperity\": 1.76,\n",
    "\"harmony\": 1,\n",
    "\"integrity\": 1,\n",
    "\"responsible consumption\": 2.29,\n",
    "\"eco-responsible\": 2.61,\n",
    "\"eco-conscious\": 2.29,\n",
    "\n",
    "\"pollution\": -3.36,\n",
    "\"waste\": -3.20,\n",
    "\"deforestation\": -3.28,\n",
    "\"greenhouse gas emissions\": 0.27,\n",
    "\"contamination\": -2.67,\n",
    "\"destructive\": -1,\n",
    "\"irresponsible\": -1,\n",
    "\"wasteful\": -3.41,\n",
    "\"harmful\": -2.77,\n",
    "\"toxic\": -3.31,\n",
    "\"deterioration\": -3.31,\n",
    "\"degradation\": -3.04,\n",
    "\"damaging\": -1,\n",
    "\"perilous\": -2.19,\n",
    "\"worrisome\": -1,\n",
    "\"catastrophic\": -1,\n",
    "\"catastrophe\": -1,\n",
    "\"dangerous\": -1,\n",
    "\"threat\": -1,\n",
    "\"risk\": -1,\n",
    "\"hazardous\": -1.28,\n",
    "\"inappropriate\": -2.11,\n",
    "\"inadequate\": -1,\n",
    "\"harm\": -1,\n",
    "\"damage\": -2.77,\n",
    "\"pollutant\": -3.31,\n",
    "\"deteriorate\": -2.99,\n",
    "\"disruption\": -1,\n",
    "\"disrespectful\": -2.93,\n",
    "\"malevolent\": -2.08,\n",
    "\"aggressive\": -1,\n",
    "\"ravager\": -3.20,\n",
    "\"spoil\": -2.13,\n",
    "\"disturb\": -1,\n",
    "\"irreparable\": -2.56,\n",
    "\"toxicity\": -3.79,\n",
    "\"unacceptable\": -1,\n",
    "\"ecological damage\": -1,\n",
    "\"illegal logging\": -0.48,\n",
    "\"overconsumption\": -3.15,\n",
    "\"resource plundering\": -1.56,\n",
    "\"environmental degradation\": -1,\n",
    "\"destroyed natural habitat\": 2.02,\n",
    "\"excessive exploitation\": -1.71,\n",
    "\"overexploitation\": -3.44,\n",
    "\"climate change\": -2.81,\n",
    "\"environmental denial\": -2.08,\n",
    "\"eco-innovation\": 1.97,\n",
    "\"irreparable\": -2.56,\n",
    "\"permaculture\": 1.87,\n",
    "\"eco-design\": 1.23,\n",
    "\"agroecology\": 1.97,\n",
    "\"recoverable\": 1.92,\n",
    "\"eco-tourism\": 0.96,\n",
    "\"eco-habitat\": 2.19,\n",
    "\"conscious consumption\": 1.71\n",
    "}\n",
    "\n",
    "#negation_list = [\"not\", \"no\", \"never\", \"none\", \"nil\", \"nothing\", \"nobody\", \"negative\", \"without\", \"more\", \"less\"]\n",
    "negation_list = [\n",
    "        \"aint\",\n",
    "        \"arent\",\n",
    "        \"cannot\",\n",
    "        \"cant\",\n",
    "        \"couldnt\",\n",
    "        \"darent\",\n",
    "        \"didnt\",\n",
    "        \"doesnt\",\n",
    "        \"ain't\",\n",
    "        \"aren't\",\n",
    "        \"can't\",\n",
    "        \"couldn't\",\n",
    "        \"daren't\",\n",
    "        \"didn't\",\n",
    "        \"doesn't\",\n",
    "        \"dont\",\n",
    "        \"hadnt\",\n",
    "        \"hasnt\",\n",
    "        \"havent\",\n",
    "        \"isnt\",\n",
    "        \"mightnt\",\n",
    "        \"mustnt\",\n",
    "        \"neither\",\n",
    "        \"don't\",\n",
    "        \"hadn't\",\n",
    "        \"hasn't\",\n",
    "        \"haven't\",\n",
    "        \"isn't\",\n",
    "        \"mightn't\",\n",
    "        \"mustn't\",\n",
    "        \"neednt\",\n",
    "        \"needn't\",\n",
    "        \"never\",\n",
    "        \"none\",\n",
    "        \"nope\",\n",
    "        \"nor\",\n",
    "        \"not\",\n",
    "        \"nothing\",\n",
    "        \"nowhere\",\n",
    "        \"oughtnt\",\n",
    "        \"shant\",\n",
    "        \"shouldnt\",\n",
    "        \"uhuh\",\n",
    "        \"wasnt\",\n",
    "        \"werent\",\n",
    "        \"oughtn't\",\n",
    "        \"shan't\",\n",
    "        \"shouldn't\",\n",
    "        \"uh-uh\",\n",
    "        \"wasn't\",\n",
    "        \"weren't\",\n",
    "        \"without\",\n",
    "        \"wont\",\n",
    "        \"wouldnt\",\n",
    "        \"won't\",\n",
    "        \"wouldn't\",\n",
    "        \"rarely\",\n",
    "        \"seldom\",\n",
    "        \"despite\",\n",
    "]\n",
    "\n",
    "##Constants##\n",
    "# (empirically derived mean sentiment intensity rating increase for booster words)\n",
    "B_INCR = 0.293\n",
    "B_DECR = -0.293\n",
    "\n",
    "BOOSTER_DICT = {\n",
    "        \"absolutely\": B_INCR,\n",
    "        \"amazingly\": B_INCR,\n",
    "        \"awfully\": B_INCR,\n",
    "        \"completely\": B_INCR,\n",
    "        \"considerably\": B_INCR,\n",
    "        \"decidedly\": B_INCR,\n",
    "        \"deeply\": B_INCR,\n",
    "        \"effing\": B_INCR,\n",
    "        \"enormously\": B_INCR,\n",
    "        \"entirely\": B_INCR,\n",
    "        \"especially\": B_INCR,\n",
    "        \"exceptionally\": B_INCR,\n",
    "        \"extremely\": B_INCR,\n",
    "        \"fabulously\": B_INCR,\n",
    "        \"flipping\": B_INCR,\n",
    "        \"flippin\": B_INCR,\n",
    "        \"fricking\": B_INCR,\n",
    "        \"frickin\": B_INCR,\n",
    "        \"frigging\": B_INCR,\n",
    "        \"friggin\": B_INCR,\n",
    "        \"fully\": B_INCR,\n",
    "        \"fucking\": B_INCR,\n",
    "        \"greatly\": B_INCR,\n",
    "        \"hella\": B_INCR,\n",
    "        \"highly\": B_INCR,\n",
    "        \"hugely\": B_INCR,\n",
    "        \"incredibly\": B_INCR,\n",
    "        \"intensely\": B_INCR,\n",
    "        \"majorly\": B_INCR,\n",
    "        \"more\": B_INCR,\n",
    "        \"most\": B_INCR,\n",
    "        \"particularly\": B_INCR,\n",
    "        \"purely\": B_INCR,\n",
    "        \"quite\": B_INCR,\n",
    "        \"really\": B_INCR,\n",
    "        \"remarkably\": B_INCR,\n",
    "        \"so\": B_INCR,\n",
    "        \"substantially\": B_INCR,\n",
    "        \"thoroughly\": B_INCR,\n",
    "        \"totally\": B_INCR,\n",
    "        \"tremendously\": B_INCR,\n",
    "        \"uber\": B_INCR,\n",
    "        \"unbelievably\": B_INCR,\n",
    "        \"unusually\": B_INCR,\n",
    "        \"utterly\": B_INCR,\n",
    "        \"very\": B_INCR,\n",
    "        \"almost\": B_DECR,\n",
    "        \"barely\": B_DECR,\n",
    "        \"hardly\": B_DECR,\n",
    "        \"just enough\": B_DECR,\n",
    "        \"kind of\": B_DECR,\n",
    "        \"kinda\": B_DECR,\n",
    "        \"kindof\": B_DECR,\n",
    "        \"kind-of\": B_DECR,\n",
    "        \"less\": B_DECR,\n",
    "        \"little\": B_DECR,\n",
    "        \"marginally\": B_DECR,\n",
    "        \"occasionally\": B_DECR,\n",
    "        \"partly\": B_DECR,\n",
    "        \"scarcely\": B_DECR,\n",
    "        \"slightly\": B_DECR,\n",
    "        \"somewhat\": B_DECR,\n",
    "        \"sort of\": B_DECR,\n",
    "        \"sorta\": B_DECR,\n",
    "        \"sortof\": B_DECR,\n",
    "        \"sort-of\": B_DECR,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dictionnaire généraliste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'explorer la puissance d'un dictionnaire pré exsitant, nous faisant le choix de considérer un dictionnaire pré-existant, même s'il n'est pas spécialisé dans l'environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK = Natural Language ToolKit\n",
    "\n",
    "SentiWordNet est une ressource lexicale qui associe des mots à des scores de sentiment en fonction de leur signification dans le contexte de WordNet, une base de données lexicale pour la langue anglaise. Chaque mot dans SentiWordNet est lié à des synsets (ensembles de synonymes) de WordNet, et chaque synset est associé à des scores de sentiment qui indiquent la positivité, la négativité et l'objectivité des mots dans ce synset. Le résultat de l'application de SentiWordNet est la sortie de 3 mesures : un taux de positivité, de négativité, et d'objectivité. La somme de ces 3 taux est de 1.\n",
    "\n",
    "Voici comment ces scores sont généralement calculés :\n",
    "\n",
    "    Étiquetage manuel dans WordNet : WordNet est une base de données lexicale qui organise les mots de la langue anglaise en synsets, en regroupant les mots ayant des significations similaires. Chaque mot dans WordNet est associé à des synsets, qui sont des ensembles de synonymes. Ces synsets sont ensuite annotés manuellement par des lexicographes, qui attribuent des étiquettes de partie du discours (POS) et des relations sémantiques aux mots.\n",
    "\n",
    "    Corrélation avec des corpus annotés pour le sentiment : Une fois que les mots ont été étiquetés dans WordNet, leur association avec des sentiments est déterminée en analysant des corpus de texte annotés pour le sentiment. Ces corpus contiennent des textes où les mots sont annotés avec des étiquettes de sentiment (positif, négatif, neutre). En analysant la distribution de ces mots dans les synsets de WordNet, on peut estimer la probabilité qu'un mot donné soit associé à un sentiment positif, négatif ou neutre.\n",
    "\n",
    "    Calcul des scores de sentiment : Les scores de sentiment dans SentiWordNet sont calculés en utilisant des techniques de pondération probabiliste. Pour chaque synset contenant un mot donné, les scores de positivité, de négativité et d'objectivité sont calculés en fonction de la fréquence d'apparition du mot dans des contextes positifs, négatifs et neutres respectivement, tels que déterminés par l'analyse des corpus annotés pour le sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/onyxia/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de sentiment pour le terme 'good':\n",
      "POS: 0.5, NEG: 0.0, OBJ: 0.5\n",
      "POS: 0.875, NEG: 0.0, OBJ: 0.125\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.75, NEG: 0.0, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 1.0, NEG: 0.0, OBJ: 0.0\n",
      "POS: 1.0, NEG: 0.0, OBJ: 0.0\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 1.0, NEG: 0.0, OBJ: 0.0\n",
      "POS: 0.75, NEG: 0.0, OBJ: 0.25\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 0.5, NEG: 0.0, OBJ: 0.5\n",
      "POS: 0.5, NEG: 0.0, OBJ: 0.5\n",
      "POS: 0.375, NEG: 0.0, OBJ: 0.625\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.625, NEG: 0.0, OBJ: 0.375\n",
      "POS: 0.75, NEG: 0.0, OBJ: 0.25\n",
      "POS: 0.75, NEG: 0.0, OBJ: 0.25\n",
      "POS: 0.875, NEG: 0.0, OBJ: 0.125\n",
      "POS: 0.5, NEG: 0.0, OBJ: 0.5\n",
      "POS: 0.375, NEG: 0.125, OBJ: 0.5\n",
      "POS: 0.75, NEG: 0.0, OBJ: 0.25\n",
      "POS: 0.375, NEG: 0.0, OBJ: 0.625\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "\n",
      "Scores de sentiment pour le terme 'bad':\n",
      "POS: 0.0, NEG: 0.875, OBJ: 0.125\n",
      "POS: 0.0, NEG: 0.625, OBJ: 0.375\n",
      "POS: 0.25, NEG: 0.25, OBJ: 0.5\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.625, OBJ: 0.375\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.625, OBJ: 0.375\n",
      "POS: 0.0, NEG: 0.5, OBJ: 0.5\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 1.0, OBJ: 0.0\n",
      "POS: 0.0, NEG: 0.375, OBJ: 0.625\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.0, NEG: 0.75, OBJ: 0.25\n",
      "POS: 0.125, NEG: 0.25, OBJ: 0.625\n",
      "POS: 0.125, NEG: 0.25, OBJ: 0.625\n",
      "\n",
      "Scores de sentiment pour le terme 'environment':\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "\n",
      "Scores de sentiment pour le terme 'technology':\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "\n",
      "Scores de sentiment pour le terme 'greenhouse':\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "\n",
      "Scores de sentiment pour le terme 'gases':\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "POS: 0.0, NEG: 0.0, OBJ: 1.0\n",
      "\n",
      "Aucun synset trouvé pour le terme 'greenhouse gas'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "# Termes à examiner\n",
    "terms = [\"good\", \"bad\", \"environment\", \"technology\",\"greenhouse\",\"gases\",\"greenhouse gas\"]\n",
    "\n",
    "for term in terms:\n",
    "    # Obtenir les synsets associés au terme\n",
    "    synsets = list(swn.senti_synsets(term))\n",
    "\n",
    "    if synsets:\n",
    "        print(f\"Scores de sentiment pour le terme '{term}':\")\n",
    "        for synset in synsets:\n",
    "            print(f\"POS: {synset.pos_score()}, NEG: {synset.neg_score()}, OBJ: {synset.obj_score()}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"Aucun synset trouvé pour le terme '{term}'.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut obtenir plusieurs scores de positivité, de négativité et d'objectivité en sortie pour plusieurs synsets associés à ce mot. Voici pourquoi cela se produit :\n",
    "\n",
    "    Polysemy : De nombreux mots ont plusieurs sens ou acceptions, ce qui est connu sous le nom de polysemy. Dans WordNet, chaque sens distinct d'un mot est représenté par un synset distinct. Par exemple, le mot \"bat\" peut se référer à l'animal chiroptère ou à l'objet utilisé pour frapper une balle dans certains sports. Chacun de ces sens serait représenté par un synset différent, et chaque synset peut avoir des scores de sentiment différents.\n",
    "\n",
    "    Ambiguïté : Même pour un sens spécifique d'un mot, il peut y avoir de l'ambiguïté dans la façon dont ce mot est utilisé dans différents contextes. Par exemple, le mot \"bank\" peut faire référence à une institution financière ou à une rive d'une rivière. Chacun de ces sens pourrait avoir des connotations différentes, ce qui pourrait se refléter dans les scores de sentiment associés aux synsets correspondants.\n",
    "\n",
    "    Composition de sens : Dans certains cas, un mot peut avoir des scores de sentiment différents en fonction de la composition de ses sens dans un synset particulier. Par exemple, le mot \"greenhouse\" peut être composé de deux sens : \"serre\" et \"gaz à effet de serre\". Chaque sens individuel pourrait avoir des scores de sentiment différents, et ces scores pourraient être combinés pour obtenir un score global pour le mot composé \"greenhouse\".\n",
    "\n",
    "Ainsi, lorsque vous exécutez le code sur un seul mot, vous pouvez obtenir plusieurs scores de sentiment en sortie en raison de la présence de plusieurs synsets associés à ce mot, de l'ambiguïté dans l'utilisation du mot, ou de la composition de ses sens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse : On a l'avantage d'avoir trois notes, qui représentent la part de positivité, de négativité et de neutralité du mot => Avancé par rapport à ce qu'on avait proposé. De plus, on considère qu'il y a plusieurs sens à chaque mot, d'où le fait qu'il y ait plusieurs évaluation pour chaque terme.\n",
    "    Avantage : Facilité d'utilisation : code très sommaire, rapidité d'éexcution, applique un travail de tokenisation, analyse de sentiment etc... en seulement quelques lignes de code. De plus, l'utilisation de cette méthode démontre une fléxibilité du code : il s'applique à un large choix de texte. l'algorithme choisi la note du mot en fonction du contexte ?\n",
    "    Problème : Performance : pour l'instant, on a pas les moyens de juger de la performance de ce code : on fait confiance au code, sans vraimetn savoir s'il fait du bon travail. De plus, la flexibilité (qui était un avantage), peut devenir un inconvéniant : on rappelle qu'on ne se situe pas dans un cadre de NLP spécialisé dans l'environnement.  les coefficients ne sont pas forcément bons, par exemple, gases n'est jamais négatif, toujours neutre... pourquoi ?\n",
    "\n",
    "On test maintenant sur une phrase entière :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de la phrase: (0.16182795698924732, 0.020833333333333332, 0.6506720430107527)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_sentiment_scores(term):\n",
    "    synsets = list(swn.senti_synsets(term))\n",
    "    if synsets:\n",
    "        pos_score = sum(s.pos_score() for s in synsets) / len(synsets)\n",
    "        neg_score = sum(s.neg_score() for s in synsets) / len(synsets)\n",
    "        obj_score = sum(s.obj_score() for s in synsets) / len(synsets)\n",
    "\n",
    "        # Normaliser les scores\n",
    "        total_score = pos_score + neg_score + obj_score\n",
    "        if total_score != 0:\n",
    "            pos_score /= total_score\n",
    "            neg_score /= total_score\n",
    "            obj_score /= total_score\n",
    "\n",
    "        return pos_score, neg_score, obj_score\n",
    "    else:\n",
    "        return 0, 0, 0\n",
    "\n",
    "def analyze_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    scores = []\n",
    "\n",
    "    for token in tokens:\n",
    "        pos_score, neg_score, obj_score = get_sentiment_scores(token)\n",
    "        scores.append((pos_score, neg_score, obj_score))\n",
    "\n",
    "    # Calculer les scores moyens pour la phrase\n",
    "    avg_pos_score = sum(score[0] for score in scores) / len(scores)\n",
    "    avg_neg_score = sum(score[1] for score in scores) / len(scores)\n",
    "    avg_obj_score = sum(score[2] for score in scores) / len(scores)\n",
    "\n",
    "    return avg_pos_score, avg_neg_score, avg_obj_score\n",
    "\n",
    "# Exemple d'utilisation\n",
    "phrase = \"Clean technology promotes sustainable development.\"\n",
    "score_phrase = analyze_sentence(phrase)\n",
    "print(\"Score de la phrase:\", score_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque un taux de positivité de 16%, contre un taux de négativité de 2%, ainsi qu'un taux de neutralité de 65%. C'est un résultat relativement encourageant, étant donné qu'on a proposé une phrase à l'algorithme qui semblait être positive d'un point de vue environnemental.\n",
    "\n",
    "On compare ce score au score qu'a la négation de la phrase testée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de la phrase: (0.12241263440860216, 0.019791666666666666, 0.6077956989247312)\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Clean technology doesn't promotes sustainable development.\"\n",
    "score_phrase = analyze_sentence(phrase)\n",
    "print(\"Score de la phrase:\", score_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse : Résultats moins encourageant. On observe en effet une baisse du taux de positivité, qui passe de 0.16 à 0.12, en revanche, le taux de négativité n'a pas augmenté, et reste faible, alors même que la phrase semble négative\n",
    "\n",
    "On test sur une autre phrase négative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de la phrase: (0.0078125, 0.046875, 0.1953125)\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Pfizer destroyes environement.\"\n",
    "score_phrase = analyze_sentence(phrase)\n",
    "print(\"Score de la phrase:\", score_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score positif très faible, mais le négatif également. Est-ce intéressant de faire un rapport des deux ? On reste incertain quant à la significativité de la construction de notre note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_jSzcowY2Uy"
   },
   "source": [
    "On suppose que l'on a un dictionnaire `Dico_env` contenant les mots environnementaux, associés avec un score $\\in [-1,1]$. Par ex: {'pollution': -1, 'conservation': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC0efxYkYiJA"
   },
   "source": [
    "#### Colonne environmental_sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wQrq34eQRM11",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'articles avec un score non nul: 81\n",
      "0     0.000000\n",
      "1     0.001267\n",
      "2    -0.025846\n",
      "3     0.000000\n",
      "4     0.000000\n",
      "        ...   \n",
      "95   -0.010920\n",
      "96   -0.010704\n",
      "97   -0.011534\n",
      "98    0.000000\n",
      "99    0.014246\n",
      "Name: environmental_sentiment_score, Length: 100, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def get_environmental_score(token_list, Dico_env):\n",
    "    score = 0\n",
    "    token_count = len(token_list)\n",
    "    i = 0\n",
    "\n",
    "    while i < token_count:\n",
    "        token = token_list[i].lower()  # Assurer la comparaison en minuscules\n",
    "        next_token = token_list[i + 1].lower() if i + 1 < token_count else \"\"\n",
    "        \n",
    "        # Gérer les boosters\n",
    "        booster_multiplier = BOOSTER_DICT.get(token, 1)\n",
    "        if booster_multiplier != 1 and next_token in Dico_env:\n",
    "            score += Dico_env[next_token] * (1 + booster_multiplier)\n",
    "            i += 1  # Passer le prochain token traité comme partie du booster\n",
    "        \n",
    "        # Gérer les négations\n",
    "        elif token in negation_list and next_token in Dico_env:\n",
    "            score -= Dico_env[next_token]  # Inverser le score du mot suivant\n",
    "            i += 1  # Passer le prochain token traité comme partie de la négation\n",
    "\n",
    "        # Gérer les mots normaux du dictionnaire\n",
    "        elif token in Dico_env:\n",
    "            score += Dico_env[token]\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    # Normaliser le score entre -1 et 1\n",
    "    if token_count > 0:\n",
    "        normalized_score = score / token_count\n",
    "        return max(min(normalized_score, 1), -1)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data['environmental_sentiment_score'] = data['Preprocessed_Article'].apply(lambda x: get_environmental_score(x, Dico_env_en))\n",
    "\n",
    "data[['environmental_sentiment_score']].to_csv('env_comm_score_articles.csv', index=False)\n",
    "\n",
    "print(\"Nombre d'articles avec un score non nul: \"+str(len(data[abs(data[\"environmental_sentiment_score\"])>0])))\n",
    "print(data['environmental_sentiment_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------- -------------------------------------------------------\n",
    "------------------------------------------------------- Inutile -------------------------------------------------------\n",
    "--------------------------------------------------------------------- -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tentative pour améliorer la précision de notre score environnemental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idée : reprendre le code de la fonction sentiment_analysis de VADER (code présent dans le notebook Score environnemental.ipynb) mais en essayant de le modifier pour qu'il prenne particulièrement en compte les termes environnementaux dans son analyse du sentiment. Pour ça on essaie deux solutions:\n",
    "\n",
    "* Modifier le dictionnaire initial de VADER (fichier vader_lexicon.txt) en rajoutan les mots environnementaux manquants + mettre un poids plus fort aux mots environnementaux ou moins fort aux autres.\n",
    "    \n",
    "* Modifier le texte initial en multipliant le nombre de mots environnementaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code pour multiplier le nombre de mots environnementaux dans le texte\n",
    "\n",
    "#def emphasize_environmental_terms(text, environmental_terms, repeat_factor=2):\n",
    "    #\"\"\"\n",
    "    #Identifies environmental terms and makes them more prominent in the text.\n",
    "    \n",
    "    #:param text: The original text.\n",
    "    #:param environmental_terms: A set or list of environmental terms to emphasize.\n",
    "    #:param repeat_factor: How many times to repeat the environmental terms. Default is 2.\n",
    "    #:return: Modified text with environmental terms emphasized.\n",
    "    #\"\"\"\n",
    "    #modified_words = []\n",
    "    #for word in text.split():\n",
    "    #    if word in environmental_terms:\n",
    "    #        # Repeat or tag the environmental word to make it more prominent\n",
    "    #        modified_word = ' '.join([word] * repeat_factor)  # Example: repeating the word\n",
    "    #        # You could also tag the word, e.g., word = 'ENVIRONMENTAL_' + word\n",
    "    #        modified_words.append(modified_word)\n",
    "    #    else:\n",
    "    #        modified_words.append(word)\n",
    "    #return ' '.join(modified_words)\n",
    "\n",
    "# Example usage:\n",
    "#text = \"The company focuses on pollution reduction and renewable energy sources.\"\n",
    "#modified_text = emphasize_environmental_terms(text, Dico_env_en)\n",
    "#print(modified_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random\n",
    "\n",
    "# Convert dictionary keys to a set\n",
    "#Dico_env_en_keys = set(Dico_env_en.keys())\n",
    "\n",
    "#lexicon_path = \"vader_lexicon_modified.txt\"\n",
    "\n",
    "#with open(lexicon_path, \"r+\") as file:\n",
    "#    existing_words = {line.split('\\t')[0] for line in file.readlines()}\n",
    "#    words_to_add = Dico_env_en_keys - existing_words\n",
    "\n",
    "#    if words_to_add:\n",
    "#        file.seek(0, 2)  # Move to the end of the file\n",
    "#        for word in words_to_add:\n",
    "#            score = round(random.uniform(-3, 3), 2)\n",
    "#            std_dev = round(random.uniform(0, 2), 5)\n",
    "#            score_list = [round(score + random.uniform(-std_dev, std_dev), 1) for _ in range(10)]\n",
    "#            line_to_add = f\"{word}\\t{score}\\t{std_dev}\\t{score_list}\\n\"\n",
    "#            file.write(line_to_add)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code possiblement inutile  (Avant de finaliser le cleaning de la base de donnée, le premier code pour attribuer un score environnemental ne marchait pas, donc on a proposé un deuxième code qui semblait fonctionner. Maintenant que la base de donné est clean, le premier code fonction... donc le code qui suit est possiblement inutile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score environnemental semble être nul pour une partie des articles, on regarde si le score est parfois différent de 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somme_valeurs_absolues = data['environmental_sentiment_score'].abs().sum()\n",
    "#print(\"La somme des valeurs absolue de la variable du score environnement est :\", somme_valeurs_absolues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui pourrait expliquer le fait qu'aucune / très peu de texte, ai une note différente de 0, est le fait que les éléments du dictionnaire n'ont pas été prétraité (tokenisation etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la fonction preprocess_text aux clés du dictionnaire\n",
    "#preprocessed_dict_en = {preprocess_text(key): value for key, value in Dico_env_en.items()}\n",
    "#preprocessed_dict_fr = {preprocess_text(key): value for key, value in Dico_env_fr.items()}\n",
    "\n",
    "\n",
    "# Affichage du dictionnaire après prétraitement des clés\n",
    "#print(preprocessed_dict_en)\n",
    "#print(preprocessed_dict_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention, il ne semble pas que la tokenisation ou la lemnisation ait fonctionné..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_environmental_score(token_list, Dico_env):\n",
    "#    score = 0\n",
    "#    token_count = len(token_list)\n",
    "#\n",
    " #   for token in token_list:\n",
    "  #      if token in Dico_env:\n",
    "   #         score += Dico_env[token]\n",
    "#\n",
    "    # Normalize the score to be between -1 and 1\n",
    "#    if token_count > 0:\n",
    " #       normalized_score = score / token_count\n",
    "  #      return max(min(normalized_score, 1), -1)\n",
    "   # else:\n",
    "    #    return 0\n",
    "\n",
    "#data['environmental_sentiment_score'] = data['Preprocessed_Article'].apply(lambda x: get_environmental_score(x, preprocessed_dict_en))\n",
    "\n",
    "#data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#somme_valeurs_absolues = data['environmental_sentiment_score'].abs().sum()\n",
    "#print(\"La somme des valeurs absolue de la variable du score environnement est :\", somme_valeurs_absolues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Est ce que les éléments de la colonne Processed_Article contiennent une liste de mots qui composent l'article, ou bien c'est un seul élément (une grande chaine de charactère) ? Si c'est une grande chaîne de caractère, ça peut poser problème car pour construire la variable de score envorionnementale, on compare un mot à un texte => Donc le score environnemental reste nul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Preprocessed_Article'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Preprocessed_Article'][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les élements de la variable Processed_Article est donc une longue et unique chaîne de caractère. On créer un code qui prends une chaîne de caractère en entrée, et qui renvoie une liste de mots qui composent cet article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def mots_dans_article(article):\n",
    "    # Diviser l'article en mots en utilisant l'espace comme délimiteur\n",
    "#    mots = article.split()\n",
    "\n",
    "    # Retourner la liste des mots\n",
    "#    return mots\n",
    "\n",
    "#data['Preprocessed_Article_split']=data['Preprocessed_Article'].apply(lambda x: mots_dans_article(x))\n",
    "\n",
    "#data['Preprocessed_Article_split']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['environmental_sentiment_score_test'] = data['Preprocessed_Article_split'].apply(lambda x: get_environmental_score(x, preprocessed_dict_en))\n",
    "\n",
    "#data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amélioration du score environnemental => Certains score ne sont pas nul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyse des entreprises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On essaie de voir maintenant quelles entreprises ont été citées dans le texte :\n",
    "On peut supposer qu'on a une colonne avec toutes les entreprises qui ont été citées dans le texte, ou dans le titre : entr_citées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      \n",
       "1      \n",
       "2      \n",
       "3      \n",
       "4      \n",
       "     ..\n",
       "95     \n",
       "96     \n",
       "97     \n",
       "98     \n",
       "99     \n",
       "Name: Entreprises_citées, Length: 100, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importer pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Charger le DataFrame des entreprises\n",
    "df_entreprises = pd.read_csv(\"Firms.csv\")\n",
    "\n",
    "# Mettre en minuscules les noms des entreprises\n",
    "df_entreprises['Company'] = df_entreprises['Company'].str.lower()\n",
    "\n",
    "# Copier la colonne 'Preprocessed_Article' du DataFrame 'data' dans un nouveau DataFrame\n",
    "df_articles = data[['Preprocessed_Article']].copy()\n",
    "\n",
    "# Appliquer la mise en minuscule aux chaînes de caractères dans la liste 'Preprocessed_Article'\n",
    "df_articles['Preprocessed_Article'] = df_articles['Preprocessed_Article'].apply(\n",
    "    lambda x: [word.lower() if isinstance(word, str) else word for word in x]\n",
    ")\n",
    "\n",
    "# Initialisation de la nouvelle colonne pour stocker les noms des entreprises citées dans chaque article\n",
    "df_articles['Entreprises_citées'] = ''\n",
    "\n",
    "# Boucle à travers les articles\n",
    "for index, article in df_articles.iterrows():\n",
    "    entreprises_citées = []\n",
    "    contenu_article = article['Preprocessed_Article']\n",
    "    \n",
    "    # Vérifier si le contenu de l'article est une chaîne de caractères\n",
    "    if isinstance(contenu_article, str):\n",
    "        # Vérifier la présence de chaque entreprise dans le contenu de l'article\n",
    "        for index_ent, entreprise in df_entreprises.iterrows():\n",
    "            nom_entreprise = entreprise['Company']\n",
    "            if isinstance(nom_entreprise, str) and nom_entreprise in contenu_article:\n",
    "                entreprises_citées.append(nom_entreprise)\n",
    "    \n",
    "    # Stocker les entreprises citées dans la nouvelle colonne\n",
    "    df_articles.at[index, 'Entreprises_citées'] = ', '.join(entreprises_citées)\n",
    "\n",
    "# Afficher les articles avec les entreprises citées\n",
    "df_articles['Entreprises_citées']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche maintenant à associer à chaque article, une entreprise spécifique (même s'il est cité plusieurs entreprises)\n",
    "\n",
    "On part du principe qu'un article qui ne cite aucune entreprise n'est pas pertinent à étudier => il n'y aurait aucune communication verte\n",
    "Si jamais un article ne cite qu'une seule entreprise, alors il ne peut y avoir (ou pas) communication verte que sur cette entreprise (mais pas de communication verte de plusieurs entreprises dans un seul article)\n",
    "Si jamais il y a plusieurs entreprises citées dans un seul articles, alors on traitera ces données à part, en pensant qu'il peut y avoir plusieurs communications vertes, venant de différentes entreprises, et tout cela dans un seul article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'articles par nombre d'entreprises citées :\n",
      "Nombre_entreprises_citées\n",
      "0    100\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Résumé du nombre d'articles par nombre d'entreprises citées :\n",
      "Aucune entreprise citée : 100\n"
     ]
    }
   ],
   "source": [
    "# Ajouter une colonne pour le nombre d'entreprises citées dans chaque article\n",
    "df_articles['Nombre_entreprises_citées'] = df_articles['Entreprises_citées'].apply(lambda x: 0 if x == '' else x.count(',') + 1)\n",
    "\n",
    "# Compter le nombre d'articles par nombre d'entreprises citées\n",
    "comptage_entreprises = df_articles['Nombre_entreprises_citées'].value_counts()\n",
    "\n",
    "# Afficher le comptage des articles par nombre d'entreprises citées\n",
    "print(\"Nombre d'articles par nombre d'entreprises citées :\")\n",
    "print(comptage_entreprises)\n",
    "\n",
    "# Afficher le nombre d'articles qui ne citent aucune entreprise, qui en citent une seule, etc.\n",
    "print(\"\\nRésumé du nombre d'articles par nombre d'entreprises citées :\")\n",
    "print(\"Aucune entreprise citée :\", comptage_entreprises.get(0, 0))\n",
    "for i in range(1, max(comptage_entreprises.index) + 1):\n",
    "    print(f\"{i} entreprise(s) citée(s) :\", comptage_entreprises.get(i, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "      <th>Entreprises_citées</th>\n",
       "      <th>Nombre_entreprises_citées</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[metropolitan, desk, sectmb, ambitious, public...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[magazine, desk, sectmm, jim, brown, raquel, w...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[magazine, desk, sectmk, talking, movie, total...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[magazine, desk, sectmk, let, kid, vote, 454, ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[magazine, desk, sectmk, doomed, disagree, 428...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[global, health, science, desk, sectd, new, ho...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[foreign, desk, secta, hotel, intended, house,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[businessfinancial, desk, sectb, eu, open, inv...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[trilobite, science, desk, sectd, proof, wine,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[impossible, energy, transition, mario, loyola...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Preprocessed_Article Entreprises_citées  \\\n",
       "0   [metropolitan, desk, sectmb, ambitious, public...                      \n",
       "1   [magazine, desk, sectmm, jim, brown, raquel, w...                      \n",
       "2   [magazine, desk, sectmk, talking, movie, total...                      \n",
       "3   [magazine, desk, sectmk, let, kid, vote, 454, ...                      \n",
       "4   [magazine, desk, sectmk, doomed, disagree, 428...                      \n",
       "..                                                ...                ...   \n",
       "95  [global, health, science, desk, sectd, new, ho...                      \n",
       "96  [foreign, desk, secta, hotel, intended, house,...                      \n",
       "97  [businessfinancial, desk, sectb, eu, open, inv...                      \n",
       "98  [trilobite, science, desk, sectd, proof, wine,...                      \n",
       "99  [impossible, energy, transition, mario, loyola...                      \n",
       "\n",
       "    Nombre_entreprises_citées  \n",
       "0                           0  \n",
       "1                           0  \n",
       "2                           0  \n",
       "3                           0  \n",
       "4                           0  \n",
       "..                        ...  \n",
       "95                          0  \n",
       "96                          0  \n",
       "97                          0  \n",
       "98                          0  \n",
       "99                          0  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Company</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US7170811035</td>\n",
       "      <td>pfizer inc</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US58933Y1055</td>\n",
       "      <td>merck &amp; co inc</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GB0009252882</td>\n",
       "      <td>gsk plc</td>\n",
       "      <td>Unable to resolve all requested identifiers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US5324571083</td>\n",
       "      <td>eli lilly and co</td>\n",
       "      <td>United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE000BAY0017</td>\n",
       "      <td>bayer ag</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ISIN           Company  \\\n",
       "0  US7170811035        pfizer inc   \n",
       "1  US58933Y1055    merck & co inc   \n",
       "2  GB0009252882           gsk plc   \n",
       "3  US5324571083  eli lilly and co   \n",
       "4  DE000BAY0017          bayer ag   \n",
       "\n",
       "                                        Country  \n",
       "0                      United States of America  \n",
       "1                      United States of America  \n",
       "2  Unable to resolve all requested identifiers.  \n",
       "3                      United States of America  \n",
       "4                                       Germany  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entreprises.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il semblerait qu'aucune entreprise n'ait été trouvé dans aucun des articles. Peut être qu'elles ne sont pas cité de la même manière dans la base de données des articles, et dans celles des entreprises. On cherche à améliorer l'identification des entreprises dans les articles. Voici des suggestions d'amélioration :\n",
    "\n",
    "- Tokenization et Stemming : Utilisez des techniques de traitement de langage naturel (NLP) telles que la tokenization et le stemming pour normaliser les noms des entreprises dans les articles et dans la base de données avant de comparer les noms. Cela peut vous aider à trouver des correspondances malgré les variations de formulation.\n",
    "\n",
    "- Algorithmes de correspondance de chaînes : Des algorithmes comme Levenshtein Distance ou Jaccard Similarity peuvent être utilisés pour mesurer la similarité entre deux chaînes de caractères. Vous pouvez utiliser ces mesures pour comparer les noms d'entreprises dans les articles avec ceux dans la base de données et établir des seuils de similarité acceptables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, on essaie d'appliquer les méthodes de NLP aux noms fournis des entreprises :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/onyxia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Vérifier si le texte est une chaîne de caractères\n",
    "    if isinstance(text, str):\n",
    "        # Minuscule\n",
    "        text = text.lower()\n",
    "        # Supprimer la ponctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Tokenization\n",
    "        words = word_tokenize(text)\n",
    "        # Suppression des stop-words\n",
    "        filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "        # Lemmatisation\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_output = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "        return lemmatized_output\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "df_entreprises['Preprocessed_Company'] = df_entreprises['Company'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Company</th>\n",
       "      <th>Country</th>\n",
       "      <th>Preprocessed_Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US7170811035</td>\n",
       "      <td>pfizer inc</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>[pfizer, inc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US58933Y1055</td>\n",
       "      <td>merck &amp; co inc</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>[merck, co, inc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GB0009252882</td>\n",
       "      <td>gsk plc</td>\n",
       "      <td>Unable to resolve all requested identifiers.</td>\n",
       "      <td>[gsk, plc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US5324571083</td>\n",
       "      <td>eli lilly and co</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>[eli, lilly, co]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE000BAY0017</td>\n",
       "      <td>bayer ag</td>\n",
       "      <td>Germany</td>\n",
       "      <td>[bayer, ag]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13294</th>\n",
       "      <td>US87410C1045</td>\n",
       "      <td>talaris therapeutics inc</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>[talaris, therapeutic, inc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13295</th>\n",
       "      <td>CNE100002DP3</td>\n",
       "      <td>shenzhen senior technology material co ltd</td>\n",
       "      <td>China</td>\n",
       "      <td>[shenzhen, senior, technology, material, co, ltd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13296</th>\n",
       "      <td>AU0000181984</td>\n",
       "      <td>vulcan steel ltd</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>[vulcan, steel, ltd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13297</th>\n",
       "      <td>TH0042010007</td>\n",
       "      <td>bangkok insurance pcl</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>[bangkok, insurance, pcl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13298</th>\n",
       "      <td>JP3766400000</td>\n",
       "      <td>hakuto co ltd</td>\n",
       "      <td>Japan</td>\n",
       "      <td>[hakuto, co, ltd]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13299 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ISIN                                     Company  \\\n",
       "0      US7170811035                                  pfizer inc   \n",
       "1      US58933Y1055                              merck & co inc   \n",
       "2      GB0009252882                                     gsk plc   \n",
       "3      US5324571083                            eli lilly and co   \n",
       "4      DE000BAY0017                                    bayer ag   \n",
       "...             ...                                         ...   \n",
       "13294  US87410C1045                    talaris therapeutics inc   \n",
       "13295  CNE100002DP3  shenzhen senior technology material co ltd   \n",
       "13296  AU0000181984                            vulcan steel ltd   \n",
       "13297  TH0042010007                       bangkok insurance pcl   \n",
       "13298  JP3766400000                               hakuto co ltd   \n",
       "\n",
       "                                            Country  \\\n",
       "0                          United States of America   \n",
       "1                          United States of America   \n",
       "2      Unable to resolve all requested identifiers.   \n",
       "3                          United States of America   \n",
       "4                                           Germany   \n",
       "...                                             ...   \n",
       "13294                      United States of America   \n",
       "13295                                         China   \n",
       "13296                                   New Zealand   \n",
       "13297                                      Thailand   \n",
       "13298                                         Japan   \n",
       "\n",
       "                                    Preprocessed_Company  \n",
       "0                                          [pfizer, inc]  \n",
       "1                                       [merck, co, inc]  \n",
       "2                                             [gsk, plc]  \n",
       "3                                       [eli, lilly, co]  \n",
       "4                                            [bayer, ag]  \n",
       "...                                                  ...  \n",
       "13294                        [talaris, therapeutic, inc]  \n",
       "13295  [shenzhen, senior, technology, material, co, ltd]  \n",
       "13296                               [vulcan, steel, ltd]  \n",
       "13297                          [bangkok, insurance, pcl]  \n",
       "13298                                  [hakuto, co, ltd]  \n",
       "\n",
       "[13299 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_entreprises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser une nouvelle colonne dans df_articles pour stocker les entreprises identifiées\n",
    "df_articles['Entreprises_Identifiees'] = None\n",
    "\n",
    "# Parcourir chaque article dans la base de données\n",
    "for index, row_article in df_articles.iterrows():\n",
    "    article = row_article['Preprocessed_Article']  # Contenu de l'article prétraité\n",
    "    entreprises_identifiees = []  # Liste pour stocker les entreprises identifiées pour cet article\n",
    "    \n",
    "    # Parcourir chaque entreprise dans votre DataFrame df\n",
    "    for index_ent, row_ent in df_entreprises.iterrows():\n",
    "        companies = row_ent['Company']  # Nom de l'entreprise correspondante à cet article\n",
    "        keywords = row_ent['Preprocessed_Company']  # Liste de mots-clés prétraités\n",
    "        \n",
    "        # Parcourir chaque mot-clé dans la liste des mots-clés prétraités\n",
    "        for keyword in keywords:\n",
    "            # Vérifiez si le mot-clé est présent dans l'article\n",
    "            if keyword in article:\n",
    "                # Ajouter l'entreprise à la liste des entreprises identifiées pour cet article\n",
    "                entreprises_identifiees.append(companies)\n",
    "    \n",
    "    # Mettre à jour la colonne 'Entreprises_Identifiees' avec les entreprises identifiées pour cet article\n",
    "    df_articles.at[index, 'Entreprises_Identifiees'] = entreprises_identifiees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles['Nombre_Entreprises_Identifiees'] = df_articles['Entreprises_Identifiees'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "      <th>Entreprises_citées</th>\n",
       "      <th>Nombre_entreprises_citées</th>\n",
       "      <th>Entreprises_Identifiees</th>\n",
       "      <th>Nombre_Entreprises_Identifiees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[metropolitan, desk, sectmb, ambitious, public...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[anglo american plc, british american tobacco ...</td>\n",
       "      <td>2784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[magazine, desk, sectmm, jim, brown, raquel, w...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[general motors co, anglo american plc, philip...</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[magazine, desk, sectmk, talking, movie, total...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, tokyo e...</td>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[magazine, desk, sectmk, let, kid, vote, 454, ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[anglo american plc, british american tobacco ...</td>\n",
       "      <td>796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[magazine, desk, sectmk, doomed, disagree, 428...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[roche holding ag, hsbc holdings plc, mercedes...</td>\n",
       "      <td>2872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[global, health, science, desk, sectd, new, ho...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[takeda pharmaceutical co ltd, rio tinto plc, ...</td>\n",
       "      <td>5011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[foreign, desk, secta, hotel, intended, house,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[general motors co, international business mac...</td>\n",
       "      <td>2653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[businessfinancial, desk, sectb, eu, open, inv...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, mercede...</td>\n",
       "      <td>2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[trilobite, science, desk, sectd, proof, wine,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, sumitom...</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[impossible, energy, transition, mario, loyola...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[pfizer inc, merck &amp; co inc, dow inc, dow inc,...</td>\n",
       "      <td>5443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Preprocessed_Article Entreprises_citées  \\\n",
       "0   [metropolitan, desk, sectmb, ambitious, public...                      \n",
       "1   [magazine, desk, sectmm, jim, brown, raquel, w...                      \n",
       "2   [magazine, desk, sectmk, talking, movie, total...                      \n",
       "3   [magazine, desk, sectmk, let, kid, vote, 454, ...                      \n",
       "4   [magazine, desk, sectmk, doomed, disagree, 428...                      \n",
       "..                                                ...                ...   \n",
       "95  [global, health, science, desk, sectd, new, ho...                      \n",
       "96  [foreign, desk, secta, hotel, intended, house,...                      \n",
       "97  [businessfinancial, desk, sectb, eu, open, inv...                      \n",
       "98  [trilobite, science, desk, sectd, proof, wine,...                      \n",
       "99  [impossible, energy, transition, mario, loyola...                      \n",
       "\n",
       "    Nombre_entreprises_citées  \\\n",
       "0                           0   \n",
       "1                           0   \n",
       "2                           0   \n",
       "3                           0   \n",
       "4                           0   \n",
       "..                        ...   \n",
       "95                          0   \n",
       "96                          0   \n",
       "97                          0   \n",
       "98                          0   \n",
       "99                          0   \n",
       "\n",
       "                              Entreprises_Identifiees  \\\n",
       "0   [anglo american plc, british american tobacco ...   \n",
       "1   [general motors co, anglo american plc, philip...   \n",
       "2   [international business machines corp, tokyo e...   \n",
       "3   [anglo american plc, british american tobacco ...   \n",
       "4   [roche holding ag, hsbc holdings plc, mercedes...   \n",
       "..                                                ...   \n",
       "95  [takeda pharmaceutical co ltd, rio tinto plc, ...   \n",
       "96  [general motors co, international business mac...   \n",
       "97  [international business machines corp, mercede...   \n",
       "98  [international business machines corp, sumitom...   \n",
       "99  [pfizer inc, merck & co inc, dow inc, dow inc,...   \n",
       "\n",
       "    Nombre_Entreprises_Identifiees  \n",
       "0                             2784  \n",
       "1                             1035  \n",
       "2                              554  \n",
       "3                              796  \n",
       "4                             2872  \n",
       "..                             ...  \n",
       "95                            5011  \n",
       "96                            2653  \n",
       "97                            2544  \n",
       "98                            1265  \n",
       "99                            5443  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsqu'on observe les deux méthodes, on voit que leur résultat est trop extrême : soit on identifie aucune entreprise, soit on en identifie beaucoup trop... Peut être que la seconde méthode énonce beaucoup trop d'entreprises car elle cherche des termes qui ne sont pas vraiment dans le nom de l'entreprise (exemple : pour pfizer, dont le nom de compagnie est Pfizer Inc, le code va également chercher les termes Inc. Ou encore, pour coca cola, dont le réel nom est coca cola co, on va chercher des termes tels que \"co\"). Pour résoudre ce problème, on propose de ne cherche à identifier que le premier terme de la liste des entreprises :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser une nouvelle colonne dans df_articles pour stocker les entreprises identifiées\n",
    "df_articles['Entreprises_Identifiees_2'] = None\n",
    "\n",
    "# Parcourir chaque article dans la base de données\n",
    "for index, row_article in df_articles.iterrows():\n",
    "    article = row_article['Preprocessed_Article']  # Contenu de l'article prétraité\n",
    "    entreprises_identifiees = []  # Liste pour stocker les entreprises identifiées pour cet article\n",
    "    \n",
    "    # Parcourir chaque entreprise dans votre DataFrame df\n",
    "    for index_ent, row_ent in df_entreprises.iterrows():\n",
    "        companies = row_ent['Company']  # Nom de l'entreprise correspondante à cet article\n",
    "        keywords = row_ent['Preprocessed_Company']  # Liste de mots-clés prétraités\n",
    "        \n",
    "        # Utiliser seulement le premier mot-clé\n",
    "        if keywords:  # Vérifiez si la liste des mots-clés n'est pas vide\n",
    "            first_keyword = keywords[0]  # Prenez le premier mot-clé\n",
    "        \n",
    "            # Vérifiez si le mot-clé est présent dans l'article\n",
    "            if first_keyword in article:\n",
    "                # Ajouter l'entreprise à la liste des entreprises identifiées pour cet article\n",
    "                entreprises_identifiees.append(companies)\n",
    "    \n",
    "    # Mettre à jour la colonne 'Entreprises_Identifiees_2' avec les entreprises identifiées pour cet article\n",
    "    df_articles.at[index, 'Entreprises_Identifiees_2'] = entreprises_identifiees\n",
    "    df_articles['Nombre_Entreprises_Identifiees_2'] = df_articles['Entreprises_Identifiees_2'].apply(lambda x: len(x) if x is not None else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "      <th>Entreprises_citées</th>\n",
       "      <th>Nombre_entreprises_citées</th>\n",
       "      <th>Entreprises_Identifiees</th>\n",
       "      <th>Nombre_Entreprises_Identifiees</th>\n",
       "      <th>Entreprises_Identifiees_2</th>\n",
       "      <th>Nombre_Entreprises_Identifiees_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[metropolitan, desk, sectmb, ambitious, public...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[anglo american plc, british american tobacco ...</td>\n",
       "      <td>2784</td>\n",
       "      <td>[american international group inc, american ex...</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[magazine, desk, sectmm, jim, brown, raquel, w...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[general motors co, anglo american plc, philip...</td>\n",
       "      <td>1035</td>\n",
       "      <td>[general motors co, general electric co, ameri...</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[magazine, desk, sectmk, talking, movie, total...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, tokyo e...</td>\n",
       "      <td>554</td>\n",
       "      <td>[new gold inc, w. r. berkley corp, city develo...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[magazine, desk, sectmk, let, kid, vote, 454, ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[anglo american plc, british american tobacco ...</td>\n",
       "      <td>796</td>\n",
       "      <td>[american international group inc, american ex...</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[magazine, desk, sectmk, doomed, disagree, 428...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[roche holding ag, hsbc holdings plc, mercedes...</td>\n",
       "      <td>2872</td>\n",
       "      <td>[u.s. bancorp, best buy co inc, new gold inc, ...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[global, health, science, desk, sectd, new, ho...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[takeda pharmaceutical co ltd, rio tinto plc, ...</td>\n",
       "      <td>5011</td>\n",
       "      <td>[rio tinto plc, international business machine...</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[foreign, desk, secta, hotel, intended, house,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[general motors co, international business mac...</td>\n",
       "      <td>2653</td>\n",
       "      <td>[general motors co, international business mac...</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[businessfinancial, desk, sectb, eu, open, inv...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, mercede...</td>\n",
       "      <td>2544</td>\n",
       "      <td>[marks and spencer group plc, united states st...</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[trilobite, science, desk, sectd, proof, wine,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, sumitom...</td>\n",
       "      <td>1265</td>\n",
       "      <td>[best buy co inc, southwest airlines co, regio...</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[impossible, energy, transition, mario, loyola...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[pfizer inc, merck &amp; co inc, dow inc, dow inc,...</td>\n",
       "      <td>5443</td>\n",
       "      <td>[dow inc, international business machines corp...</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Preprocessed_Article Entreprises_citées  \\\n",
       "0   [metropolitan, desk, sectmb, ambitious, public...                      \n",
       "1   [magazine, desk, sectmm, jim, brown, raquel, w...                      \n",
       "2   [magazine, desk, sectmk, talking, movie, total...                      \n",
       "3   [magazine, desk, sectmk, let, kid, vote, 454, ...                      \n",
       "4   [magazine, desk, sectmk, doomed, disagree, 428...                      \n",
       "..                                                ...                ...   \n",
       "95  [global, health, science, desk, sectd, new, ho...                      \n",
       "96  [foreign, desk, secta, hotel, intended, house,...                      \n",
       "97  [businessfinancial, desk, sectb, eu, open, inv...                      \n",
       "98  [trilobite, science, desk, sectd, proof, wine,...                      \n",
       "99  [impossible, energy, transition, mario, loyola...                      \n",
       "\n",
       "    Nombre_entreprises_citées  \\\n",
       "0                           0   \n",
       "1                           0   \n",
       "2                           0   \n",
       "3                           0   \n",
       "4                           0   \n",
       "..                        ...   \n",
       "95                          0   \n",
       "96                          0   \n",
       "97                          0   \n",
       "98                          0   \n",
       "99                          0   \n",
       "\n",
       "                              Entreprises_Identifiees  \\\n",
       "0   [anglo american plc, british american tobacco ...   \n",
       "1   [general motors co, anglo american plc, philip...   \n",
       "2   [international business machines corp, tokyo e...   \n",
       "3   [anglo american plc, british american tobacco ...   \n",
       "4   [roche holding ag, hsbc holdings plc, mercedes...   \n",
       "..                                                ...   \n",
       "95  [takeda pharmaceutical co ltd, rio tinto plc, ...   \n",
       "96  [general motors co, international business mac...   \n",
       "97  [international business machines corp, mercede...   \n",
       "98  [international business machines corp, sumitom...   \n",
       "99  [pfizer inc, merck & co inc, dow inc, dow inc,...   \n",
       "\n",
       "    Nombre_Entreprises_Identifiees  \\\n",
       "0                             2784   \n",
       "1                             1035   \n",
       "2                              554   \n",
       "3                              796   \n",
       "4                             2872   \n",
       "..                             ...   \n",
       "95                            5011   \n",
       "96                            2653   \n",
       "97                            2544   \n",
       "98                            1265   \n",
       "99                            5443   \n",
       "\n",
       "                            Entreprises_Identifiees_2  \\\n",
       "0   [american international group inc, american ex...   \n",
       "1   [general motors co, general electric co, ameri...   \n",
       "2   [new gold inc, w. r. berkley corp, city develo...   \n",
       "3   [american international group inc, american ex...   \n",
       "4   [u.s. bancorp, best buy co inc, new gold inc, ...   \n",
       "..                                                ...   \n",
       "95  [rio tinto plc, international business machine...   \n",
       "96  [general motors co, international business mac...   \n",
       "97  [marks and spencer group plc, united states st...   \n",
       "98  [best buy co inc, southwest airlines co, regio...   \n",
       "99  [dow inc, international business machines corp...   \n",
       "\n",
       "    Nombre_Entreprises_Identifiees_2  \n",
       "0                                394  \n",
       "1                                320  \n",
       "2                                 60  \n",
       "3                                130  \n",
       "4                                 92  \n",
       "..                               ...  \n",
       "95                               640  \n",
       "96                               345  \n",
       "97                               252  \n",
       "98                               165  \n",
       "99                               440  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On arrive à réduire le nombre de compagnies identifiées... sans doute parce qu'on ne prends pas en compte les 'Inc', 'Co' ... On peut sans doute encore améliorer ce code de manière à affiner la manière dont on identifie nos entreprises. Pour cela, on utilise la méthode du Fuzzy matching.\n",
    "\n",
    "Le score de similarité calculé par la méthode fuzzywuzzy est basé sur différentes techniques d'évaluation de la similarité entre deux chaînes de caractères. La méthode principale utilisée par fuzzywuzzy pour calculer le score de similarité est la métrique de similarité de Levenshtein, également connue sous le nom de distance d'édition.\n",
    "\n",
    "La distance de Levenshtein entre deux chaînes est le nombre minimum d'opérations d'insertion, de suppression ou de substitution nécessaires pour passer d'une chaîne à l'autre. Plus ce nombre est faible, plus les chaînes sont similaires.\n",
    "\n",
    "Voici les étapes de calcul du score de similarité avec la méthode de Levenshtein :\n",
    "\n",
    "    Calcul de la distance de Levenshtein : Tout d'abord, la distance de Levenshtein entre les deux chaînes est calculée. Cela implique de déterminer le nombre minimum d'opérations nécessaires pour transformer une chaîne en une autre.\n",
    "\n",
    "    Normalisation du score : Ensuite, le score de similarité est calculé en normalisant la distance de Levenshtein par rapport à la longueur totale des chaînes. Cela donne un score compris entre 0 et 100, où 100 indique une correspondance parfaite et 0 indique une absence de correspondance.\n",
    "\n",
    "    Adaptation par Fuzzywuzzy : Fuzzywuzzy apporte également quelques ajustements pour mieux correspondre à l'intuition humaine de la similarité. Par exemple, il pondère différemment les opérations d'insertion, de suppression et de substitution, ce qui peut donner des résultats légèrement différents de la distance de Levenshtein brute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ne teste cette méthode que sur les 5 premiers articles, étant donné que le temps de calcul est exponentiel suivant le nombre d'article à traiter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install fuzzywuzzy\n",
    "\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "# Définir la constante d'acceptation du matching entre le mot et d'un article et l'entreprise\n",
    "k = 90\n",
    "\n",
    "# Initialiser une nouvelle colonne dans df_articles pour stocker les entreprises identifiées\n",
    "df_articles['Entreprises_Identifiees_3'] = None\n",
    "\n",
    "# Boucle à travers les articles\n",
    "for index, article in df_articles.head(1).iterrows():\n",
    "    entreprises_citees = []\n",
    "    contenu_articles = article['Preprocessed_Article']  # Liste de chaînes de caractères\n",
    "    \n",
    "    # Vérifier si le contenu de l'article est une liste de chaînes de caractères\n",
    "    if isinstance(contenu_articles, list):\n",
    "        # Parcourir chaque article dans la liste\n",
    "        for contenu_article in contenu_articles:\n",
    "            # Vérifier si l'article est une chaîne de caractères\n",
    "            if isinstance(contenu_article, str):\n",
    "                # Parcourir chaque entreprise dans le DataFrame des entreprises\n",
    "                for index_ent, entreprise in df_entreprises.iterrows():\n",
    "                    nom_entreprise = entreprise['Company']\n",
    "                    if isinstance(nom_entreprise, str):\n",
    "                        # Utiliser le fuzzy matching pour trouver les correspondances\n",
    "                        score = process.extractOne(nom_entreprise, contenu_article, scorer=fuzz.ratio)\n",
    "                        \n",
    "                        # Si le score de similarité est supérieur à la constante k, ajouter l'entreprise identifiée\n",
    "                        if score[1] >= k:\n",
    "                            entreprises_citees.append(nom_entreprise)\n",
    "    \n",
    "    # Stocker les entreprises citées dans la nouvelle colonne\n",
    "    df_articles.at[index, 'Entreprises_Identifiees_3'] = ', '.join(entreprises_citees)\n",
    "\n",
    "# Afficher les articles avec les entreprises citées\n",
    "print(df_articles['Entreprises_Identifiees_3'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temps de calcul beaucoup trop long (plus de 30 min pour un seul article), et avec cela, pas de résultat concluant => aucune entreprise identifié"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "k = 90 # Constante d'acceptation du matching entre le mot et d'un article et l'entreprise \n",
    "\n",
    "# Initialiser une nouvelle colonne dans df_articles pour stocker les entreprises identifiées\n",
    "df_articles['Entreprises_Identifiees_3'] = None\n",
    "\n",
    "# Parcourir chaque article dans la base de données\n",
    "for index, row_article in df_articles.head(1).iterrows():\n",
    "    article = row_article['Preprocessed_Article']  # Contenu de l'article prétraité\n",
    "    entreprises_identifiees = []  # Liste pour stocker les entreprises identifiées pour cet article\n",
    "    \n",
    "    # Parcourir chaque entreprise dans votre DataFrame df\n",
    "    for index_ent, row_ent in df_entreprises.iterrows():\n",
    "        companies = row_ent['Company']  # Nom de l'entreprise correspondante à cet article\n",
    "        keywords = row_ent['Preprocessed_Company']  # Liste de mots-clés prétraités\n",
    "        \n",
    "        # Utiliser seulement le premier mot-clé\n",
    "        if keywords:  # Vérifiez si la liste des mots-clés n'est pas vide\n",
    "            first_keyword = keywords[0]  # Prenez le premier mot-clé\n",
    "\n",
    "            for word in article:\n",
    "                similarity_score = fuzz.ratio(first_keyword, word)\n",
    "                \n",
    "                if similarity_score > k:\n",
    "                    entreprises_identifiees.append(companies)\n",
    "        \n",
    "    \n",
    "    # Mettre à jour la colonne 'Entreprises_Identifiees_2' avec les entreprises identifiées pour cet article\n",
    "    df_articles.at[index, 'Entreprises_Identifiees_3'] = entreprises_identifiees\n",
    "    df_articles['Nombre_Entreprises_Identifiees_3'] = df_articles['Entreprises_Identifiees_3'].apply(lambda x: len(x) if x is not None else 0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temps de calcul très long... étant donné qu'on a 3 boucle for."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_entreprises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On essaie d'améliorer le temps d'éxecution du code, en supprimant au maximum les fonctions qui coûtent cher au niveau du temps d'éxecution (comme les boucles for), en utilisant des structures de stockages de données plus efficace (tels que les sets) etc..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "# Définir la constante d'acceptation du matching entre le mot et d'un article et l'entreprise\n",
    "k = 90\n",
    "\n",
    "# Nombre d'articles sur lesquels appliquer le fuzzymatching. On choisit n = 1 pour évaluer le temps de calcul pour identifier des entreprises dans un seul article.\n",
    "n=1\n",
    "\n",
    "# Initialiser une nouvelle colonne dans df_articles pour stocker les entreprises identifiées\n",
    "df_articles['Entreprises_Identifiees_3'] = None\n",
    "\n",
    "# Accéder aux trois premiers articles du DataFrame\n",
    "articles = df_articles.head(n)\n",
    "\n",
    "# Itérer sur les trois premiers articles\n",
    "for index, article in articles.iterrows():\n",
    "    entreprises_citees = set()\n",
    "    contenu_articles = article['Preprocessed_Article']  # Liste de chaînes de caractères\n",
    "    \n",
    "    # Vérifier si le contenu de l'article est une liste de chaînes de caractères\n",
    "    if isinstance(contenu_articles, list):\n",
    "        # Concaténer les chaînes de caractères des articles pour former un seul texte\n",
    "        contenu_article_concatene = ' '.join(contenu_articles)\n",
    "        \n",
    "        # Utiliser le fuzzy matching pour trouver les correspondances entre les noms d'entreprises et le contenu de l'article\n",
    "        for nom_entreprise in df_entreprises['Company']:\n",
    "            score = process.extractOne(nom_entreprise, contenu_article_concatene, scorer=fuzz.ratio)\n",
    "            \n",
    "            # Si le score de similarité est supérieur à la constante k, ajouter l'entreprise identifiée\n",
    "            if score[1] >= k:\n",
    "                entreprises_citees.add(nom_entreprise)\n",
    "    \n",
    "    # Stocker les entreprises citées dans la nouvelle colonne\n",
    "    df_articles.at[index, 'Entreprises_Identifiees_3'] = ', '.join(entreprises_citees)\n",
    "\n",
    "# Afficher les trois premiers articles avec les entreprises citées\n",
    "print(df_articles['Entreprises_Identifiees_3'].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque un temps de calcul bien trop élevé pour un seul article... On s'arrête ici pour essayer d'identifier les entreprises dans les articles, et on modifie les articles de manière à ce qu'on y introduise le nom de certaines entreprises, afin de faciliter la partie identification d'une entreprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Articles dont on a forcé l'insertion des entreprises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On travaille maintenant sur les articles auxquels on a insérer les noms d'entreprises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Entreprises_inserées_2</th>\n",
       "      <th>Articles_avec_entreprises_2</th>\n",
       "      <th>Coeur_Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Nombre de mots</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Titre</th>\n",
       "      <th>ID</th>\n",
       "      <th>Entreprise_Insérée_1</th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Coeur_Article_Inséré_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...</td>\n",
       "      <td>[Hua Hong Semiconductor Ltd, AZ-Com Maruwa Hol...</td>\n",
       "      <td>\\n\\nStony Brook University, one of two state ...</td>\n",
       "      <td>\\n\\nStony Brook University, one of two state ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nick Tabor</td>\n",
       "      <td>529</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Copyright 2023 The New York Times Company.  Al...</td>\n",
       "      <td>NYTF000020240104ejcv0000d</td>\n",
       "      <td>International Paper Co</td>\n",
       "      <td>US4601461035</td>\n",
       "      <td>\\n\\nStony Brook University, one of two state ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...</td>\n",
       "      <td>[Global Digital Niaga Tbk PT, Acconeer AB]</td>\n",
       "      <td>\\n\\nIn their one movie together, their chemis...</td>\n",
       "      <td>\\n\\nIn their one movie together, their chemis...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Wesley Morris</td>\n",
       "      <td>422</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>When Jim Brown and Raquel Welch, Two Sexy Star...</td>\n",
       "      <td>NYTF000020231231ejcv0006h</td>\n",
       "      <td>China Development Bank Financial Leasing Co Ltd</td>\n",
       "      <td>CNE1000027C9</td>\n",
       "      <td>\\n\\nIn their one movie together, their chemis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nTalking During Movi...</td>\n",
       "      <td>[Sensei Biotherapeutics Inc]</td>\n",
       "      <td>\\n\\ndebatethis\\n\\nTalking during movies: Tota...</td>\n",
       "      <td>\\n\\ndebatethis\\n\\nTalking during movies: Tota...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>179</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Talking During Movies: Totally Evil or Part of...</td>\n",
       "      <td>NYTF000020231231ejcv00064</td>\n",
       "      <td>Southern Co</td>\n",
       "      <td>US8425871071</td>\n",
       "      <td>\\n\\ndebatethis\\n\\nTalking during movies: Tota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...</td>\n",
       "      <td>[ProQR Therapeutics NV, Taihan Electric Wire C...</td>\n",
       "      <td>\\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...</td>\n",
       "      <td>\\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>454</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Let Kids Vote!</td>\n",
       "      <td>NYTF000020231231ejcv00063</td>\n",
       "      <td>Sappi Ltd</td>\n",
       "      <td>ZAE000006284</td>\n",
       "      <td>\\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...</td>\n",
       "      <td>[Baxter International Inc, Shunfeng Internatio...</td>\n",
       "      <td>\\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...</td>\n",
       "      <td>\\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Christina Caron</td>\n",
       "      <td>428</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Are We Doomed to Disagree?</td>\n",
       "      <td>NYTF000020231231ejcv0005z</td>\n",
       "      <td>Johnson Controls International PLC</td>\n",
       "      <td>IE00BY7QL619</td>\n",
       "      <td>\\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>\\n\\nGLOBAL HEALTH\\nScience Desk; SECTD\\nNew Ho...</td>\n",
       "      <td>[Telekom Austria AG]</td>\n",
       "      <td>\\n\\nResearchers in developing countries are t...</td>\n",
       "      <td>\\n\\nResearchers in developing countries are t...</td>\n",
       "      <td>19 December 2023</td>\n",
       "      <td>Stephanie Nolen</td>\n",
       "      <td>818</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>New Hope, Old Hurdle</td>\n",
       "      <td>NYTF000020231219ejcj0004s</td>\n",
       "      <td>Sanofi SA</td>\n",
       "      <td>FR0000120578</td>\n",
       "      <td>\\n\\nResearchers in developing countries are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>\\n\\nForeign Desk; SECTA\\nHotel Intended to Hou...</td>\n",
       "      <td>[Capital Securities Corp]</td>\n",
       "      <td>\\n\\nPrime Minister Leo Varadkar said there wa...</td>\n",
       "      <td>\\n\\nPrime Minister Leo Varadkar said there wa...</td>\n",
       "      <td>19 December 2023</td>\n",
       "      <td>Megan Specia</td>\n",
       "      <td>61</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Hotel Intended to House Asylum Seekers Is Burn...</td>\n",
       "      <td>NYTF000020231219ejcj00045</td>\n",
       "      <td>Conagra Brands Inc</td>\n",
       "      <td>US2058871029</td>\n",
       "      <td>\\n\\nPrime Minister Leo Varadkar said there wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>\\n\\nBusiness/Financial Desk; SECTB\\nE.U. Opens...</td>\n",
       "      <td>[Towa Pharmaceutical, Godrej Industries Ltd]</td>\n",
       "      <td>\\n\\nThe inquiry is perhaps the most substanti...</td>\n",
       "      <td>\\n\\nThe inquiry is perhaps the most substanti...</td>\n",
       "      <td>19 December 2023</td>\n",
       "      <td>Adam Satariano</td>\n",
       "      <td>902</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>E.U. Opens Investigation Of Musk's X</td>\n",
       "      <td>NYTF000020231219ejcj0003h</td>\n",
       "      <td>Southwest Airlines Co</td>\n",
       "      <td>US8447411088</td>\n",
       "      <td>\\n\\nThe inquiry is perhaps the most substanti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>\\n\\nTRILOBITES\\nScience Desk; SECTD\\nProof of ...</td>\n",
       "      <td>[Osaka Titanium Technologies Co Ltd, Huizhou D...</td>\n",
       "      <td>\\n\\nWith machine learning, scientists are try...</td>\n",
       "      <td>\\n\\nWith machine learning, scientists are try...</td>\n",
       "      <td>19 December 2023</td>\n",
       "      <td>Virginia Hughes</td>\n",
       "      <td>911</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Proof of a Wine's Terroir May Be in the Math</td>\n",
       "      <td>NYTF000020231219ejcj0002u</td>\n",
       "      <td>Mitsui &amp; Co Ltd</td>\n",
       "      <td>JP3893600001</td>\n",
       "      <td>\\n\\nWith machine learning, scientists are try...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>\\n\\nThe Impossible Energy 'Transition'\\n\\nBy M...</td>\n",
       "      <td>[Entra ASA]</td>\n",
       "      <td>\\n\\nAfter two weeks of negotiation, the Unite...</td>\n",
       "      <td>\\n\\nAfter two weeks of negotiation, the Unite...</td>\n",
       "      <td>19 December 2023</td>\n",
       "      <td>Mario Loyola</td>\n",
       "      <td>641</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>The Impossible Energy 'Transition'</td>\n",
       "      <td>J000000020231219ejcj0002y</td>\n",
       "      <td>Oracle Corp</td>\n",
       "      <td>US68389X1054</td>\n",
       "      <td>\\n\\nAfter two weeks of negotiation, the Unite...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Article  \\\n",
       "0   \\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...   \n",
       "1   \\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...   \n",
       "2   \\n\\nMagazine Desk; SECTMK\\nTalking During Movi...   \n",
       "3   \\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...   \n",
       "4   \\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...   \n",
       "..                                                ...   \n",
       "95  \\n\\nGLOBAL HEALTH\\nScience Desk; SECTD\\nNew Ho...   \n",
       "96  \\n\\nForeign Desk; SECTA\\nHotel Intended to Hou...   \n",
       "97  \\n\\nBusiness/Financial Desk; SECTB\\nE.U. Opens...   \n",
       "98  \\n\\nTRILOBITES\\nScience Desk; SECTD\\nProof of ...   \n",
       "99  \\n\\nThe Impossible Energy 'Transition'\\n\\nBy M...   \n",
       "\n",
       "                               Entreprises_inserées_2  \\\n",
       "0   [Hua Hong Semiconductor Ltd, AZ-Com Maruwa Hol...   \n",
       "1          [Global Digital Niaga Tbk PT, Acconeer AB]   \n",
       "2                        [Sensei Biotherapeutics Inc]   \n",
       "3   [ProQR Therapeutics NV, Taihan Electric Wire C...   \n",
       "4   [Baxter International Inc, Shunfeng Internatio...   \n",
       "..                                                ...   \n",
       "95                               [Telekom Austria AG]   \n",
       "96                          [Capital Securities Corp]   \n",
       "97       [Towa Pharmaceutical, Godrej Industries Ltd]   \n",
       "98  [Osaka Titanium Technologies Co Ltd, Huizhou D...   \n",
       "99                                        [Entra ASA]   \n",
       "\n",
       "                          Articles_avec_entreprises_2  \\\n",
       "0    \\n\\nStony Brook University, one of two state ...   \n",
       "1    \\n\\nIn their one movie together, their chemis...   \n",
       "2    \\n\\ndebatethis\\n\\nTalking during movies: Tota...   \n",
       "3    \\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...   \n",
       "4    \\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...   \n",
       "..                                                ...   \n",
       "95   \\n\\nResearchers in developing countries are t...   \n",
       "96   \\n\\nPrime Minister Leo Varadkar said there wa...   \n",
       "97   \\n\\nThe inquiry is perhaps the most substanti...   \n",
       "98   \\n\\nWith machine learning, scientists are try...   \n",
       "99   \\n\\nAfter two weeks of negotiation, the Unite...   \n",
       "\n",
       "                                        Coeur_Article              Date  \\\n",
       "0    \\n\\nStony Brook University, one of two state ...  31 December 2023   \n",
       "1    \\n\\nIn their one movie together, their chemis...  31 December 2023   \n",
       "2    \\n\\ndebatethis\\n\\nTalking during movies: Tota...  31 December 2023   \n",
       "3    \\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...  31 December 2023   \n",
       "4    \\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...  31 December 2023   \n",
       "..                                                ...               ...   \n",
       "95   \\n\\nResearchers in developing countries are t...  19 December 2023   \n",
       "96   \\n\\nPrime Minister Leo Varadkar said there wa...  19 December 2023   \n",
       "97   \\n\\nThe inquiry is perhaps the most substanti...  19 December 2023   \n",
       "98   \\n\\nWith machine learning, scientists are try...  19 December 2023   \n",
       "99   \\n\\nAfter two weeks of negotiation, the Unite...  19 December 2023   \n",
       "\n",
       "             Auteur  Nombre de mots              Journal  \\\n",
       "0        Nick Tabor             529       New York Times   \n",
       "1     Wesley Morris             422       New York Times   \n",
       "2              None             179       New York Times   \n",
       "3              None             454       New York Times   \n",
       "4   Christina Caron             428       New York Times   \n",
       "..              ...             ...                  ...   \n",
       "95  Stephanie Nolen             818       New York Times   \n",
       "96     Megan Specia              61       New York Times   \n",
       "97   Adam Satariano             902       New York Times   \n",
       "98  Virginia Hughes             911       New York Times   \n",
       "99     Mario Loyola             641  Wall Street Journal   \n",
       "\n",
       "                                                Titre  \\\n",
       "0   Copyright 2023 The New York Times Company.  Al...   \n",
       "1   When Jim Brown and Raquel Welch, Two Sexy Star...   \n",
       "2   Talking During Movies: Totally Evil or Part of...   \n",
       "3                                      Let Kids Vote!   \n",
       "4                          Are We Doomed to Disagree?   \n",
       "..                                                ...   \n",
       "95                               New Hope, Old Hurdle   \n",
       "96  Hotel Intended to House Asylum Seekers Is Burn...   \n",
       "97               E.U. Opens Investigation Of Musk's X   \n",
       "98       Proof of a Wine's Terroir May Be in the Math   \n",
       "99                 The Impossible Energy 'Transition'   \n",
       "\n",
       "                             ID  \\\n",
       "0     NYTF000020240104ejcv0000d   \n",
       "1     NYTF000020231231ejcv0006h   \n",
       "2    NYTF000020231231ejcv00064    \n",
       "3    NYTF000020231231ejcv00063    \n",
       "4    NYTF000020231231ejcv0005z    \n",
       "..                          ...   \n",
       "95   NYTF000020231219ejcj0004s    \n",
       "96   NYTF000020231219ejcj00045    \n",
       "97   NYTF000020231219ejcj0003h    \n",
       "98   NYTF000020231219ejcj0002u    \n",
       "99    J000000020231219ejcj0002y   \n",
       "\n",
       "                               Entreprise_Insérée_1          ISIN  \\\n",
       "0                            International Paper Co  US4601461035   \n",
       "1   China Development Bank Financial Leasing Co Ltd  CNE1000027C9   \n",
       "2                                       Southern Co  US8425871071   \n",
       "3                                         Sappi Ltd  ZAE000006284   \n",
       "4                Johnson Controls International PLC  IE00BY7QL619   \n",
       "..                                              ...           ...   \n",
       "95                                        Sanofi SA  FR0000120578   \n",
       "96                               Conagra Brands Inc  US2058871029   \n",
       "97                            Southwest Airlines Co  US8447411088   \n",
       "98                                  Mitsui & Co Ltd  JP3893600001   \n",
       "99                                      Oracle Corp  US68389X1054   \n",
       "\n",
       "                               Coeur_Article_Inséré_1  \n",
       "0    \\n\\nStony Brook University, one of two state ...  \n",
       "1    \\n\\nIn their one movie together, their chemis...  \n",
       "2    \\n\\ndebatethis\\n\\nTalking during movies: Tota...  \n",
       "3    \\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...  \n",
       "4    \\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...  \n",
       "..                                                ...  \n",
       "95   \\n\\nResearchers in developing countries are t...  \n",
       "96   \\n\\nPrime Minister Leo Varadkar said there wa...  \n",
       "97   \\n\\nThe inquiry is perhaps the most substanti...  \n",
       "98   \\n\\nWith machine learning, scientists are try...  \n",
       "99   \\n\\nAfter two weeks of negotiation, the Unite...  \n",
       "\n",
       "[100 rows x 13 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_avec_entreprises = pd.read_pickle('data_avec_entreprises_2.pkl')\n",
    "data_avec_entreprises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Entreprises_inserées_2</th>\n",
       "      <th>Articles_avec_entreprises_2</th>\n",
       "      <th>Coeur_Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Nombre de mots</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Titre</th>\n",
       "      <th>ID</th>\n",
       "      <th>Entreprise_Insérée_1</th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Coeur_Article_Inséré_1</th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...</td>\n",
       "      <td>[Hua Hong Semiconductor Ltd, AZ-Com Maruwa Hol...</td>\n",
       "      <td>\\n\\nStony Brook University, one of two state ...</td>\n",
       "      <td>\\n\\nStony Brook University, one of two state ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nick Tabor</td>\n",
       "      <td>529</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Copyright 2023 The New York Times Company.  Al...</td>\n",
       "      <td>NYTF000020240104ejcv0000d</td>\n",
       "      <td>International Paper Co</td>\n",
       "      <td>US4601461035</td>\n",
       "      <td>\\n\\nStony Brook University, one of two state ...</td>\n",
       "      <td>[stony, brook, university, one, two, state, sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...</td>\n",
       "      <td>[Global Digital Niaga Tbk PT, Acconeer AB]</td>\n",
       "      <td>\\n\\nIn their one movie together, their chemis...</td>\n",
       "      <td>\\n\\nIn their one movie together, their chemis...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Wesley Morris</td>\n",
       "      <td>422</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>When Jim Brown and Raquel Welch, Two Sexy Star...</td>\n",
       "      <td>NYTF000020231231ejcv0006h</td>\n",
       "      <td>China Development Bank Financial Leasing Co Ltd</td>\n",
       "      <td>CNE1000027C9</td>\n",
       "      <td>\\n\\nIn their one movie together, their chemis...</td>\n",
       "      <td>[one, movie, together, chemistry, radical, jim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nTalking During Movi...</td>\n",
       "      <td>[Sensei Biotherapeutics Inc]</td>\n",
       "      <td>\\n\\ndebatethis\\n\\nTalking during movies: Tota...</td>\n",
       "      <td>\\n\\ndebatethis\\n\\nTalking during movies: Tota...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>179</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Talking During Movies: Totally Evil or Part of...</td>\n",
       "      <td>NYTF000020231231ejcv00064</td>\n",
       "      <td>Southern Co</td>\n",
       "      <td>US8425871071</td>\n",
       "      <td>\\n\\ndebatethis\\n\\nTalking during movies: Tota...</td>\n",
       "      <td>[debatethis, talking, movie, totally, evil, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...</td>\n",
       "      <td>[ProQR Therapeutics NV, Taihan Electric Wire C...</td>\n",
       "      <td>\\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...</td>\n",
       "      <td>\\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>454</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Let Kids Vote!</td>\n",
       "      <td>NYTF000020231231ejcv00063</td>\n",
       "      <td>Sappi Ltd</td>\n",
       "      <td>ZAE000006284</td>\n",
       "      <td>\\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...</td>\n",
       "      <td>[let, kid, vote, katherine, cusumano, julia, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...</td>\n",
       "      <td>[Baxter International Inc, Shunfeng Internatio...</td>\n",
       "      <td>\\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...</td>\n",
       "      <td>\\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Christina Caron</td>\n",
       "      <td>428</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Are We Doomed to Disagree?</td>\n",
       "      <td>NYTF000020231231ejcv0005z</td>\n",
       "      <td>Johnson Controls International PLC</td>\n",
       "      <td>IE00BY7QL619</td>\n",
       "      <td>\\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...</td>\n",
       "      <td>[doomed, disagree, hard, change, people, mind,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  \\\n",
       "0  \\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...   \n",
       "1  \\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...   \n",
       "2  \\n\\nMagazine Desk; SECTMK\\nTalking During Movi...   \n",
       "3  \\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...   \n",
       "4  \\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...   \n",
       "\n",
       "                              Entreprises_inserées_2  \\\n",
       "0  [Hua Hong Semiconductor Ltd, AZ-Com Maruwa Hol...   \n",
       "1         [Global Digital Niaga Tbk PT, Acconeer AB]   \n",
       "2                       [Sensei Biotherapeutics Inc]   \n",
       "3  [ProQR Therapeutics NV, Taihan Electric Wire C...   \n",
       "4  [Baxter International Inc, Shunfeng Internatio...   \n",
       "\n",
       "                         Articles_avec_entreprises_2  \\\n",
       "0   \\n\\nStony Brook University, one of two state ...   \n",
       "1   \\n\\nIn their one movie together, their chemis...   \n",
       "2   \\n\\ndebatethis\\n\\nTalking during movies: Tota...   \n",
       "3   \\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...   \n",
       "4   \\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...   \n",
       "\n",
       "                                       Coeur_Article              Date  \\\n",
       "0   \\n\\nStony Brook University, one of two state ...  31 December 2023   \n",
       "1   \\n\\nIn their one movie together, their chemis...  31 December 2023   \n",
       "2   \\n\\ndebatethis\\n\\nTalking during movies: Tota...  31 December 2023   \n",
       "3   \\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...  31 December 2023   \n",
       "4   \\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...  31 December 2023   \n",
       "\n",
       "            Auteur  Nombre de mots         Journal  \\\n",
       "0       Nick Tabor             529  New York Times   \n",
       "1    Wesley Morris             422  New York Times   \n",
       "2             None             179  New York Times   \n",
       "3             None             454  New York Times   \n",
       "4  Christina Caron             428  New York Times   \n",
       "\n",
       "                                               Titre  \\\n",
       "0  Copyright 2023 The New York Times Company.  Al...   \n",
       "1  When Jim Brown and Raquel Welch, Two Sexy Star...   \n",
       "2  Talking During Movies: Totally Evil or Part of...   \n",
       "3                                     Let Kids Vote!   \n",
       "4                         Are We Doomed to Disagree?   \n",
       "\n",
       "                            ID  \\\n",
       "0    NYTF000020240104ejcv0000d   \n",
       "1    NYTF000020231231ejcv0006h   \n",
       "2   NYTF000020231231ejcv00064    \n",
       "3   NYTF000020231231ejcv00063    \n",
       "4   NYTF000020231231ejcv0005z    \n",
       "\n",
       "                              Entreprise_Insérée_1          ISIN  \\\n",
       "0                           International Paper Co  US4601461035   \n",
       "1  China Development Bank Financial Leasing Co Ltd  CNE1000027C9   \n",
       "2                                      Southern Co  US8425871071   \n",
       "3                                        Sappi Ltd  ZAE000006284   \n",
       "4               Johnson Controls International PLC  IE00BY7QL619   \n",
       "\n",
       "                              Coeur_Article_Inséré_1  \\\n",
       "0   \\n\\nStony Brook University, one of two state ...   \n",
       "1   \\n\\nIn their one movie together, their chemis...   \n",
       "2   \\n\\ndebatethis\\n\\nTalking during movies: Tota...   \n",
       "3   \\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...   \n",
       "4   \\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...   \n",
       "\n",
       "                                Preprocessed_Article  \n",
       "0  [stony, brook, university, one, two, state, sc...  \n",
       "1  [one, movie, together, chemistry, radical, jim...  \n",
       "2  [debatethis, talking, movie, totally, evil, pa...  \n",
       "3  [let, kid, vote, katherine, cusumano, julia, r...  \n",
       "4  [doomed, disagree, hard, change, people, mind,...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_avec_entreprises['Preprocessed_Article'] = data_avec_entreprises['Coeur_Article_Inséré_1'].apply(preprocess_text)\n",
    "data_avec_entreprises.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_avec_entreprises['environmental_sentiment_score'] = data_avec_entreprises['Preprocessed_Article'].apply(lambda x: get_environmental_score(x, Dico_env_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Entreprises_inserées_2</th>\n",
       "      <th>Articles_avec_entreprises_2</th>\n",
       "      <th>Coeur_Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Nombre de mots</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Titre</th>\n",
       "      <th>ID</th>\n",
       "      <th>Entreprise_Insérée_1</th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Coeur_Article_Inséré_1</th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "      <th>environmental_sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...</td>\n",
       "      <td>[Hua Hong Semiconductor Ltd, AZ-Com Maruwa Hol...</td>\n",
       "      <td>\\n\\nStony Brook University, one of two state ...</td>\n",
       "      <td>\\n\\nStony Brook University, one of two state ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nick Tabor</td>\n",
       "      <td>529</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Copyright 2023 The New York Times Company.  Al...</td>\n",
       "      <td>NYTF000020240104ejcv0000d</td>\n",
       "      <td>International Paper Co</td>\n",
       "      <td>US4601461035</td>\n",
       "      <td>\\n\\nStony Brook University, one of two state ...</td>\n",
       "      <td>[stony, brook, university, one, two, state, sc...</td>\n",
       "      <td>-0.002011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...</td>\n",
       "      <td>[Global Digital Niaga Tbk PT, Acconeer AB]</td>\n",
       "      <td>\\n\\nIn their one movie together, their chemis...</td>\n",
       "      <td>\\n\\nIn their one movie together, their chemis...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Wesley Morris</td>\n",
       "      <td>422</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>When Jim Brown and Raquel Welch, Two Sexy Star...</td>\n",
       "      <td>NYTF000020231231ejcv0006h</td>\n",
       "      <td>China Development Bank Financial Leasing Co Ltd</td>\n",
       "      <td>CNE1000027C9</td>\n",
       "      <td>\\n\\nIn their one movie together, their chemis...</td>\n",
       "      <td>[one, movie, together, chemistry, radical, jim...</td>\n",
       "      <td>-0.005395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nTalking During Movi...</td>\n",
       "      <td>[Sensei Biotherapeutics Inc]</td>\n",
       "      <td>\\n\\ndebatethis\\n\\nTalking during movies: Tota...</td>\n",
       "      <td>\\n\\ndebatethis\\n\\nTalking during movies: Tota...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>179</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Talking During Movies: Totally Evil or Part of...</td>\n",
       "      <td>NYTF000020231231ejcv00064</td>\n",
       "      <td>Southern Co</td>\n",
       "      <td>US8425871071</td>\n",
       "      <td>\\n\\ndebatethis\\n\\nTalking during movies: Tota...</td>\n",
       "      <td>[debatethis, talking, movie, totally, evil, pa...</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...</td>\n",
       "      <td>[ProQR Therapeutics NV, Taihan Electric Wire C...</td>\n",
       "      <td>\\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...</td>\n",
       "      <td>\\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>454</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Let Kids Vote!</td>\n",
       "      <td>NYTF000020231231ejcv00063</td>\n",
       "      <td>Sappi Ltd</td>\n",
       "      <td>ZAE000006284</td>\n",
       "      <td>\\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...</td>\n",
       "      <td>[let, kid, vote, katherine, cusumano, julia, r...</td>\n",
       "      <td>0.024060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...</td>\n",
       "      <td>[Baxter International Inc, Shunfeng Internatio...</td>\n",
       "      <td>\\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...</td>\n",
       "      <td>\\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Christina Caron</td>\n",
       "      <td>428</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Are We Doomed to Disagree?</td>\n",
       "      <td>NYTF000020231231ejcv0005z</td>\n",
       "      <td>Johnson Controls International PLC</td>\n",
       "      <td>IE00BY7QL619</td>\n",
       "      <td>\\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...</td>\n",
       "      <td>[doomed, disagree, hard, change, people, mind,...</td>\n",
       "      <td>0.003908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  \\\n",
       "0  \\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...   \n",
       "1  \\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...   \n",
       "2  \\n\\nMagazine Desk; SECTMK\\nTalking During Movi...   \n",
       "3  \\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...   \n",
       "4  \\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...   \n",
       "\n",
       "                              Entreprises_inserées_2  \\\n",
       "0  [Hua Hong Semiconductor Ltd, AZ-Com Maruwa Hol...   \n",
       "1         [Global Digital Niaga Tbk PT, Acconeer AB]   \n",
       "2                       [Sensei Biotherapeutics Inc]   \n",
       "3  [ProQR Therapeutics NV, Taihan Electric Wire C...   \n",
       "4  [Baxter International Inc, Shunfeng Internatio...   \n",
       "\n",
       "                         Articles_avec_entreprises_2  \\\n",
       "0   \\n\\nStony Brook University, one of two state ...   \n",
       "1   \\n\\nIn their one movie together, their chemis...   \n",
       "2   \\n\\ndebatethis\\n\\nTalking during movies: Tota...   \n",
       "3   \\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...   \n",
       "4   \\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...   \n",
       "\n",
       "                                       Coeur_Article              Date  \\\n",
       "0   \\n\\nStony Brook University, one of two state ...  31 December 2023   \n",
       "1   \\n\\nIn their one movie together, their chemis...  31 December 2023   \n",
       "2   \\n\\ndebatethis\\n\\nTalking during movies: Tota...  31 December 2023   \n",
       "3   \\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...  31 December 2023   \n",
       "4   \\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...  31 December 2023   \n",
       "\n",
       "            Auteur  Nombre de mots         Journal  \\\n",
       "0       Nick Tabor             529  New York Times   \n",
       "1    Wesley Morris             422  New York Times   \n",
       "2             None             179  New York Times   \n",
       "3             None             454  New York Times   \n",
       "4  Christina Caron             428  New York Times   \n",
       "\n",
       "                                               Titre  \\\n",
       "0  Copyright 2023 The New York Times Company.  Al...   \n",
       "1  When Jim Brown and Raquel Welch, Two Sexy Star...   \n",
       "2  Talking During Movies: Totally Evil or Part of...   \n",
       "3                                     Let Kids Vote!   \n",
       "4                         Are We Doomed to Disagree?   \n",
       "\n",
       "                            ID  \\\n",
       "0    NYTF000020240104ejcv0000d   \n",
       "1    NYTF000020231231ejcv0006h   \n",
       "2   NYTF000020231231ejcv00064    \n",
       "3   NYTF000020231231ejcv00063    \n",
       "4   NYTF000020231231ejcv0005z    \n",
       "\n",
       "                              Entreprise_Insérée_1          ISIN  \\\n",
       "0                           International Paper Co  US4601461035   \n",
       "1  China Development Bank Financial Leasing Co Ltd  CNE1000027C9   \n",
       "2                                      Southern Co  US8425871071   \n",
       "3                                        Sappi Ltd  ZAE000006284   \n",
       "4               Johnson Controls International PLC  IE00BY7QL619   \n",
       "\n",
       "                              Coeur_Article_Inséré_1  \\\n",
       "0   \\n\\nStony Brook University, one of two state ...   \n",
       "1   \\n\\nIn their one movie together, their chemis...   \n",
       "2   \\n\\ndebatethis\\n\\nTalking during movies: Tota...   \n",
       "3   \\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...   \n",
       "4   \\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...   \n",
       "\n",
       "                                Preprocessed_Article  \\\n",
       "0  [stony, brook, university, one, two, state, sc...   \n",
       "1  [one, movie, together, chemistry, radical, jim...   \n",
       "2  [debatethis, talking, movie, totally, evil, pa...   \n",
       "3  [let, kid, vote, katherine, cusumano, julia, r...   \n",
       "4  [doomed, disagree, hard, change, people, mind,...   \n",
       "\n",
       "   environmental_sentiment_score  \n",
       "0                      -0.002011  \n",
       "1                      -0.005395  \n",
       "2                       0.010000  \n",
       "3                       0.024060  \n",
       "4                       0.003908  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_avec_entreprises.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nombre d'article dont le score environnemental est différent de 0\n",
    "(data_avec_entreprises['environmental_sentiment_score'] != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_avec_entreprises[['Entreprise_Insérée_1','ISIN','environmental_sentiment_score']].to_csv('env_comm_score_articles_forces.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Liste des noms d'entreprises\n",
    "noms_entreprises = pd.read_csv('Firms.csv')['Company'].tolist()\n",
    "\n",
    "# Fonction pour identifier les entreprises dans un article\n",
    "def identifier_entreprises(article):\n",
    "    entreprises_identifiees = []\n",
    "    if isinstance(article, str):\n",
    "        for entreprise in noms_entreprises:\n",
    "            if isinstance(entreprise, str) and entreprise in article:\n",
    "                entreprises_identifiees.append(entreprise)\n",
    "    return entreprises_identifiees\n",
    "\n",
    "# Appliquer la fonction à chaque article\n",
    "data_avec_entreprises['Entreprises_identifiees'] = data_avec_entreprises['Coeur_Article_Inséré_1'].apply(identifier_entreprises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Entreprises_inserées_2</th>\n",
       "      <th>Articles_avec_entreprises_2</th>\n",
       "      <th>Coeur_Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Nombre de mots</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Titre</th>\n",
       "      <th>ID</th>\n",
       "      <th>Entreprise_Insérée_1</th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Coeur_Article_Inséré_1</th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "      <th>environmental_sentiment_score</th>\n",
       "      <th>Entreprises_identifiees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...</td>\n",
       "      <td>[Hua Hong Semiconductor Ltd, AZ-Com Maruwa Hol...</td>\n",
       "      <td>\\n\\nStony Brook University, one of two state ...</td>\n",
       "      <td>\\n\\nStony Brook University, one of two state ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nick Tabor</td>\n",
       "      <td>529</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Copyright 2023 The New York Times Company.  Al...</td>\n",
       "      <td>NYTF000020240104ejcv0000d</td>\n",
       "      <td>International Paper Co</td>\n",
       "      <td>US4601461035</td>\n",
       "      <td>\\n\\nStony Brook University, one of two state ...</td>\n",
       "      <td>[stony, brook, university, one, two, state, sc...</td>\n",
       "      <td>-0.002011</td>\n",
       "      <td>[International Paper Co]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...</td>\n",
       "      <td>[Global Digital Niaga Tbk PT, Acconeer AB]</td>\n",
       "      <td>\\n\\nIn their one movie together, their chemis...</td>\n",
       "      <td>\\n\\nIn their one movie together, their chemis...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Wesley Morris</td>\n",
       "      <td>422</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>When Jim Brown and Raquel Welch, Two Sexy Star...</td>\n",
       "      <td>NYTF000020231231ejcv0006h</td>\n",
       "      <td>China Development Bank Financial Leasing Co Ltd</td>\n",
       "      <td>CNE1000027C9</td>\n",
       "      <td>\\n\\nIn their one movie together, their chemis...</td>\n",
       "      <td>[one, movie, together, chemistry, radical, jim...</td>\n",
       "      <td>-0.005395</td>\n",
       "      <td>[China Development Bank Financial Leasing Co Ltd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nTalking During Movi...</td>\n",
       "      <td>[Sensei Biotherapeutics Inc]</td>\n",
       "      <td>\\n\\ndebatethis\\n\\nTalking during movies: Tota...</td>\n",
       "      <td>\\n\\ndebatethis\\n\\nTalking during movies: Tota...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>179</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Talking During Movies: Totally Evil or Part of...</td>\n",
       "      <td>NYTF000020231231ejcv00064</td>\n",
       "      <td>Southern Co</td>\n",
       "      <td>US8425871071</td>\n",
       "      <td>\\n\\ndebatethis\\n\\nTalking during movies: Tota...</td>\n",
       "      <td>[debatethis, talking, movie, totally, evil, pa...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>[Southern Co]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...</td>\n",
       "      <td>[ProQR Therapeutics NV, Taihan Electric Wire C...</td>\n",
       "      <td>\\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...</td>\n",
       "      <td>\\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>454</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Let Kids Vote!</td>\n",
       "      <td>NYTF000020231231ejcv00063</td>\n",
       "      <td>Sappi Ltd</td>\n",
       "      <td>ZAE000006284</td>\n",
       "      <td>\\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...</td>\n",
       "      <td>[let, kid, vote, katherine, cusumano, julia, r...</td>\n",
       "      <td>0.024060</td>\n",
       "      <td>[Sappi Ltd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...</td>\n",
       "      <td>[Baxter International Inc, Shunfeng Internatio...</td>\n",
       "      <td>\\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...</td>\n",
       "      <td>\\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Christina Caron</td>\n",
       "      <td>428</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Are We Doomed to Disagree?</td>\n",
       "      <td>NYTF000020231231ejcv0005z</td>\n",
       "      <td>Johnson Controls International PLC</td>\n",
       "      <td>IE00BY7QL619</td>\n",
       "      <td>\\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...</td>\n",
       "      <td>[doomed, disagree, hard, change, people, mind,...</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>[Johnson Controls International PLC]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  \\\n",
       "0  \\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...   \n",
       "1  \\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...   \n",
       "2  \\n\\nMagazine Desk; SECTMK\\nTalking During Movi...   \n",
       "3  \\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...   \n",
       "4  \\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...   \n",
       "\n",
       "                              Entreprises_inserées_2  \\\n",
       "0  [Hua Hong Semiconductor Ltd, AZ-Com Maruwa Hol...   \n",
       "1         [Global Digital Niaga Tbk PT, Acconeer AB]   \n",
       "2                       [Sensei Biotherapeutics Inc]   \n",
       "3  [ProQR Therapeutics NV, Taihan Electric Wire C...   \n",
       "4  [Baxter International Inc, Shunfeng Internatio...   \n",
       "\n",
       "                         Articles_avec_entreprises_2  \\\n",
       "0   \\n\\nStony Brook University, one of two state ...   \n",
       "1   \\n\\nIn their one movie together, their chemis...   \n",
       "2   \\n\\ndebatethis\\n\\nTalking during movies: Tota...   \n",
       "3   \\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...   \n",
       "4   \\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...   \n",
       "\n",
       "                                       Coeur_Article              Date  \\\n",
       "0   \\n\\nStony Brook University, one of two state ...  31 December 2023   \n",
       "1   \\n\\nIn their one movie together, their chemis...  31 December 2023   \n",
       "2   \\n\\ndebatethis\\n\\nTalking during movies: Tota...  31 December 2023   \n",
       "3   \\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...  31 December 2023   \n",
       "4   \\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...  31 December 2023   \n",
       "\n",
       "            Auteur  Nombre de mots         Journal  \\\n",
       "0       Nick Tabor             529  New York Times   \n",
       "1    Wesley Morris             422  New York Times   \n",
       "2             None             179  New York Times   \n",
       "3             None             454  New York Times   \n",
       "4  Christina Caron             428  New York Times   \n",
       "\n",
       "                                               Titre  \\\n",
       "0  Copyright 2023 The New York Times Company.  Al...   \n",
       "1  When Jim Brown and Raquel Welch, Two Sexy Star...   \n",
       "2  Talking During Movies: Totally Evil or Part of...   \n",
       "3                                     Let Kids Vote!   \n",
       "4                         Are We Doomed to Disagree?   \n",
       "\n",
       "                            ID  \\\n",
       "0    NYTF000020240104ejcv0000d   \n",
       "1    NYTF000020231231ejcv0006h   \n",
       "2   NYTF000020231231ejcv00064    \n",
       "3   NYTF000020231231ejcv00063    \n",
       "4   NYTF000020231231ejcv0005z    \n",
       "\n",
       "                              Entreprise_Insérée_1          ISIN  \\\n",
       "0                           International Paper Co  US4601461035   \n",
       "1  China Development Bank Financial Leasing Co Ltd  CNE1000027C9   \n",
       "2                                      Southern Co  US8425871071   \n",
       "3                                        Sappi Ltd  ZAE000006284   \n",
       "4               Johnson Controls International PLC  IE00BY7QL619   \n",
       "\n",
       "                              Coeur_Article_Inséré_1  \\\n",
       "0   \\n\\nStony Brook University, one of two state ...   \n",
       "1   \\n\\nIn their one movie together, their chemis...   \n",
       "2   \\n\\ndebatethis\\n\\nTalking during movies: Tota...   \n",
       "3   \\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...   \n",
       "4   \\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...   \n",
       "\n",
       "                                Preprocessed_Article  \\\n",
       "0  [stony, brook, university, one, two, state, sc...   \n",
       "1  [one, movie, together, chemistry, radical, jim...   \n",
       "2  [debatethis, talking, movie, totally, evil, pa...   \n",
       "3  [let, kid, vote, katherine, cusumano, julia, r...   \n",
       "4  [doomed, disagree, hard, change, people, mind,...   \n",
       "\n",
       "   environmental_sentiment_score  \\\n",
       "0                      -0.002011   \n",
       "1                      -0.005395   \n",
       "2                       0.010000   \n",
       "3                       0.024060   \n",
       "4                       0.003908   \n",
       "\n",
       "                             Entreprises_identifiees  \n",
       "0                           [International Paper Co]  \n",
       "1  [China Development Bank Financial Leasing Co Ltd]  \n",
       "2                                      [Southern Co]  \n",
       "3                                        [Sappi Ltd]  \n",
       "4               [Johnson Controls International PLC]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les articles avec les entreprises identifiées\n",
    "data_avec_entreprises.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'on a identifié les entreprises dans un article, il faut lui attribuer une note de score environnemental. On peut soit décider qu'on attribue la note globale de l'article, à chaque entreprises identifiée (on pourrait faire une moyenne des notes, si jamais une entreprises est identifiées dans plusieurs articles), ou bien, on peut essayer de construire une pondération de la note, en fonction de la distance dans le texte entre l'entreprise, et le mot identifié par le dictionnaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On identifie une entreprise, et on associe à cette identification, sa place dans le texte (dans le cas où on veut pondérer la note environnementale, en fonction de la distance dans le texte entre l'entreprise, et le mot identifié par le dictionnaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Liste des noms d'entreprises\n",
    "noms_entreprises = pd.read_csv('Firms.csv')['Company'].tolist()\n",
    "\n",
    "# Fonction pour identifier les entreprises dans un article avec leur position\n",
    "def identifier_entreprises(article):\n",
    "    entreprises_identifiees = []\n",
    "    if isinstance(article, str):\n",
    "        for entreprise in noms_entreprises:\n",
    "            if isinstance(entreprise, str) and entreprise in article:\n",
    "                positions = [i for i in range(len(article)) if article.startswith(entreprise, i)]\n",
    "                for position in positions:\n",
    "                    entreprises_identifiees.append((entreprise, position))\n",
    "    return entreprises_identifiees\n",
    "\n",
    "# Appliquer la fonction à chaque article\n",
    "data_avec_entreprises['Entreprises_identifiees'] = data_avec_entreprises['Coeur_Article_Inséré_1'].apply(identifier_entreprises)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Entreprises_inserées_2</th>\n",
       "      <th>Articles_avec_entreprises_2</th>\n",
       "      <th>Coeur_Article</th>\n",
       "      <th>Date</th>\n",
       "      <th>Auteur</th>\n",
       "      <th>Nombre de mots</th>\n",
       "      <th>Journal</th>\n",
       "      <th>Titre</th>\n",
       "      <th>ID</th>\n",
       "      <th>Entreprise_Insérée_1</th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Coeur_Article_Inséré_1</th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "      <th>environmental_sentiment_score</th>\n",
       "      <th>Entreprises_identifiees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...</td>\n",
       "      <td>[Hua Hong Semiconductor Ltd, AZ-Com Maruwa Hol...</td>\n",
       "      <td>\\n\\nStony Brook University, one of two state ...</td>\n",
       "      <td>\\n\\nStony Brook University, one of two state ...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Nick Tabor</td>\n",
       "      <td>529</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Copyright 2023 The New York Times Company.  Al...</td>\n",
       "      <td>NYTF000020240104ejcv0000d</td>\n",
       "      <td>International Paper Co</td>\n",
       "      <td>US4601461035</td>\n",
       "      <td>\\n\\nStony Brook University, one of two state ...</td>\n",
       "      <td>[stony, brook, university, one, two, state, sc...</td>\n",
       "      <td>-0.002011</td>\n",
       "      <td>[(International Paper Co, 3742), (Internationa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...</td>\n",
       "      <td>[Global Digital Niaga Tbk PT, Acconeer AB]</td>\n",
       "      <td>\\n\\nIn their one movie together, their chemis...</td>\n",
       "      <td>\\n\\nIn their one movie together, their chemis...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Wesley Morris</td>\n",
       "      <td>422</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>When Jim Brown and Raquel Welch, Two Sexy Star...</td>\n",
       "      <td>NYTF000020231231ejcv0006h</td>\n",
       "      <td>China Development Bank Financial Leasing Co Ltd</td>\n",
       "      <td>CNE1000027C9</td>\n",
       "      <td>\\n\\nIn their one movie together, their chemis...</td>\n",
       "      <td>[one, movie, together, chemistry, radical, jim...</td>\n",
       "      <td>-0.005395</td>\n",
       "      <td>[(China Development Bank Financial Leasing Co ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nTalking During Movi...</td>\n",
       "      <td>[Sensei Biotherapeutics Inc]</td>\n",
       "      <td>\\n\\ndebatethis\\n\\nTalking during movies: Tota...</td>\n",
       "      <td>\\n\\ndebatethis\\n\\nTalking during movies: Tota...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>179</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Talking During Movies: Totally Evil or Part of...</td>\n",
       "      <td>NYTF000020231231ejcv00064</td>\n",
       "      <td>Southern Co</td>\n",
       "      <td>US8425871071</td>\n",
       "      <td>\\n\\ndebatethis\\n\\nTalking during movies: Tota...</td>\n",
       "      <td>[debatethis, talking, movie, totally, evil, pa...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>[(Southern Co, 538), (Southern Co, 622), (Sout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...</td>\n",
       "      <td>[ProQR Therapeutics NV, Taihan Electric Wire C...</td>\n",
       "      <td>\\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...</td>\n",
       "      <td>\\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>None</td>\n",
       "      <td>454</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Let Kids Vote!</td>\n",
       "      <td>NYTF000020231231ejcv00063</td>\n",
       "      <td>Sappi Ltd</td>\n",
       "      <td>ZAE000006284</td>\n",
       "      <td>\\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...</td>\n",
       "      <td>[let, kid, vote, katherine, cusumano, julia, r...</td>\n",
       "      <td>0.024060</td>\n",
       "      <td>[(Sappi Ltd, 766), (Sappi Ltd, 898), (Sappi Lt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...</td>\n",
       "      <td>[Baxter International Inc, Shunfeng Internatio...</td>\n",
       "      <td>\\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...</td>\n",
       "      <td>\\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...</td>\n",
       "      <td>31 December 2023</td>\n",
       "      <td>Christina Caron</td>\n",
       "      <td>428</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Are We Doomed to Disagree?</td>\n",
       "      <td>NYTF000020231231ejcv0005z</td>\n",
       "      <td>Johnson Controls International PLC</td>\n",
       "      <td>IE00BY7QL619</td>\n",
       "      <td>\\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...</td>\n",
       "      <td>[doomed, disagree, hard, change, people, mind,...</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>[(Johnson Controls International PLC, 127), (J...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  \\\n",
       "0  \\nMetropolitan Desk; SECTMB\\nCan an Ambitious ...   \n",
       "1  \\n\\nMagazine Desk; SECTMM\\nWhen Jim Brown and ...   \n",
       "2  \\n\\nMagazine Desk; SECTMK\\nTalking During Movi...   \n",
       "3  \\n\\nMagazine Desk; SECTMK\\nLet Kids Vote!\\n\\n4...   \n",
       "4  \\n\\nMagazine Desk; SECTMK\\nAre We Doomed to Di...   \n",
       "\n",
       "                              Entreprises_inserées_2  \\\n",
       "0  [Hua Hong Semiconductor Ltd, AZ-Com Maruwa Hol...   \n",
       "1         [Global Digital Niaga Tbk PT, Acconeer AB]   \n",
       "2                       [Sensei Biotherapeutics Inc]   \n",
       "3  [ProQR Therapeutics NV, Taihan Electric Wire C...   \n",
       "4  [Baxter International Inc, Shunfeng Internatio...   \n",
       "\n",
       "                         Articles_avec_entreprises_2  \\\n",
       "0   \\n\\nStony Brook University, one of two state ...   \n",
       "1   \\n\\nIn their one movie together, their chemis...   \n",
       "2   \\n\\ndebatethis\\n\\nTalking during movies: Tota...   \n",
       "3   \\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...   \n",
       "4   \\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...   \n",
       "\n",
       "                                       Coeur_Article              Date  \\\n",
       "0   \\n\\nStony Brook University, one of two state ...  31 December 2023   \n",
       "1   \\n\\nIn their one movie together, their chemis...  31 December 2023   \n",
       "2   \\n\\ndebatethis\\n\\nTalking during movies: Tota...  31 December 2023   \n",
       "3   \\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...  31 December 2023   \n",
       "4   \\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...  31 December 2023   \n",
       "\n",
       "            Auteur  Nombre de mots         Journal  \\\n",
       "0       Nick Tabor             529  New York Times   \n",
       "1    Wesley Morris             422  New York Times   \n",
       "2             None             179  New York Times   \n",
       "3             None             454  New York Times   \n",
       "4  Christina Caron             428  New York Times   \n",
       "\n",
       "                                               Titre  \\\n",
       "0  Copyright 2023 The New York Times Company.  Al...   \n",
       "1  When Jim Brown and Raquel Welch, Two Sexy Star...   \n",
       "2  Talking During Movies: Totally Evil or Part of...   \n",
       "3                                     Let Kids Vote!   \n",
       "4                         Are We Doomed to Disagree?   \n",
       "\n",
       "                            ID  \\\n",
       "0    NYTF000020240104ejcv0000d   \n",
       "1    NYTF000020231231ejcv0006h   \n",
       "2   NYTF000020231231ejcv00064    \n",
       "3   NYTF000020231231ejcv00063    \n",
       "4   NYTF000020231231ejcv0005z    \n",
       "\n",
       "                              Entreprise_Insérée_1          ISIN  \\\n",
       "0                           International Paper Co  US4601461035   \n",
       "1  China Development Bank Financial Leasing Co Ltd  CNE1000027C9   \n",
       "2                                      Southern Co  US8425871071   \n",
       "3                                        Sappi Ltd  ZAE000006284   \n",
       "4               Johnson Controls International PLC  IE00BY7QL619   \n",
       "\n",
       "                              Coeur_Article_Inséré_1  \\\n",
       "0   \\n\\nStony Brook University, one of two state ...   \n",
       "1   \\n\\nIn their one movie together, their chemis...   \n",
       "2   \\n\\ndebatethis\\n\\nTalking during movies: Tota...   \n",
       "3   \\n\\nLET KIDS\\n\\nVOTE!\\n\\nby Katherine Cusuman...   \n",
       "4   \\n\\nare we DOOMED TO DISAGREE?\\n\\nwhy it's so...   \n",
       "\n",
       "                                Preprocessed_Article  \\\n",
       "0  [stony, brook, university, one, two, state, sc...   \n",
       "1  [one, movie, together, chemistry, radical, jim...   \n",
       "2  [debatethis, talking, movie, totally, evil, pa...   \n",
       "3  [let, kid, vote, katherine, cusumano, julia, r...   \n",
       "4  [doomed, disagree, hard, change, people, mind,...   \n",
       "\n",
       "   environmental_sentiment_score  \\\n",
       "0                      -0.002011   \n",
       "1                      -0.005395   \n",
       "2                       0.010000   \n",
       "3                       0.024060   \n",
       "4                       0.003908   \n",
       "\n",
       "                             Entreprises_identifiees  \n",
       "0  [(International Paper Co, 3742), (Internationa...  \n",
       "1  [(China Development Bank Financial Leasing Co ...  \n",
       "2  [(Southern Co, 538), (Southern Co, 622), (Sout...  \n",
       "3  [(Sappi Ltd, 766), (Sappi Ltd, 898), (Sappi Lt...  \n",
       "4  [(Johnson Controls International PLC, 127), (J...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_avec_entreprises.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer en tant que fichier pickle pour conserver les types de données\n",
    "data.to_pickle('data_avec_entreprises.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaites maintenant récupérer le contexte autour de l'entreprise dans l'article. Pour cela, on propose de récupérer les k (constante fixée arbitrairement) mots avant et après le mot qui a permis d'identifier une entreprise. L'intérêt de choisir un k faible, est de s'assurer que le contexte s'applique bien à l'entreprise que l'on souhaites évaluer. En revanche, on perds de l'information si on choisis un k trop faible. A l'inverse, choisir un k élevé permet de récupérer beaucoup de termes, mais on prends le risque de liéer ces termes à l'entreprises, alors qu'il est possible que dans l'article, il n'y ait pas de lien. On améliore donc le code précédent de la manière suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constante k\n",
    "k = 5  # Vous pouvez ajuster cette valeur selon vos besoins\n",
    "\n",
    "# Parcourir chaque article dans la base de données\n",
    "for index, row_article in df_articles.iterrows():\n",
    "    article = row_article['Preprocessed_Article']  # Contenu de l'article prétraité\n",
    "    entreprises_identifiees = []  # Liste pour stocker les entreprises identifiées pour cet article\n",
    "    \n",
    "    # Parcourir chaque entreprise dans votre DataFrame df\n",
    "    for index_ent, row_ent in df_entreprises.iterrows():\n",
    "        companies = row_ent['Company']  # Nom de l'entreprise correspondante à cet article\n",
    "        keywords = row_ent['Preprocessed_Company']  # Liste de mots-clés prétraités\n",
    "        \n",
    "        # Utiliser seulement le premier mot-clé\n",
    "        if keywords:  # Vérifiez si la liste des mots-clés n'est pas vide\n",
    "            first_keyword = keywords[0]  # Prenez le premier mot-clé\n",
    "        \n",
    "            # Vérifiez si le mot-clé est présent dans l'article\n",
    "            if first_keyword in article:\n",
    "                # Trouver l'indice du premier mot-clé dans l'article\n",
    "                idx_keyword = article.index(first_keyword)\n",
    "                \n",
    "                # Extraire les k mots avant et après le mot identifié\n",
    "                start_idx = max(0, idx_keyword - k)\n",
    "                end_idx = min(len(article), idx_keyword + k + 1)  # Ajouter 1 pour inclure le dernier indice\n",
    "                context_words = article[start_idx:end_idx]\n",
    "                \n",
    "                # Ajouter l'entreprise et le contexte à la liste des entreprises identifiées pour cet article\n",
    "                entreprises_identifiees.append((companies, context_words))\n",
    "    \n",
    "    # Mettre à jour la colonne 'Entreprises_Identifiees_2' avec les entreprises identifiées pour cet article\n",
    "    df_articles.at[index, 'Entreprises_Identifiees_2'] = entreprises_identifiees\n",
    "    df_articles['Nombre_Entreprises_Identifiees_2'] = df_articles['Entreprises_Identifiees_2'].apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('american international group inc',\n",
       " ['including',\n",
       "  'one',\n",
       "  'largest',\n",
       "  'donation',\n",
       "  'university',\n",
       "  'american',\n",
       "  'history',\n",
       "  'right',\n",
       "  'develop',\n",
       "  'climate',\n",
       "  'solution'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles['Entreprises_Identifiees_2'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['including',\n",
       " 'one',\n",
       " 'largest',\n",
       " 'donation',\n",
       " 'university',\n",
       " 'american',\n",
       " 'history',\n",
       " 'right',\n",
       " 'develop',\n",
       " 'climate',\n",
       " 'solution']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles['Entreprises_Identifiees_2'][0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourrait chercher à appliquer l'analyse de sentiment à ces mots, puis l'associer à l'entreprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_environmental_score(df_articles['Entreprises_Identifiees_2'][0][0][1],Dico_env_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constante k\n",
    "k = 5  # Vous pouvez ajuster cette valeur selon vos besoins\n",
    "\n",
    "# Parcourir chaque article dans la base de données\n",
    "for index, row_article in df_articles.iterrows():\n",
    "    article = row_article['Preprocessed_Article']  # Contenu de l'article prétraité\n",
    "    entreprises_identifiees = []  # Liste pour stocker les entreprises identifiées pour cet article\n",
    "    \n",
    "    # Parcourir chaque entreprise dans votre DataFrame df\n",
    "    for index_ent, row_ent in df_entreprises.iterrows():\n",
    "        companies = row_ent['Company']  # Nom de l'entreprise correspondante à cet article\n",
    "        keywords = row_ent['Preprocessed_Company']  # Liste de mots-clés prétraités\n",
    "        \n",
    "        # Utiliser seulement le premier mot-clé\n",
    "        if keywords:  # Vérifiez si la liste des mots-clés n'est pas vide\n",
    "            first_keyword = keywords[0]  # Prenez le premier mot-clé\n",
    "        \n",
    "            # Vérifiez si le mot-clé est présent dans l'article\n",
    "            if first_keyword in article:\n",
    "                # Trouver l'indice du premier mot-clé dans l'article\n",
    "                idx_keyword = article.index(first_keyword)\n",
    "                \n",
    "                # Extraire les k mots avant et après le mot identifié\n",
    "                start_idx = max(0, idx_keyword - k)\n",
    "                end_idx = min(len(article), idx_keyword + k + 1)  # Ajouter 1 pour inclure le dernier indice\n",
    "                context_words = article[start_idx:end_idx]\n",
    "                environmental_score = get_environmental_score(context_words,Dico_env_en)\n",
    "                \n",
    "                # Ajouter l'entreprise et le contexte à la liste des entreprises identifiées pour cet article\n",
    "                entreprises_identifiees.append((companies, context_words, environmental_score))\n",
    "    \n",
    "    # Mettre à jour la colonne 'Entreprises_Identifiees_2' avec les entreprises identifiées pour cet article\n",
    "    df_articles.at[index, 'Entreprises_Identifiees_2'] = entreprises_identifiees\n",
    "    df_articles['Nombre_Entreprises_Identifiees_2'] = df_articles['Entreprises_Identifiees_2'].apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preprocessed_Article</th>\n",
       "      <th>Entreprises_citées</th>\n",
       "      <th>Nombre_entreprises_citées</th>\n",
       "      <th>Entreprises_Identifiees</th>\n",
       "      <th>Nombre_Entreprises_Identifiees</th>\n",
       "      <th>Entreprises_Identifiees_2</th>\n",
       "      <th>Nombre_Entreprises_Identifiees_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[metropolitan, desk, sectmb, ambitious, public...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[anglo american plc, british american tobacco ...</td>\n",
       "      <td>2784</td>\n",
       "      <td>[(american international group inc, [including...</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[magazine, desk, sectmm, jim, brown, raquel, w...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[general motors co, anglo american plc, philip...</td>\n",
       "      <td>1035</td>\n",
       "      <td>[(general motors co, [play, sheriff, triumphan...</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[magazine, desk, sectmk, talking, movie, total...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, tokyo e...</td>\n",
       "      <td>554</td>\n",
       "      <td>[(new gold inc, [179, word, 31, december, 2023...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[magazine, desk, sectmk, let, kid, vote, 454, ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[anglo american plc, british american tobacco ...</td>\n",
       "      <td>796</td>\n",
       "      <td>[(american international group inc, [past, 10,...</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[magazine, desk, sectmk, doomed, disagree, 428...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[roche holding ag, hsbc holdings plc, mercedes...</td>\n",
       "      <td>2872</td>\n",
       "      <td>[(u.s. bancorp, [live, connected, world, inter...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[global, health, science, desk, sectd, new, ho...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[takeda pharmaceutical co ltd, rio tinto plc, ...</td>\n",
       "      <td>5011</td>\n",
       "      <td>[(rio tinto plc, [therapy, leishmaniasis, mede...</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[foreign, desk, secta, hotel, intended, house,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[general motors co, international business mac...</td>\n",
       "      <td>2653</td>\n",
       "      <td>[(general motors co, [circumstance, recognize,...</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[businessfinancial, desk, sectb, eu, open, inv...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, mercede...</td>\n",
       "      <td>2544</td>\n",
       "      <td>[(marks and spencer group plc, [pay, authentic...</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[trilobite, science, desk, sectd, proof, wine,...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[international business machines corp, sumitom...</td>\n",
       "      <td>1265</td>\n",
       "      <td>[(best buy co inc, [ranking, instituted, napol...</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[impossible, energy, transition, mario, loyola...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[pfizer inc, merck &amp; co inc, dow inc, dow inc,...</td>\n",
       "      <td>5443</td>\n",
       "      <td>[(dow inc, [j, a13, english, copyright, 2023, ...</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Preprocessed_Article Entreprises_citées  \\\n",
       "0   [metropolitan, desk, sectmb, ambitious, public...                      \n",
       "1   [magazine, desk, sectmm, jim, brown, raquel, w...                      \n",
       "2   [magazine, desk, sectmk, talking, movie, total...                      \n",
       "3   [magazine, desk, sectmk, let, kid, vote, 454, ...                      \n",
       "4   [magazine, desk, sectmk, doomed, disagree, 428...                      \n",
       "..                                                ...                ...   \n",
       "95  [global, health, science, desk, sectd, new, ho...                      \n",
       "96  [foreign, desk, secta, hotel, intended, house,...                      \n",
       "97  [businessfinancial, desk, sectb, eu, open, inv...                      \n",
       "98  [trilobite, science, desk, sectd, proof, wine,...                      \n",
       "99  [impossible, energy, transition, mario, loyola...                      \n",
       "\n",
       "    Nombre_entreprises_citées  \\\n",
       "0                           0   \n",
       "1                           0   \n",
       "2                           0   \n",
       "3                           0   \n",
       "4                           0   \n",
       "..                        ...   \n",
       "95                          0   \n",
       "96                          0   \n",
       "97                          0   \n",
       "98                          0   \n",
       "99                          0   \n",
       "\n",
       "                              Entreprises_Identifiees  \\\n",
       "0   [anglo american plc, british american tobacco ...   \n",
       "1   [general motors co, anglo american plc, philip...   \n",
       "2   [international business machines corp, tokyo e...   \n",
       "3   [anglo american plc, british american tobacco ...   \n",
       "4   [roche holding ag, hsbc holdings plc, mercedes...   \n",
       "..                                                ...   \n",
       "95  [takeda pharmaceutical co ltd, rio tinto plc, ...   \n",
       "96  [general motors co, international business mac...   \n",
       "97  [international business machines corp, mercede...   \n",
       "98  [international business machines corp, sumitom...   \n",
       "99  [pfizer inc, merck & co inc, dow inc, dow inc,...   \n",
       "\n",
       "    Nombre_Entreprises_Identifiees  \\\n",
       "0                             2784   \n",
       "1                             1035   \n",
       "2                              554   \n",
       "3                              796   \n",
       "4                             2872   \n",
       "..                             ...   \n",
       "95                            5011   \n",
       "96                            2653   \n",
       "97                            2544   \n",
       "98                            1265   \n",
       "99                            5443   \n",
       "\n",
       "                            Entreprises_Identifiees_2  \\\n",
       "0   [(american international group inc, [including...   \n",
       "1   [(general motors co, [play, sheriff, triumphan...   \n",
       "2   [(new gold inc, [179, word, 31, december, 2023...   \n",
       "3   [(american international group inc, [past, 10,...   \n",
       "4   [(u.s. bancorp, [live, connected, world, inter...   \n",
       "..                                                ...   \n",
       "95  [(rio tinto plc, [therapy, leishmaniasis, mede...   \n",
       "96  [(general motors co, [circumstance, recognize,...   \n",
       "97  [(marks and spencer group plc, [pay, authentic...   \n",
       "98  [(best buy co inc, [ranking, instituted, napol...   \n",
       "99  [(dow inc, [j, a13, english, copyright, 2023, ...   \n",
       "\n",
       "    Nombre_Entreprises_Identifiees_2  \n",
       "0                                394  \n",
       "1                                320  \n",
       "2                                 60  \n",
       "3                                130  \n",
       "4                                 92  \n",
       "..                               ...  \n",
       "95                               640  \n",
       "96                               345  \n",
       "97                               252  \n",
       "98                               165  \n",
       "99                               440  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles['Entreprises_Identifiees_2'][0][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles['Entreprises_Identifiees_2'][0][1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles['Entreprises_Identifiees_2'][0][2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles['Entreprises_Identifiees_2'][0][3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut toujours qu'on cherche à identifier si l'entreprise à communiqué, ou bien si c'est une impression du journaliste..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Titre_article'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Affichage des articles avec une seule entreprise citée\u001b[39;00m\n\u001b[1;32m      8\u001b[0m articles_avec_entreprise_unique \u001b[38;5;241m=\u001b[39m df_articles[df_articles[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntreprise_unique\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43marticles_avec_entreprise_unique\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTitre_article\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEntreprise_unique\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/pandas/core/frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.11/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Titre_article'] not in index\""
     ]
    }
   ],
   "source": [
    "# Suppression des données sans entreprises citées\n",
    "df_articles = df_articles[df_articles['Entreprises_citées'] != '']\n",
    "\n",
    "# Identification des articles avec une seule entreprise citée\n",
    "df_articles['Entreprise_unique'] = df_articles['Entreprises_citées'].apply(lambda x: x.split(',')[0] if ',' not in x else '')\n",
    "\n",
    "# Affichage des articles avec une seule entreprise citée\n",
    "articles_avec_entreprise_unique = df_articles[df_articles['Entreprise_unique'] != '']\n",
    "print(articles_avec_entreprise_unique[['Titre_article', 'Entreprise_unique']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkb1ZHzje3fw"
   },
   "source": [
    "Si on a une base de données qui contient un certain nomre d'articles deja labellisés avec une note environnementale, on pourrait entrainer un modèle de machine learning plus traditionnel que de l'analyse de sentiment.\n",
    "\n",
    "en effet, on peut passer par de la vectorization des mots par TF-IDF (ou autre - à voir), puis entrainer un modèle de régression linéaire (ou autre - à voir), et prédire pour les nouveaux articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYxpksZle1d_"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dataset['text'])  # 'texts' is the column with your text data\n",
    "y = dataset['scores']  # 'scores' is the column with your positivity scores\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "new_text = vectorizer.transform([\"New text\"])\n",
    "new_score = model.predict(new_text)\n",
    "print(f'Predicted Sentiment Score: {new_score}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
