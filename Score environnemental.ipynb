{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dicctionnary in vader_lexicon.txt\n",
    "\n",
    "This dictionary is part of the VADER sentiment analysis tool, which stands for **Valence Aware Dictionary and sEntiment Reasoner**. It's specifically designed to analyze sentiments expressed in social media, and it operates by scoring words based on their sentiment valence â€” the intensity of emotion they convey.\n",
    "\n",
    "Here's how the dictionary is structured:\n",
    "\n",
    "1. **Word/Phrase:** The first column lists the word or phrase being scored.\n",
    "\n",
    "2. **Mean Sentiment Rating:** The second column shows the mean sentiment rating for the word or phrase on a scale from -4 to 4. Negative numbers indicate negative sentiment, positive numbers indicate positive sentiment, and numbers close to 0 indicate neutrality. For example:\n",
    "   - `\"abandon\"` has a mean sentiment rating of -1.9, indicating a strongly negative sentiment.\n",
    "   - `\"abilities\"` has a mean sentiment rating of 1.0, indicating a positive sentiment.\n",
    "\n",
    "3. **Standard Deviation:** The third column shows the standard deviation of the sentiment ratings, indicating the variability or dispersion of the sentiment scores from the mean. A higher standard deviation indicates greater disagreement among the raters about the sentiment score of the word.\n",
    "\n",
    "4. **Sentiment Ratings Array:** The fourth column is an array of individual sentiment ratings given by different raters, contributing to the mean sentiment rating. This array shows how each rater scored the word or phrase, indicating the range of sentiment different people might associate with each word.\n",
    "\n",
    "For example, for `\"abandon\"`:\n",
    "- The **mean sentiment rating** is -1.9, indicating a generally negative sentiment.\n",
    "- The **standard deviation** is 0.53852, suggesting some variability in how negatively the term is viewed.\n",
    "- The **sentiment ratings array** `[-1, -2, -2, -2, -2, -3, -2, -2, -1, -2]` shows individual scores from various raters, mostly negative, contributing to its negative mean score.\n",
    "\n",
    "The VADER sentiment analysis tool uses these scores, along with grammatical and syntactical rules (e.g., capitalization, punctuation, word modifiers), to accurately assess the overall sentiment of texts, especially short texts like social media posts, where context and intensity of sentiment are crucial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to adapt this dictionnary and the VADER sentiment analysis function to our environmental scoring goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The  VADER sentiment_analysis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Toolkit: vader\n",
    "#\n",
    "# Copyright (C) 2001-2023 NLTK Project\n",
    "# Author: C.J. Hutto <Clayton.Hutto@gtri.gatech.edu>\n",
    "#         Ewan Klein <ewan@inf.ed.ac.uk> (modifications)\n",
    "#         Pierpaolo Pantone <24alsecondo@gmail.com> (modifications)\n",
    "#         George Berry <geb97@cornell.edu> (modifications)\n",
    "#         Malavika Suresh <malavika.suresh0794@gmail.com> (modifications)\n",
    "# URL: <https://www.nltk.org/>\n",
    "# For license information, see LICENSE.TXT\n",
    "#\n",
    "# Modifications to the original VADER code have been made in order to\n",
    "# integrate it into NLTK. These have involved changes to\n",
    "# ensure Python 3 compatibility, and refactoring to achieve greater modularity.\n",
    "\n",
    "\"\"\"\n",
    "If you use the VADER sentiment analysis tools, please cite:\n",
    "\n",
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for\n",
    "Sentiment Analysis of Social Media Text. Eighth International Conference on\n",
    "Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "from itertools import product\n",
    "\n",
    "import nltk.data\n",
    "from nltk.util import pairwise\n",
    "\n",
    "\n",
    "class VaderConstants:\n",
    "    \"\"\"\n",
    "    A class to keep the Vader lists and constants.\n",
    "    \"\"\"\n",
    "\n",
    "    ##Constants##\n",
    "    # (empirically derived mean sentiment intensity rating increase for booster words)\n",
    "    B_INCR = 0.293\n",
    "    B_DECR = -0.293\n",
    "\n",
    "    # (empirically derived mean sentiment intensity rating increase for using\n",
    "    # ALLCAPs to emphasize a word)\n",
    "    C_INCR = 0.733\n",
    "\n",
    "    N_SCALAR = -0.74\n",
    "\n",
    "    NEGATE = {\n",
    "        \"aint\",\n",
    "        \"arent\",\n",
    "        \"cannot\",\n",
    "        \"cant\",\n",
    "        \"couldnt\",\n",
    "        \"darent\",\n",
    "        \"didnt\",\n",
    "        \"doesnt\",\n",
    "        \"ain't\",\n",
    "        \"aren't\",\n",
    "        \"can't\",\n",
    "        \"couldn't\",\n",
    "        \"daren't\",\n",
    "        \"didn't\",\n",
    "        \"doesn't\",\n",
    "        \"dont\",\n",
    "        \"hadnt\",\n",
    "        \"hasnt\",\n",
    "        \"havent\",\n",
    "        \"isnt\",\n",
    "        \"mightnt\",\n",
    "        \"mustnt\",\n",
    "        \"neither\",\n",
    "        \"don't\",\n",
    "        \"hadn't\",\n",
    "        \"hasn't\",\n",
    "        \"haven't\",\n",
    "        \"isn't\",\n",
    "        \"mightn't\",\n",
    "        \"mustn't\",\n",
    "        \"neednt\",\n",
    "        \"needn't\",\n",
    "        \"never\",\n",
    "        \"none\",\n",
    "        \"nope\",\n",
    "        \"nor\",\n",
    "        \"not\",\n",
    "        \"nothing\",\n",
    "        \"nowhere\",\n",
    "        \"oughtnt\",\n",
    "        \"shant\",\n",
    "        \"shouldnt\",\n",
    "        \"uhuh\",\n",
    "        \"wasnt\",\n",
    "        \"werent\",\n",
    "        \"oughtn't\",\n",
    "        \"shan't\",\n",
    "        \"shouldn't\",\n",
    "        \"uh-uh\",\n",
    "        \"wasn't\",\n",
    "        \"weren't\",\n",
    "        \"without\",\n",
    "        \"wont\",\n",
    "        \"wouldnt\",\n",
    "        \"won't\",\n",
    "        \"wouldn't\",\n",
    "        \"rarely\",\n",
    "        \"seldom\",\n",
    "        \"despite\",\n",
    "    }\n",
    "\n",
    "    # booster/dampener 'intensifiers' or 'degree adverbs'\n",
    "    # https://en.wiktionary.org/wiki/Category:English_degree_adverbs\n",
    "\n",
    "    BOOSTER_DICT = {\n",
    "        \"absolutely\": B_INCR,\n",
    "        \"amazingly\": B_INCR,\n",
    "        \"awfully\": B_INCR,\n",
    "        \"completely\": B_INCR,\n",
    "        \"considerably\": B_INCR,\n",
    "        \"decidedly\": B_INCR,\n",
    "        \"deeply\": B_INCR,\n",
    "        \"effing\": B_INCR,\n",
    "        \"enormously\": B_INCR,\n",
    "        \"entirely\": B_INCR,\n",
    "        \"especially\": B_INCR,\n",
    "        \"exceptionally\": B_INCR,\n",
    "        \"extremely\": B_INCR,\n",
    "        \"fabulously\": B_INCR,\n",
    "        \"flipping\": B_INCR,\n",
    "        \"flippin\": B_INCR,\n",
    "        \"fricking\": B_INCR,\n",
    "        \"frickin\": B_INCR,\n",
    "        \"frigging\": B_INCR,\n",
    "        \"friggin\": B_INCR,\n",
    "        \"fully\": B_INCR,\n",
    "        \"fucking\": B_INCR,\n",
    "        \"greatly\": B_INCR,\n",
    "        \"hella\": B_INCR,\n",
    "        \"highly\": B_INCR,\n",
    "        \"hugely\": B_INCR,\n",
    "        \"incredibly\": B_INCR,\n",
    "        \"intensely\": B_INCR,\n",
    "        \"majorly\": B_INCR,\n",
    "        \"more\": B_INCR,\n",
    "        \"most\": B_INCR,\n",
    "        \"particularly\": B_INCR,\n",
    "        \"purely\": B_INCR,\n",
    "        \"quite\": B_INCR,\n",
    "        \"really\": B_INCR,\n",
    "        \"remarkably\": B_INCR,\n",
    "        \"so\": B_INCR,\n",
    "        \"substantially\": B_INCR,\n",
    "        \"thoroughly\": B_INCR,\n",
    "        \"totally\": B_INCR,\n",
    "        \"tremendously\": B_INCR,\n",
    "        \"uber\": B_INCR,\n",
    "        \"unbelievably\": B_INCR,\n",
    "        \"unusually\": B_INCR,\n",
    "        \"utterly\": B_INCR,\n",
    "        \"very\": B_INCR,\n",
    "        \"almost\": B_DECR,\n",
    "        \"barely\": B_DECR,\n",
    "        \"hardly\": B_DECR,\n",
    "        \"just enough\": B_DECR,\n",
    "        \"kind of\": B_DECR,\n",
    "        \"kinda\": B_DECR,\n",
    "        \"kindof\": B_DECR,\n",
    "        \"kind-of\": B_DECR,\n",
    "        \"less\": B_DECR,\n",
    "        \"little\": B_DECR,\n",
    "        \"marginally\": B_DECR,\n",
    "        \"occasionally\": B_DECR,\n",
    "        \"partly\": B_DECR,\n",
    "        \"scarcely\": B_DECR,\n",
    "        \"slightly\": B_DECR,\n",
    "        \"somewhat\": B_DECR,\n",
    "        \"sort of\": B_DECR,\n",
    "        \"sorta\": B_DECR,\n",
    "        \"sortof\": B_DECR,\n",
    "        \"sort-of\": B_DECR,\n",
    "    }\n",
    "\n",
    "    # check for special case idioms using a sentiment-laden keyword known to SAGE\n",
    "    SPECIAL_CASE_IDIOMS = {\n",
    "        \"the shit\": 3,\n",
    "        \"the bomb\": 3,\n",
    "        \"bad ass\": 1.5,\n",
    "        \"yeah right\": -2,\n",
    "        \"cut the mustard\": 2,\n",
    "        \"kiss of death\": -1.5,\n",
    "        \"hand to mouth\": -2,\n",
    "    }\n",
    "\n",
    "    # for removing punctuation\n",
    "    REGEX_REMOVE_PUNCTUATION = re.compile(f\"[{re.escape(string.punctuation)}]\")\n",
    "\n",
    "    PUNC_LIST = [\n",
    "        \".\",\n",
    "        \"!\",\n",
    "        \"?\",\n",
    "        \",\",\n",
    "        \";\",\n",
    "        \":\",\n",
    "        \"-\",\n",
    "        \"'\",\n",
    "        '\"',\n",
    "        \"!!\",\n",
    "        \"!!!\",\n",
    "        \"??\",\n",
    "        \"???\",\n",
    "        \"?!?\",\n",
    "        \"!?!\",\n",
    "        \"?!?!\",\n",
    "        \"!?!?\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def negated(self, input_words, include_nt=True):\n",
    "        \"\"\"\n",
    "        Determine if input contains negation words\n",
    "        \"\"\"\n",
    "        neg_words = self.NEGATE\n",
    "        if any(word.lower() in neg_words for word in input_words):\n",
    "            return True\n",
    "        if include_nt:\n",
    "            if any(\"n't\" in word.lower() for word in input_words):\n",
    "                return True\n",
    "        for first, second in pairwise(input_words):\n",
    "            if second.lower() == \"least\" and first.lower() != \"at\":\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def normalize(self, score, alpha=15):\n",
    "        \"\"\"\n",
    "        Normalize the score to be between -1 and 1 using an alpha that\n",
    "        approximates the max expected value\n",
    "        \"\"\"\n",
    "        norm_score = score / math.sqrt((score * score) + alpha)\n",
    "        return norm_score\n",
    "\n",
    "\n",
    "    def scalar_inc_dec(self, word, valence, is_cap_diff):\n",
    "        \"\"\"\n",
    "        Check if the preceding words increase, decrease, or negate/nullify the\n",
    "        valence\n",
    "        \"\"\"\n",
    "        scalar = 0.0\n",
    "        word_lower = word.lower()\n",
    "        if word_lower in self.BOOSTER_DICT:\n",
    "            scalar = self.BOOSTER_DICT[word_lower]\n",
    "            if valence < 0:\n",
    "                scalar *= -1\n",
    "            # check if booster/dampener word is in ALLCAPS (while others aren't)\n",
    "            if word.isupper() and is_cap_diff:\n",
    "                if valence > 0:\n",
    "                    scalar += self.C_INCR\n",
    "                else:\n",
    "                    scalar -= self.C_INCR\n",
    "        return scalar\n",
    "\n",
    "\n",
    "\n",
    "class SentiText:\n",
    "    \"\"\"\n",
    "    Identify sentiment-relevant string-level properties of input text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text, punc_list, regex_remove_punctuation):\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text.encode(\"utf-8\"))\n",
    "        self.text = text\n",
    "        self.PUNC_LIST = punc_list\n",
    "        self.REGEX_REMOVE_PUNCTUATION = regex_remove_punctuation\n",
    "        self.words_and_emoticons = self._words_and_emoticons()\n",
    "        # doesn't separate words from\n",
    "        # adjacent punctuation (keeps emoticons & contractions)\n",
    "        self.is_cap_diff = self.allcap_differential(self.words_and_emoticons)\n",
    "\n",
    "\n",
    "    def _words_plus_punc(self):\n",
    "        \"\"\"\n",
    "        Returns mapping of form:\n",
    "        {\n",
    "            'cat,': 'cat',\n",
    "            ',cat': 'cat',\n",
    "        }\n",
    "        \"\"\"\n",
    "        no_punc_text = self.REGEX_REMOVE_PUNCTUATION.sub(\"\", self.text)\n",
    "        # removes punctuation (but loses emoticons & contractions)\n",
    "        words_only = no_punc_text.split()\n",
    "        # remove singletons\n",
    "        words_only = {w for w in words_only if len(w) > 1}\n",
    "        # the product gives ('cat', ',') and (',', 'cat')\n",
    "        punc_before = {\"\".join(p): p[1] for p in product(self.PUNC_LIST, words_only)}\n",
    "        punc_after = {\"\".join(p): p[0] for p in product(words_only, self.PUNC_LIST)}\n",
    "        words_punc_dict = punc_before\n",
    "        words_punc_dict.update(punc_after)\n",
    "        return words_punc_dict\n",
    "\n",
    "    def _words_and_emoticons(self):\n",
    "        \"\"\"\n",
    "        Removes leading and trailing puncutation\n",
    "        Leaves contractions and most emoticons\n",
    "            Does not preserve punc-plus-letter emoticons (e.g. :D)\n",
    "        \"\"\"\n",
    "        wes = self.text.split()\n",
    "        words_punc_dict = self._words_plus_punc()\n",
    "        wes = [we for we in wes if len(we) > 1]\n",
    "        for i, we in enumerate(wes):\n",
    "            if we in words_punc_dict:\n",
    "                wes[i] = words_punc_dict[we]\n",
    "        return wes\n",
    "\n",
    "    def allcap_differential(self, words):\n",
    "        \"\"\"\n",
    "        Check whether just some words in the input are ALL CAPS\n",
    "\n",
    "        :param list words: The words to inspect\n",
    "        :returns: `True` if some but not all items in `words` are ALL CAPS\n",
    "        \"\"\"\n",
    "        is_different = False\n",
    "        allcap_words = 0\n",
    "        for word in words:\n",
    "            if word.isupper():\n",
    "                allcap_words += 1\n",
    "        cap_differential = len(words) - allcap_words\n",
    "        if 0 < cap_differential < len(words):\n",
    "            is_different = True\n",
    "        return is_different\n",
    "\n",
    "\n",
    "\n",
    "class SentimentIntensityAnalyzer:\n",
    "    \"\"\"\n",
    "    Give a sentiment intensity score to sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # lexicon_file=\"sentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\",\n",
    "        lexicon_file=\"\",\n",
    "    ):\n",
    "        self.lexicon_file = nltk.data.load(lexicon_file)\n",
    "        self.lexicon = self.make_lex_dict()\n",
    "        self.constants = VaderConstants()\n",
    "\n",
    "\n",
    "    def make_lex_dict(self):\n",
    "        \"\"\"\n",
    "        Convert lexicon file to a dictionary\n",
    "        \"\"\"\n",
    "        lex_dict = {}\n",
    "        for line in self.lexicon_file.split(\"\\n\"):\n",
    "            (word, measure) = line.strip().split(\"\\t\")[0:2]\n",
    "            lex_dict[word] = float(measure)\n",
    "        return lex_dict\n",
    "\n",
    "\n",
    "    def polarity_scores(self, text):\n",
    "        \"\"\"\n",
    "        Return a float for sentiment strength based on the input text.\n",
    "        Positive values are positive valence, negative value are negative\n",
    "        valence.\n",
    "\n",
    "        :note: Hashtags are not taken into consideration (e.g. #BAD is neutral). If you\n",
    "            are interested in processing the text in the hashtags too, then we recommend\n",
    "            preprocessing your data to remove the #, after which the hashtag text may be\n",
    "            matched as if it was a normal word in the sentence.\n",
    "        \"\"\"\n",
    "        # text, words_and_emoticons, is_cap_diff = self.preprocess(text)\n",
    "        sentitext = SentiText(\n",
    "            text, self.constants.PUNC_LIST, self.constants.REGEX_REMOVE_PUNCTUATION\n",
    "        )\n",
    "        sentiments = []\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        for item in words_and_emoticons:\n",
    "            valence = 0\n",
    "            i = words_and_emoticons.index(item)\n",
    "            if (\n",
    "                i < len(words_and_emoticons) - 1\n",
    "                and item.lower() == \"kind\"\n",
    "                and words_and_emoticons[i + 1].lower() == \"of\"\n",
    "            ) or item.lower() in self.constants.BOOSTER_DICT:\n",
    "                sentiments.append(valence)\n",
    "                continue\n",
    "\n",
    "            sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments)\n",
    "\n",
    "        sentiments = self._but_check(words_and_emoticons, sentiments)\n",
    "\n",
    "        return self.score_valence(sentiments, text)\n",
    "\n",
    "\n",
    "    def sentiment_valence(self, valence, sentitext, item, i, sentiments):\n",
    "        is_cap_diff = sentitext.is_cap_diff\n",
    "        words_and_emoticons = sentitext.words_and_emoticons\n",
    "        item_lowercase = item.lower()\n",
    "        if item_lowercase in self.lexicon:\n",
    "            # get the sentiment valence\n",
    "            valence = self.lexicon[item_lowercase]\n",
    "\n",
    "            # check if sentiment laden word is in ALL CAPS (while others aren't)\n",
    "            if item.isupper() and is_cap_diff:\n",
    "                if valence > 0:\n",
    "                    valence += self.constants.C_INCR\n",
    "                else:\n",
    "                    valence -= self.constants.C_INCR\n",
    "\n",
    "            for start_i in range(0, 3):\n",
    "                if (\n",
    "                    i > start_i\n",
    "                    and words_and_emoticons[i - (start_i + 1)].lower()\n",
    "                    not in self.lexicon\n",
    "                ):\n",
    "                    # dampen the scalar modifier of preceding words and emoticons\n",
    "                    # (excluding the ones that immediately preceed the item) based\n",
    "                    # on their distance from the current item.\n",
    "                    s = self.constants.scalar_inc_dec(\n",
    "                        words_and_emoticons[i - (start_i + 1)], valence, is_cap_diff\n",
    "                    )\n",
    "                    if start_i == 1 and s != 0:\n",
    "                        s = s * 0.95\n",
    "                    if start_i == 2 and s != 0:\n",
    "                        s = s * 0.9\n",
    "                    valence = valence + s\n",
    "                    valence = self._never_check(\n",
    "                        valence, words_and_emoticons, start_i, i\n",
    "                    )\n",
    "                    if start_i == 2:\n",
    "                        valence = self._idioms_check(valence, words_and_emoticons, i)\n",
    "\n",
    "                        # future work: consider other sentiment-laden idioms\n",
    "                        # other_idioms =\n",
    "                        # {\"back handed\": -2, \"blow smoke\": -2, \"blowing smoke\": -2,\n",
    "                        #  \"upper hand\": 1, \"break a leg\": 2,\n",
    "                        #  \"cooking with gas\": 2, \"in the black\": 2, \"in the red\": -2,\n",
    "                        #  \"on the ball\": 2,\"under the weather\": -2}\n",
    "\n",
    "            valence = self._least_check(valence, words_and_emoticons, i)\n",
    "\n",
    "        sentiments.append(valence)\n",
    "        return sentiments\n",
    "\n",
    "\n",
    "    def _least_check(self, valence, words_and_emoticons, i):\n",
    "        # check for negation case using \"least\"\n",
    "        if (\n",
    "            i > 1\n",
    "            and words_and_emoticons[i - 1].lower() not in self.lexicon\n",
    "            and words_and_emoticons[i - 1].lower() == \"least\"\n",
    "        ):\n",
    "            if (\n",
    "                words_and_emoticons[i - 2].lower() != \"at\"\n",
    "                and words_and_emoticons[i - 2].lower() != \"very\"\n",
    "            ):\n",
    "                valence = valence * self.constants.N_SCALAR\n",
    "        elif (\n",
    "            i > 0\n",
    "            and words_and_emoticons[i - 1].lower() not in self.lexicon\n",
    "            and words_and_emoticons[i - 1].lower() == \"least\"\n",
    "        ):\n",
    "            valence = valence * self.constants.N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    def _but_check(self, words_and_emoticons, sentiments):\n",
    "        words_and_emoticons = [w_e.lower() for w_e in words_and_emoticons]\n",
    "        but = {\"but\"} & set(words_and_emoticons)\n",
    "        if but:\n",
    "            bi = words_and_emoticons.index(next(iter(but)))\n",
    "            for sidx, sentiment in enumerate(sentiments):\n",
    "                if sidx < bi:\n",
    "                    sentiments[sidx] = sentiment * 0.5\n",
    "                elif sidx > bi:\n",
    "                    sentiments[sidx] = sentiment * 1.5\n",
    "        return sentiments\n",
    "\n",
    "    def _idioms_check(self, valence, words_and_emoticons, i):\n",
    "        onezero = f\"{words_and_emoticons[i - 1]} {words_and_emoticons[i]}\"\n",
    "\n",
    "        twoonezero = \"{} {} {}\".format(\n",
    "            words_and_emoticons[i - 2],\n",
    "            words_and_emoticons[i - 1],\n",
    "            words_and_emoticons[i],\n",
    "        )\n",
    "\n",
    "        twoone = f\"{words_and_emoticons[i - 2]} {words_and_emoticons[i - 1]}\"\n",
    "\n",
    "        threetwoone = \"{} {} {}\".format(\n",
    "            words_and_emoticons[i - 3],\n",
    "            words_and_emoticons[i - 2],\n",
    "            words_and_emoticons[i - 1],\n",
    "        )\n",
    "\n",
    "        threetwo = \"{} {}\".format(\n",
    "            words_and_emoticons[i - 3], words_and_emoticons[i - 2]\n",
    "        )\n",
    "\n",
    "        sequences = [onezero, twoonezero, twoone, threetwoone, threetwo]\n",
    "\n",
    "        for seq in sequences:\n",
    "            if seq in self.constants.SPECIAL_CASE_IDIOMS:\n",
    "                valence = self.constants.SPECIAL_CASE_IDIOMS[seq]\n",
    "                break\n",
    "\n",
    "        if len(words_and_emoticons) - 1 > i:\n",
    "            zeroone = f\"{words_and_emoticons[i]} {words_and_emoticons[i + 1]}\"\n",
    "            if zeroone in self.constants.SPECIAL_CASE_IDIOMS:\n",
    "                valence = self.constants.SPECIAL_CASE_IDIOMS[zeroone]\n",
    "        if len(words_and_emoticons) - 1 > i + 1:\n",
    "            zeroonetwo = \"{} {} {}\".format(\n",
    "                words_and_emoticons[i],\n",
    "                words_and_emoticons[i + 1],\n",
    "                words_and_emoticons[i + 2],\n",
    "            )\n",
    "            if zeroonetwo in self.constants.SPECIAL_CASE_IDIOMS:\n",
    "                valence = self.constants.SPECIAL_CASE_IDIOMS[zeroonetwo]\n",
    "\n",
    "        # check for booster/dampener bi-grams such as 'sort of' or 'kind of'\n",
    "        if (\n",
    "            threetwo in self.constants.BOOSTER_DICT\n",
    "            or twoone in self.constants.BOOSTER_DICT\n",
    "        ):\n",
    "            valence = valence + self.constants.B_DECR\n",
    "        return valence\n",
    "\n",
    "    def _never_check(self, valence, words_and_emoticons, start_i, i):\n",
    "        if start_i == 0:\n",
    "            if self.constants.negated([words_and_emoticons[i - 1]]):\n",
    "                valence = valence * self.constants.N_SCALAR\n",
    "        if start_i == 1:\n",
    "            if words_and_emoticons[i - 2] == \"never\" and (\n",
    "                words_and_emoticons[i - 1] == \"so\"\n",
    "                or words_and_emoticons[i - 1] == \"this\"\n",
    "            ):\n",
    "                valence = valence * 1.5\n",
    "            elif self.constants.negated([words_and_emoticons[i - (start_i + 1)]]):\n",
    "                valence = valence * self.constants.N_SCALAR\n",
    "        if start_i == 2:\n",
    "            if (\n",
    "                words_and_emoticons[i - 3] == \"never\"\n",
    "                and (\n",
    "                    words_and_emoticons[i - 2] == \"so\"\n",
    "                    or words_and_emoticons[i - 2] == \"this\"\n",
    "                )\n",
    "                or (\n",
    "                    words_and_emoticons[i - 1] == \"so\"\n",
    "                    or words_and_emoticons[i - 1] == \"this\"\n",
    "                )\n",
    "            ):\n",
    "                valence = valence * 1.25\n",
    "            elif self.constants.negated([words_and_emoticons[i - (start_i + 1)]]):\n",
    "                valence = valence * self.constants.N_SCALAR\n",
    "        return valence\n",
    "\n",
    "    def _punctuation_emphasis(self, sum_s, text):\n",
    "        # add emphasis from exclamation points and question marks\n",
    "        ep_amplifier = self._amplify_ep(text)\n",
    "        qm_amplifier = self._amplify_qm(text)\n",
    "        punct_emph_amplifier = ep_amplifier + qm_amplifier\n",
    "        return punct_emph_amplifier\n",
    "\n",
    "    def _amplify_ep(self, text):\n",
    "        # check for added emphasis resulting from exclamation points (up to 4 of them)\n",
    "        ep_count = text.count(\"!\")\n",
    "        if ep_count > 4:\n",
    "            ep_count = 4\n",
    "        # (empirically derived mean sentiment intensity rating increase for\n",
    "        # exclamation points)\n",
    "        ep_amplifier = ep_count * 0.292\n",
    "        return ep_amplifier\n",
    "\n",
    "    def _amplify_qm(self, text):\n",
    "        # check for added emphasis resulting from question marks (2 or 3+)\n",
    "        qm_count = text.count(\"?\")\n",
    "        qm_amplifier = 0\n",
    "        if qm_count > 1:\n",
    "            if qm_count <= 3:\n",
    "                # (empirically derived mean sentiment intensity rating increase for\n",
    "                # question marks)\n",
    "                qm_amplifier = qm_count * 0.18\n",
    "            else:\n",
    "                qm_amplifier = 0.96\n",
    "        return qm_amplifier\n",
    "\n",
    "    def _sift_sentiment_scores(self, sentiments):\n",
    "        # want separate positive versus negative sentiment scores\n",
    "        pos_sum = 0.0\n",
    "        neg_sum = 0.0\n",
    "        neu_count = 0\n",
    "        for sentiment_score in sentiments:\n",
    "            if sentiment_score > 0:\n",
    "                pos_sum += (\n",
    "                    float(sentiment_score) + 1\n",
    "                )  # compensates for neutral words that are counted as 1\n",
    "            if sentiment_score < 0:\n",
    "                neg_sum += (\n",
    "                    float(sentiment_score) - 1\n",
    "                )  # when used with math.fabs(), compensates for neutrals\n",
    "            if sentiment_score == 0:\n",
    "                neu_count += 1\n",
    "        return pos_sum, neg_sum, neu_count\n",
    "\n",
    "    def score_valence(self, sentiments, text):\n",
    "        if sentiments:\n",
    "            sum_s = float(sum(sentiments))\n",
    "            # compute and add emphasis from punctuation in text\n",
    "            punct_emph_amplifier = self._punctuation_emphasis(sum_s, text)\n",
    "            if sum_s > 0:\n",
    "                sum_s += punct_emph_amplifier\n",
    "            elif sum_s < 0:\n",
    "                sum_s -= punct_emph_amplifier\n",
    "\n",
    "            compound = self.constants.normalize(sum_s)\n",
    "            # discriminate between positive, negative and neutral sentiment scores\n",
    "            pos_sum, neg_sum, neu_count = self._sift_sentiment_scores(sentiments)\n",
    "\n",
    "            if pos_sum > math.fabs(neg_sum):\n",
    "                pos_sum += punct_emph_amplifier\n",
    "            elif pos_sum < math.fabs(neg_sum):\n",
    "                neg_sum -= punct_emph_amplifier\n",
    "\n",
    "            total = pos_sum + math.fabs(neg_sum) + neu_count\n",
    "            pos = math.fabs(pos_sum / total)\n",
    "            neg = math.fabs(neg_sum / total)\n",
    "            neu = math.fabs(neu_count / total)\n",
    "\n",
    "        else:\n",
    "            compound = 0.0\n",
    "            pos = 0.0\n",
    "            neg = 0.0\n",
    "            neu = 0.0\n",
    "\n",
    "        sentiment_dict = {\n",
    "            \"neg\": round(neg, 3),\n",
    "            \"neu\": round(neu, 3),\n",
    "            \"pos\": round(pos, 3),\n",
    "            \"compound\": round(compound, 4),\n",
    "        }\n",
    "\n",
    "        return sentiment_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
